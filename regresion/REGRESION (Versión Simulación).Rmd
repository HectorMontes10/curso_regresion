---
title: "PRIMERA CLASE: REGRESI√ìN LINEAL"
subtitle: "Maestr√≠a en Investigaci√≥n Operativa y Estad√≠stica"
author:
  - "Juli√°n Piedrah√≠ta Monroy"
  - "H√©ctor Hern√°n Montes"
output: 
  rmdformats::readthedown:
    css: styles.css
  html_document:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
date: "2023"
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: inline
---

<div>

<img src="https://media2.utp.edu.co/imagenes/Logo-UTP-Azul.png" alt="UPN" class="watermark"/>

</div>

```{r include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = TRUE,
  message = FALSE
)

# Librer√≠as
library(tidyverse)
library(janitor)
library(openxlsx)
library(flextable)
library(viridis)
library(scales)
library(DT)
library(lubridate)
library(gridExtra)


```

# INTRODUCCI√ìN

## DOCENTES

### H√©ctor Hern√°n Montes Garc√≠a.

Ingeniero industrial de la Universidad Tecnol√≥gica de Pereira.

Estudios en Maestr√≠a en Ciencias con orientaci√≥n en Matem√°ticas

[Desempe√±o laboral]{style="color: red;"}

Cient√≠fico de datos con m√°s de 3 a√±os de experiencia en la construcci√≥n
de modelos de aprendizaje autom√°tico y soluciones de miner√≠a de datos
para obtener mejores conocimientos comerciales. Experiencia utilizando
SQL, bibliotecas de Python (matplotlib, seaborn, flask, scikit-learn,
pandas, numpy), modelos matem√°ticos y estad√≠sticos para ofrecer
soluciones robustas que agregan valor al negocio.

Ha trabajado para clientes del sector industrial y financiero en M√©xico
y Colombia, en √°reas tales como: 

- Mantenimiento predictivo
- Dise√±o de campa√±as comerciales basadas en datos
- Reconocimiento de im√°genes
- Modelos de procesamiento de lenguaje natural (NER).

### Juli√°n Piedrah√≠ta Monroy

Ingeniero industrial.

Mag√≠ster en desarrollo agroindustrial

Universidad Tecnol√≥gica de Pereira

[Desempe√±o laboral]{style="color: red;"}

-   Consultor en soluciones an√°liticas para instituciones educativas.
-   Analista de datos en el observatorio social de la UTP.
-   Actualmente dise√±ador de tableros informativos (dashboards) para la
    Universidad Pedag√≥gica Nacional de Bogot√° y la misma Universidad
    Tecnol√≥gica de Pereira.
-   Docente catedr√°tico en el √°rea de inform√°tica y estad√≠stica general.
-   Uso principal del lenguaje R y sus librerias Tidyverse, RMarkdown,
    RShiny y otras.

## Contenido del curso

- T1. Regresi√≥n Lineal, suposiciones y requisitos. 
- T2. Estimaci√≥n de los par√°metros e interpretaci√≥n de los valores.
- T3. Pruebas de hip√≥tesis relacionadas con los par√°metros.
- T4. Evaluaci√≥n de modelos de acuerdo a las suposiciones de normalidad e independencia.
- T5. Transformaciones de las variables, y sus implicaciones en las pruebas de hip√≥tesis.
- T6. Series de Tiempo. Estacionariedad y c√≥mo obtener una serie estacionaria a partir de una que no lo es. Correlogramas.
- T7. Modelos de Box y Cox. Estimaci√≥n de par√°metros.
- T8. Criterios de evaluaci√≥n de un modelo de series de tiempo. Transformaciones.

## Bibliograf√≠a.

Regression Modeling and Data Analysis with Applications in R. Samprit
Chatterjee, Jeffrey S. Simonoff\
Montgomery, D. C., Peck, E. A., & Vining, G. G. (2002). Introducci√≥n al
an√°lisis de regresi√≥n lineal (3¬™ ed.). Limusa Wiley.\
Time Series Analysis and Its Applications: With R Examples by Shumway
and Stoffer.\
Time Series Analysis: Forecasting and Control by Author(s): George E. P.
Box, Gwilym M. Jenkins, Gregory C. Reinsel, Greta M. Ljung.\
<https://fhernanb.github.io/libro_regresion/rls.html#modelo-estad%C3%ADstico>\
<https://bookdown.org/victor_morales/SeriesdeTiempo/>

## Motivaci√≥n

Los m√©todos de regresi√≥n son t√©cnicas estad√≠sticas utilizadas para
modelar y comprender la relaci√≥n entre variables. La motivaci√≥n detr√°s
de estos m√©todos radica en la necesidad de analizar y cuantificar c√≥mo
una o m√°s variables predictoras influyen en una variable respuesta.
Los m√©todos de regresi√≥n son ampliamente utilizados en diversos campos,
incluyendo la investigaci√≥n cient√≠fica, la econom√≠a, la medicina, la
ingenier√≠a y m√°s. A continuaci√≥n, se detallan algunas de las principales
motivaciones detr√°s de estos m√©todos:

<figure>
<img src ="Imagen de regresi√≥n lineal 1.jpeg">
<figcaption>Img 1: Ejemplo de fen√≥meno lineal</figcaption>
</figure>


[Modelado de Relaciones:]{style="color: red;"}

La motivaci√≥n fundamental de la regresi√≥n es modelar la relaci√≥n entre
variables. En muchos casos, existe la necesidad de entender c√≥mo una
variable dependiente cambia en funci√≥n de una o m√°s variables
independientes. Por ejemplo, en la econom√≠a, podr√≠a ser necesario
entender c√≥mo las tasas de inter√©s afectan el gasto del consumidor.

[Predicci√≥n y Estimaci√≥n:]{style="color: red;"} Los modelos de regresi√≥n
tambi√©n se utilizan para hacer predicciones y estimaciones. Dado un
conjunto de datos hist√≥ricos, los modelos de regresi√≥n pueden ayudar a
predecir valores futuros de la variable dependiente en funci√≥n de los
valores de las variables independientes. Esto es especialmente √∫til en
campos como la meteorolog√≠a, la finanzas y el marketing.

<figure>
<img src ="Imagen de regresi√≥n lineal 2.jpeg">
<figcaption>Img 2: Prediciendo la tendencia hist√≥rica del S&P</figcaption>
</figure>


[Control y Optimizaci√≥n:]{style="color: red;"} En algunos casos, se
utilizan modelos de regresi√≥n para optimizar y controlar procesos. Por
ejemplo, en la manufactura, los modelos de regresi√≥n pueden usarse para
identificar las condiciones ideales que conducen a la m√°xima eficiencia
o calidad del producto.

[Validaci√≥n de Teor√≠as:]{style="color: red;"} En la investigaci√≥n
cient√≠fica y social, los modelos de regresi√≥n pueden usarse para validar
o refutar teor√≠as existentes. Al comparar los resultados del modelo con
las expectativas te√≥ricas, se puede evaluar la validez de las hip√≥tesis.

[Gesti√≥n de Riesgos:]{style="color: red;"} Los modelos de regresi√≥n
tambi√©n se utilizan para evaluar riesgos y tomar decisiones informadas.
En finanzas, por ejemplo, se pueden utilizar para evaluar el riesgo de
inversi√≥n y la exposici√≥n a diferentes factores del mercado.

**Ejemplo**

El modelo CAPM (Capital Asset Pricing Model) es un modelo de valoraci√≥n de activos financieros desarrollado por William Sharpe que permite estimar su rentabilidad esperada en funci√≥n del riesgo sistem√°tico. Su desarrollo est√° basado en diversas formulaciones de Harry Markowitz sobre la diversificaci√≥n y la teor√≠a moderna de Portfolio 1. El modelo CAPM es utilizado para calcular la rentabilidad que un inversionista debe exigir al realizar una inversi√≥n en un activo financiero en funci√≥n del riesgo que est√° asumiendo 2. El modelo CAPM establece una relaci√≥n lineal entre el rendimiento esperado de un activo y su riesgo sistem√°tico, medido por su beta 1. La f√≥rmula del modelo CAPM es la siguiente:

$E(r_i)=r_f+\beta_i*(E(r_m)-r_f)$

Donde:

$E(r_i)$ es la tasa de rentabilidad esperada de un activo concreto.
$rf$ es la rentabilidad del activo sin riesgo.
$\beta_i$ es la medida de la sensibilidad del activo respecto a su benchmark.
$E(r_m)$ es la tasa de rentabilidad esperada del mercado en que cotiza el activo.


<figure>
<img src ="Imagen de regresi√≥n lineal 3.jpeg">
<figcaption>Img 3: Modelo CAMP para valoraci√≥n de activos financieros</figcaption>
</figure>

[An√°lisis de Causa y Efecto:]{style="color: red;"} Los modelos de
regresi√≥n pueden ayudar a establecer relaciones causa-efecto entre
variables. Esto es √∫til para comprender c√≥mo los cambios en una variable
influyen en otras variables y viceversa.


<figure>
<img src ="Imagen de regresi√≥n lineal 4.png">
<figcaption>Img 4: Predicci√≥n de rendimientos de cultivos</figcaption>
</figure>


En resumen, la motivaci√≥n detr√°s de los m√©todos de regresi√≥n radica en
la necesidad de entender, modelar, predecir y cuantificar las relaciones
entre variables en una variedad de campos. Estos m√©todos permiten tomar
decisiones informadas, realizar an√°lisis profundos y obtener informaci√≥n
valiosa a partir de los datos disponibles.


# Ejercicio de introducci√≥n.

## Lectura de datos

A continuaci√≥n, cargaremos dos tablas de datos que contienen informaci√≥n sobre el precio del dolar, tambi√©n conocida como tasa representativa del mercado, y otra con la informaci√≥n del precio del aguacate Hass tomada de una fuente gubernamental y de la p√°gina del SIPSA.

Los enlaces para acceder a informaci√≥n relacionada son:

https://www.agronet.gov.co/estadistica/Paginas/home.aspx?cod=11
https://www.bde.es/webbe/es/estadisticas/temas/tipos-cambio.html


```{r}
# Cargue y limpieza de datos.

dolar <- read.csv2("datasets/Tasa_de_Cambio_Representativa_del__Mercado_-Historico.csv") %>% 
            clean_names()
hass <- read.xlsx("datasets/Hass_Precios_Historicos.xlsx") %>% clean_names()
```

## Creaci√≥n de funciones.

Se programa una funci√≥n de flextable que mejorar√° la impresi√≥n de tablas de resumen.
Para usarla s√≥lo bastar√° invocar la funci√≥n ftable() al final de la sentencia (usando %>% tidyverse) o con el ftable() encapsular o encerrar la tabla que se quiere ajustar.

```{r}
# Funciones

## Funci√≥n para crear flextable
ftable <- function(x) {
  x %>% 
    flextable() %>% 
    theme_vanilla() %>%
    color(part = "footer", color = "#666666") %>%
    color( part = "header", color = "#FFFFFF") %>%
    bg( part = "header", bg = "#2c7fb8") %>%
    fontsize(size = 11) %>%
    font(fontname = 'Calibri') %>%
    # Ajustes de ancho y tipo de alineaci√≥n de las columnas
    set_table_properties(layout = "autofit") %>% 
    # width(j=1, width = 3) %>%
    align(i = NULL, j = c(2:ncol(x)), align = "right", part = "all")
}
```

Como ejemplo del uso de la funci√≥n:

*Sin flextable*

```{r}
dolar %>% 
  head(5) 
```

*Con flextable*

```{r}
dolar %>% 
  head(5) %>% 
  ftable()
```


# DEPURACI√ìN DE LOS DATOS

## Dolar

Como en todo proceso de an√°lisis de datos, es necesario realizar una depuraci√≥n y ajuste de los datos. Para este caso, fue necesario trabajar sobre la variable de la fecha y el valor.

```{r}
# Vamos a sacar un valor promedio de cada variable por mes
#Para el dolar vamos a tomar el campo vigenciadesde

dolar <- dolar %>% 
    mutate(fecha = as.Date(vigenciadesde, format = "%d/%m/%y")) %>% 
    mutate(mes = month(fecha), anio = year(fecha)) %>% 
    mutate(valor = as.double(str_replace(valor,",",""))) %>% 
    group_by(anio,mes) %>%
    summarise(precio_dolar = mean(valor), .groups = "drop") 
```

A continuaci√≥n se muestra una fracci√≥n de los datos depurados.

```{r}
dolar %>%
  datatable()
```

## Aguacate

```{r}
hass %>% 
  head(5) %>% 
  ftable()
```


```{r}

# üñáÔ∏è Se genera el vector de meses para usarlo m√°s abajo con la funci√≥n match.
meses <- c("Enero", "Febrero", "Marzo", "Abril", "Mayo",
           "Junio", "Julio", "Agosto", "Septiembre", "Octubre",
           "Noviembre", "Diciembre")

hass <- hass %>% 
      # Otra forma de sacar el anio.
      #mutate(anio = substring(fecha,nchar(fecha)-4,nchar(fecha))) %>% 
      #mutate(espacio = grepl(", ",fecha))
      mutate(mes = sapply(strsplit(fecha, " "), "[", 2),
             anio = sapply(strsplit(fecha, " "), "[", 5)) %>% 
      mutate(anio = as.double(anio)) %>% 
      # üñá Se utiliza la funci√≥n match con el vector meses.
      mutate(mes = match(mes,meses)) %>% 
      group_by(anio,mes) %>%
      summarise(precio_aguacate_kg = mean(precio_kg), .groups = "drop") 
```

```{r}

hass %>% datatable()

```

# Datos unidos

```{r}

hass_dolar <- dolar %>% 
  right_join(hass, by = c("mes","anio")) 

hass_dolar %>% 
  head(5) %>% 
  ftable()
```

# Regresi√≥n Lineal con los datos.

## Gr√°fico de dispersi√≥n

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ theme_light()
```

C√≥mo se puede notar uno intuye que hay alguna relaci√≥n entre ambas variables en la medida en que parece suceder que cuando el precio del d√≥lar aumenta, tambi√©n lo hace el precio de aguacate. La relaci√≥n no es perfecta pues no vemos una curva suave que sea capaz de pasar por todos los puntos, m√°s bien debemos reconocer que hay ciertas variaciones en el proceso. Estas variaciones pueden deberse a factores no contemplados por el modelo, despu√©s de todo no podemos esperar que el precio del aguacate dependa exclusivamente del precio del d√≥lar. 

## Generando un modelo s√∫per simplificado

Ahora vamos a generar el modelo m√°s sencillo que podamos imaginar para predecir el precio del aguacate, al cual llamaremos un modelo ingenuo. La raz√≥n del apelativo es que el modelo "ingenuamente" supondr√° que es posible predecir el precio que tendr√° el aguacate con el promedio hist√≥rico. 

La siguiente gr√°fica muestra la linea horizontal que simboliza el promedio hist√≥rico del precio del aguacate. Tambi√©n debe notar que predecir basado en esta l√≠nea es despreciar cualquier aporte de la variable precio del d√≥lar en la predicci√≥n, es decir, es un modelo que no explota la relaci√≥n (sea esta d√©bil o sea esta fuerte) entre el precio del aguacate y su variable explicativa precio del d√≥lar. 

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ 
  geom_hline(yintercept = mean(hass_dolar$precio_aguacate_kg), color = "red")+
  geom_text(aes(x= 5700, y = mean(hass_dolar$precio_aguacate_kg),
                label = paste("Precio promedio hist√≥rico: $",
                              round(mean(hass_dolar$precio_aguacate_kg), 2))),
    hjust = 1.2, vjust = -0.2, color = "red"
  ) +
  theme_light()
```

Observe que el modelo ingenuo no ser√≠a tan inadecuado en dos circunstancias:

- Si los datos se encuentran muy cercanos al precio hist√≥rico de manera consistente sin importar en que valor del precio del d√≥lar me posicione. Lo que vendr√≠a a indicar un escenario donde moverme a lo largo de diferentes valores del precio del d√≥lar no genera ning√∫n patr√≥n de distanciamiento frente al valor hist√≥rico de referencia.

- Si los datos del precio del aguacate se alejan de la l√≠nea hist√≥rica de referencia pero el precio del d√≥lar tampoco puede seguirlos, es decir, si la relaci√≥n entre precio del d√≥lar y precio del aguacate se parece a una nube de puntos sin ning√∫n patr√≥n o tendencia evidente. En este caso cualquier cosa que supongamos de la relaci√≥n entre precio de d√≥lar y precio del aguacate ser√° producto de nuestra imaginaci√≥n :)

En estos dos escenarios es mejor retener el modelo ingenuo a falta de una variable que de verdad explique lo que pasa con el precio del aguacate.

Ahora bien, ninguno de los dos escenarios anteriores parece ser el nuestro, m√°s bien ac√° se nota que el precio del d√≥lar si est√° acompasado con el precio del aguacate. Sin embargo antes de entrar en la b√∫squeda de esas relaciones es importante notar dos cosas:

- El modelo ingenuo es mi modelo de referencia, esto significa que si un modelo predictivo construido para predecir el precio del aguacate es mejor, lo tendr√° que ser respecto a este modelo ingenuo.

- Es posible medir el desajuste actual de mi modelo ingenuo tomando la distancia de cada punto a la recta horizontal de promedio hist√≥rico, esto es:

Sea $e_i = y_i - \bar{y}$ las diferencia entre el dato del precio de aguacate $y_i$ y el promedio hist√≥rico $\bar{y}$. Como tengo muchas diferencias, ser√° necesario definir una m√©trica resumen, por conveniencia elijamos la suma de los cuadrados de las diferencias, debido a que si sumamos las diferencias, √©stas se me compensar√°n, pues diferencias negativas cancelar√°n las positivas, y no quiero esto. M√°s bien me interesa que ambas sumen a mi medida de desajuste. Llamemos a esta medida la suma de cuadrados del error:

$$SCE_{\text{ingenuo}} = \sum^{n}_{i=1}e_i¬≤=\sum^{n}_{i=1}(y_i - \bar{y})¬≤$$
En la definici√≥n de la cantidad quisimos usar el sub√≠ndice "ingenuo" para nombra al SCE, esto con el fin de hacer √©nfasis que tal m√©trica se asocia al modelo, si el modelo cambia a otro tipo de modelo el SCE cambiar√° tambi√©n: ser√° la distancia de cada $y_i$ a la curva definida por el otro modelo. Veremos esto m√°s adelante. No obstante ya podemos sacar nuestras primeras conclusiones:

1. Entre m√°s grandes sean cada una de las distancias $e_i$ individuales, m√°s grande ser√° SCE.
2. Todas las distancias contribuyen con igual importancia al SCE, excepto en lo que se refiere a su magnitud no hay porque pensar que un distanciamiento de $u$ unidades se m√°s importante que otro de las mismas $u$ unidades. Esto puede ser obvio, pero no lo es cuando queremos castigar m√°s las distancias que se dan en la zona central del gr√°fico que las que se dan en los extremos del gr√°fico, por ejemplo. 
3. El SCE es sensible a la cantidad de datos, raz√≥n por la cual entre m√°s datos (m√°s grande sea n), m√°s grande ser√° SCE. Esto dificulta seriamente la posibilidad de comparar modelos que fueron calculados sobre una cantidad distinta de puntos. 

De acuerdo con lo anterior modifiquemos un poco nuestra definici√≥n de desajuste, as√≠:

$$MSE_{\text{ingenuo}} =  \frac{1}{n}\sum^{n}_{i=1}(y_i - \bar{y})¬≤$$

Es decir, nuestra medida de desajuste ser√° el promedio de la suma de las diferencias cuadradas de cada dato $y_i$ respecto a su media $\bar{y}$, dicho de otra manera la varianza de la variable $Y$ a predecir!!!!

A continuaci√≥n invitamos al estudiante a calcular la varianza de la variable a predecir,
es decir, el desajuste del modelo ingenuo.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.

# üí° Recordemos que la varianza de los residuales respecto del modelo ingenuo, es la misma varianza de la variable a predecir.



```

Observe el siguiente gr√°fico donde adem√°s hemos pintado las distancias que hay entre cada dato $y_i$ observado y el promedio hist√≥rico $\bar{y}$:

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ 
  geom_hline(yintercept = mean(hass_dolar$precio_aguacate_kg))+
  geom_segment(aes(xend=precio_dolar, yend=mean(precio_aguacate_kg)),
               col='red', lty='dashed')+
  theme_light()
```

## Modelo

Este es el momento de comenzar a usar la informaci√≥n extra con la que contamos, esto es, el precio del d√≥lar, la cual se convertir√° en una variable predictora del precio del aguacate. En estad√≠stica no hay una √∫nica forma de hacer esto, pero lo usual es partir de especificaciones muy sencillas. 


```{r}

# lm(formula, data, subset, weights, na.action,
#    method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
#    singular.ok = TRUE, contrasts = NULL, offset, ...)


mod1 <- hass_dolar %>% lm(precio_aguacate_kg ~ precio_dolar,.)

summary(mod1)

# Generando el modelo con R Base ser√≠a as√≠:
# mod1 <- lm(promedio_aguacate_kg ~ promedio_dolar, hass_dolar)

```

¬øCu√°l es la representaci√≥n funcional del modelo?

```{r}




```



```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ theme_light()+
  geom_smooth(method='lm', formula=y~x, se=FALSE, col='dodgerblue1')
```

# Varianza de los residuales del modelo



# Obteniendo los coeficientes.

```{r}
mod1$coefficients
```

# Valores ajustados - Fitted values.

```{r}
mod1$fitted.values %>% 
  head(20)
```

# Residuales

```{r}
mod1$residuals %>% 
  head(20)

# Los valores ajustados y los residuales tambi√©n se pueden recuperar usando las funciones fitted( ) y residuals( ). Consulte la ayuda de estas funciones para conocer otros detalles.

```


Calcular la varianza de los residuales.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.



```

# Porcentaje de variabilidad no explicada por el modelo.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.




```

# Diagrama de dispersi√≥n con los puntos originales

## Creaci√≥n de la columna de predicci√≥n

En este chunk trabajamos con el modo de escritura Tidyverse, aunque se
muestra c√≥mo ser√≠a con R base.

```{r}

#hass_dolar$predicciones <- predict(mod1)

hass_dolar <- hass_dolar %>% 
                mutate(predicciones = predict(mod1))
                

```

```{r}


hass_dolar %>% 
ggplot(aes(x = precio_dolar, y = precio_aguacate_kg)) +
  geom_smooth(method = "lm", se = FALSE, color="lightblue") +
  geom_segment(aes(xend=precio_dolar, yend=predicciones),
               col='red', lty='dashed') +
  geom_point() +
  geom_point(aes(y=predicciones), col='red') +
  theme_light()


#Con R Base
# ggplot(datos, aes(x=Edad, y=Resistencia)) +
#   geom_smooth(method="lm", se=FALSE, color="lightgrey") +
#   geom_segment(aes(xend=Edad, yend=predicciones), col='red', lty='dashed') +
#   geom_point() +
#   geom_point(aes(y=predicciones), col='red') +
#   theme_light()

```

# REVISI√ìN B√ÅSICA DE CONCEPTOS - Simulaci√≥n.

Vamos a simular un modelo de regresi√≥n cuya especificaci√≥n funcional es la siguiente:

$$ y = \beta_0 + \beta_1 * x + e $$

Con $e \text{~} N(0,\sigma^2)$


Como se puede notar, todo modelo teorico se compone de dos t√©rminos:

1) El valor E(y|x)= E_y_x (valor esperado de y dado x)
2) El componente aleatorio tambi√©n llamado error.

Aunque en el modelo anterior hemos elegido una estructura lineal para representar E(y|x) en realidad podemos elegir cualquier otra estructura alternativa, siempre que respetemos la linealidad en los betas.

```{r}

bo = 2
b1_1 = 3
b1_2 = 0.5
x <- seq(1,10)

# Estructura para el valor esperado de y dado x ()
# Esta estructura admitir√≠a otras formas funcionales

# El valor esperado es deterministico.
E_y_x_1 <- bo + b1_1 * x

E_y_x_2 <- bo + b1_1 * x + b1_2 * x^2


# el valor observado o valor real, es aleatorio.



y_obs_1 <- E_y_x_1 + rnorm(10, 0, 4)

y_obs_2 <- E_y_x_2 + rnorm(10, 0, 4)

datos_simulados <- data.frame(
  x = x,
  x_2 = x^2,
  E_y_x_1 = E_y_x_1,
  E_y_x_2 = E_y_x_2,
  y_obs_1 = y_obs_1,
  y_obs_2 = y_obs_2,
  y_pred_1 = predict(lm(y_obs_1 ~ x)),
  y_pred_2 = predict(lm(y_obs_2 ~ x+ x^2)))

  
```

# Dibujamos un gr√°fico de dispersi√≥n

```{r}


datos_simulados %>% 
ggplot(aes(x = x, y = y_obs_1)) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE, color="lightblue") +
  geom_point(col = "green")+
  geom_point(aes(y=y_obs_2), col='red')+ 
  geom_smooth(method="lm", se= FALSE ,formula=y_obs_2~poly(x, 2),color = "blue")+
  ylab("Y")

  

```

# DISCUTIENDO SOBRE LA LINEALIDAD DE LOS BETAS

En esta secci√≥n vamos a aclarar el asunto de que los BETAS sean lineales.

La linealidad en este contexto hace referencia a que no tengan potencias y que los **coeficientes sean independendientes entre ellos**.

Supongamos un modelo con coeficientes repetidos.

$$E(y|x) = \beta_0 + \alpha * x + \alpha * x^2 $$


Debemos factorizarlos para evitar estimarlos por separado y producir inconsistencias,esto debido a que ambos alpha (coeficientes) son dependientes.Es decir, reescribimos:

$$E(y|x) = \beta_0 + \alpha (x + x^2) $$ 

Donde el componente x + x¬≤ ser√° el vector de valores de la variable predictora, que puede pensarse como una variable nueva:

$$ \tilde{x} = x + x^2$$
Y por lo tanto pensar al modelo como:

$$ E(y|x) = \beta_0 + \alpha * \tilde{x}$$
Esta transformaci√≥n garantiza la creaci√≥n de un nuevo modelo lineal respecto a $\beta_0$ y $\alpha$.

Es importante anotar que el dataframe que alimentar√° el modelo deber√° tener computada la variable auxiliar $\tilde{x}$, la cual debe ser pasada a la funci√≥n encargada de estimar par√°metros.

Esto corresponde a crear una nueva variable con los c√°lculos mencionados $\tilde{x} = x + x^2$.
Sin embargo, para graficar es conveniente usar la variable original.



```{r}

# Se define una semilla para generar los aleatorios.
set.seed(77)

alpha <- 0.7

E_y_x_3 = bo + alpha * (x + x^2)
y_obs_3 = E_y_x_3 + rnorm(10, 0, 4)

x_auxiliar = x + x^2

datos_simulados_2 <- data.frame(
  x = x,
  x_2 = x^2,
  x_auxiliar = x_auxiliar,
  E_y_x_3 = E_y_x_3,
  y_obs_3 =  y_obs_3,
  y_pred_3_1 = predict(lm(y_obs_3 ~ poly(x, 2))),
  y_pred_3_2 = predict(lm(y_obs_3 ~ x_auxiliar )))


modelo1a <-  lm(y_obs_3 ~ poly(x, 2))
modelo1b <-  lm(y_obs_3 ~ poly(x, 2, raw = TRUE))
modelo2 <-  lm(y_obs_3 ~ x_auxiliar )



```


Notese que los coeficientes arrojados por el modelo1a al invocar la funci√≥n poly sin par√°metro de raw=TRUE, no son atribuibles o no est√°n asociados a los t√©rminos $1,x,x^2$, es decir, **No es v√°lido escribir**:


$$E(y|x)= `r round(modelo1a$coefficients[1], 2)` * 1 + `r round(modelo1a$coefficients[2], 2)` * x + `r round(modelo1a$coefficients[3], 2)` * x^2$$
Esta ecuaci√≥n no ser√≠a v√°lida porque al remplazar para $x=5$ se obtiene $E(y|5)=$ `r round(modelo1a$coefficients[1] * 1 + modelo1a$coefficients[2] * 5 + modelo1a$coefficients[3] * 5^2, 2)`, mientras que al remplazar en el modelo con el par√°metro raw = TRUE, se obtendr√≠a: $E(y|5)=$ `r round(modelo1b$coefficients[1] * 1 + modelo1b$coefficients[2] * 5 + modelo1b$coefficients[3] * 5^2, 2)`.

Observe que para el valor $x=5$ los valores observados de $y$ est√°n cercanos a 24 por lo que una predicci√≥n arrojando un valor cercano a 836, est√° claramente desfasada. Observe el gr√°fico m√°s abajo para comprender lo explicado.

Esto se debe a que los coeficientes anclados al modelo_1a est√°n dise√±ados para ser usados sobre la siguiente base de polinomios ortogonales:

- $P_1(x)= a$
- $P_2(x)=m*x - c$
- $P_3(x)= k_1 + k_2 *x + k_3 * x ^2$

Donde $a,m,c,k_1,k_2,k_3$ son par√°metros estimados que lamentablemente no est√°n siendo arrojados por el modelo.

A continuaci√≥n se muestra el resumen de cada uno de los modelos, utilizando la funci√≥n summary().

```{r}


summary(modelo1a)
summary(modelo1b)
summary(modelo2)




```

```{r}

datos_simulados_2 %>% 
  ggplot(aes(x = x , y = y_obs_3))+
  geom_point(col = 'green')+
  geom_point(aes(y = y_pred_3_1), col = 'blue')+
  geom_point(aes(y = y_pred_3_2), col = 'red')


```


```{r eval=FALSE, include=FALSE}

# TAREA,
# Buscar la descomposici√≥n ortogonal de los vectores.

u1 = 1/sqrt(2)
u2 = sqrt(3/2) * (5-1)
u3 = sqrt(5/2) * ((3/2)*(5^2) - 3*5 + 1)

#E_y_5 = coef1 * u1 + coef2*u2 + coef3*u3
E_y_5 = modelo1a$coefficients[1] * u1 + modelo1a$coefficients[2]*u2 + modelo1a$coefficients[3]*u3

modelo1a$coefficients[1]
modelo1a$coefficients[2]
modelo1a$coefficients[3]

E_y_5

```


# ESTIMACI√ìN DE PAR√ÅMETROS 

## Por m√°xima verosimilitud.

## Por m√≠nimos cuadrados.


```{r}


n <- 1000

vector_x <- c(1: n ) 
sigma <- 4 
beta_0 <- 2
beta_1 <- 0.5

# Esperada de y dado x
E_y_x <- beta_0 + (beta_1 * vector_x )

print(E_y_x)

plot(vector_x,E_y_x)



```

```{r}


error <- rnorm(n = n, mean = 0, sd = sigma)

y_observado <- beta_0 + (beta_1 * vector_x ) + error

print(y_observado)

plot(vector_x,y_observado)


```

# Modelo lineal

```{r}

modelo_y_predicho <- lm(y_observado ~ vector_x)

y_predicho <- predict(modelo_y_predicho)

print(y_predicho)


```

# Uni√≥n en un dataframe

```{r}

datos_modelo <- as.data.frame(vector_x) %>% 
                    mutate(E_y_x = E_y_x,
                           y_observado = y_observado,
                           y_predicho = y_predicho)

```

# Gr√°ficando los valores

```{r}

datos_modelo %>% 
  ggplot(  )+
  geom_point( aes( x= vector_x, y = E_y_x), col = "blue")+
  geom_point( aes( x= vector_x, y = y_observado), col = "red")+
  geom_point( aes( x= vector_x, y = y_predicho), col = "brown")+
  geom_line(aes( x= vector_x, y = E_y_x), col = "blue")+
  geom_line(aes( x= vector_x, y = y_predicho), col = "black")

```

# Diferencia entre esperado y el predicho (Error)

```{r}


modelo_y_predicho$coefficients

b_0_estimado <- modelo_y_predicho$coefficients[1]
b_1_estimado <- modelo_y_predicho$coefficients[2]



datos_modelo <- datos_modelo %>% 
                  mutate(error_predicho_observado = y_predicho - y_observado) 
                  


```

# Construir funci√≥n

```{r}


estimar_betas <-  function(n,b0,b1,sigma,distribucion){
  
  
  if (distribucion == "normal") {
  
    errores <- rnorm(40,0,4)
      
  } else
    
  { errores <- runif(40,-6.928203,6.928203)  }
  
  
  vector_x <- c(1: n) 
  E_y_x <- b0 + (b1 * vector_x )
  
  #error <- rnorm(n = n, mean = 0, sd = sigma)
  #error <- runif(n = n, min=-6.928203, max = 6.928203)
  
  
  y_observado <- E_y_x + errores
  modelo <- lm(y_observado ~ vector_x)
  y_predicho <- predict(modelo)
  
  b0_estimado <- modelo$coefficients[1]
  b1_estimado <- modelo$coefficients[2]
  
  return(list("b0_estimado" = b0_estimado,
              "b1_estimado" = b1_estimado))

}



```

# Usando la funci√≥n

```{r}

resultados1 <- estimar_betas(40,beta_0,beta_1,sigma,"normal")
# resultados2 <- estimar_betas(40,beta_0,beta_1,sigma)
# resultados3 <- estimar_betas(40,beta_0,beta_1,sigma)

print(resultados1)
# print(resultados2)
# print(resultados3)


```

# Creando vector BETAS

```{r}

vector_betas <- function(n,beta_0,beta_1,sigma, n_sim, distribucion) {
  
  vector_b0 <- vector()
  vector_b1 <- vector()
  
  for (i in 1:n_sim) {
    
    resultados <-  estimar_betas(n,beta_0,beta_1,sigma,distribucion)
  
    vector_b0[i] <- resultados$b0_estimado
    vector_b1[i] <- resultados$b1_estimado
    
  }
  
  df_resultados <- data.frame(
                      b0 = vector_b0,
                      b1 = vector_b1)
  
 

  return(df_resultados)

    }


```

```{r}


df_resultados1 <- vector_betas(40,2,2,4,1000,"normal")
df_resultados2 <- vector_betas(40,2,2,4,1000,"runif")


```

# Histograma comparando distribuciones.

```{r}

 histo_b0_normal <- df_resultados1 %>% 
                ggplot()+
                geom_histogram(aes(x = b0))+
                ylim(0,110)
  
  histo_b0_uniforme <- df_resultados2 %>% 
                ggplot()+
                geom_histogram(aes(x = b0))+
                ylim(0,110)
  
  grid.arrange(histo_b0_normal,
               histo_b0_uniforme, 
               ncol = 2)


```

```{r}

 histo_b0_normal <- df_resultados1 %>% 
                ggplot()+
                geom_histogram(aes(x = b0))+
                ylim(0,110)
  
  histo_b1_normal <- df_resultados1 %>% 
                ggplot()+
                geom_histogram(aes(x = b1))+
                ylim(0,110)
  
  grid.arrange(histo_b0_normal,
               histo_b1_normal, 
               ncol = 2)


```

```{r include=FALSE}

# Este chunk se configur√≥ para no imprimirse

vector_betas(100,2,2,4,1000,"normal")



```
