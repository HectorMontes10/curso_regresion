---
title: "PRIMERA CLASE: REGRESI√ìN LINEAL"
subtitle: "Maestr√≠a en Investigaci√≥n Operativa y Estad√≠stica"
author:
  - "Juli√°n Piedrah√≠ta Monroy"
  - "H√©ctor Hern√°n Montes"
output: 
  rmdformats::readthedown:
    css: styles.css
  bookdown::html_document2:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
date: "2023"
editor_options: 
  markdown: 
    wrap: 72
  wrap: 72
chunk_output_type: inline
---

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
```
<div>

<img src="https://media2.utp.edu.co/imagenes/Logo-UTP-Azul.png" alt="UTP" class="watermark"/>

</div>

```{r include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = TRUE,
  message = FALSE,
  options(scipen=999)
)

# Librer√≠as
library(tidyverse)
library(janitor)
library(openxlsx)
library(flextable)
# library(kableExtra)
library(viridis)
library(scales)
library(DT)
library(lubridate)
library(gridExtra)
library(ggplot2)
library(gganimate)
library(animation)
library(gifski)
library(magick)
library(nortest)
library(fitdistrplus)

```

# INTRODUCCI√ìN

## DOCENTES

### H√©ctor Hern√°n Montes Garc√≠a.

Ingeniero industrial de la Universidad Tecnol√≥gica de Pereira.

Estudios en Maestr√≠a en Ciencias con orientaci√≥n en Matem√°ticas

[Desempe√±o laboral]{style="color: red;"}

Cient√≠fico de datos con m√°s de 3 a√±os de experiencia en la construcci√≥n
de modelos de aprendizaje autom√°tico y soluciones de miner√≠a de datos
para obtener mejores conocimientos comerciales. Experiencia utilizando
SQL, bibliotecas de Python (matplotlib, seaborn, flask, scikit-learn,
pandas, numpy), modelos matem√°ticos y estad√≠sticos para ofrecer
soluciones robustas que agregan valor al negocio.

Ha trabajado para clientes del sector industrial y financiero en M√©xico
y Colombia, en √°reas tales como:

-   Mantenimiento predictivo
-   Dise√±o de campa√±as comerciales basadas en datos
-   Reconocimiento de im√°genes
-   Modelos de procesamiento de lenguaje natural (NER).

### Juli√°n Piedrah√≠ta Monroy

Ingeniero industrial.

Mag√≠ster en desarrollo agroindustrial

Universidad Tecnol√≥gica de Pereira

[Desempe√±o laboral]{style="color: red;"}

-   Consultor en soluciones an√°liticas para instituciones educativas.
-   Analista de datos en el observatorio social de la UTP.
-   Actualmente dise√±ador de tableros informativos (dashboards) para la
    Universidad Pedag√≥gica Nacional de Bogot√° y la misma Universidad
    Tecnol√≥gica de Pereira.
-   Docente catedr√°tico en el √°rea de inform√°tica y estad√≠stica general.
-   Uso principal del lenguaje R y sus librerias Tidyverse, RMarkdown,
    RShiny y otras.

## Contenido del curso

-   T1. Regresi√≥n Lineal, suposiciones y requisitos.
-   T2. Estimaci√≥n de los par√°metros e interpretaci√≥n de los valores.
-   T3. Pruebas de hip√≥tesis relacionadas con los par√°metros.
-   T4. Evaluaci√≥n de modelos de acuerdo a las suposiciones de
    normalidad e independencia.
-   T5. Transformaciones de las variables, y sus implicaciones en las
    pruebas de hip√≥tesis.
-   T6. Series de Tiempo. Estacionariedad y c√≥mo obtener una serie
    estacionaria a partir de una que no lo es. Correlogramas.
-   T7. Modelos de Box y Cox. Estimaci√≥n de par√°metros.
-   T8. Criterios de evaluaci√≥n de un modelo de series de tiempo.
    Transformaciones.

## Bibliograf√≠a.

Regression Modeling and Data Analysis with Applications in R. Samprit
Chatterjee, Jeffrey S. Simonoff\
Montgomery, D. C., Peck, E. A., & Vining, G. G. (2002). Introducci√≥n al
an√°lisis de regresi√≥n lineal (3¬™ ed.). Limusa Wiley.\
Time Series Analysis and Its Applications: With R Examples by Shumway
and Stoffer.\
Time Series Analysis: Forecasting and Control by Author(s): George E. P.
Box, Gwilym M. Jenkins, Gregory C. Reinsel, Greta M. Ljung.\
<https://fhernanb.github.io/libro_regresion/rls.html#modelo-estad%C3%ADstico>\
<https://bookdown.org/victor_morales/SeriesdeTiempo/>

## Motivaci√≥n

Los m√©todos de regresi√≥n son t√©cnicas estad√≠sticas utilizadas para
modelar y comprender la relaci√≥n entre variables. La motivaci√≥n detr√°s
de estos m√©todos radica en la necesidad de analizar y cuantificar c√≥mo
una o m√°s variables predictoras influyen en una variable respuesta. Los
m√©todos de regresi√≥n son ampliamente utilizados en diversos campos,
incluyendo la investigaci√≥n cient√≠fica, la econom√≠a, la medicina, la
ingenier√≠a y m√°s. A continuaci√≥n, se detallan algunas de las principales
motivaciones detr√°s de estos m√©todos:

<figure>

<img src="Imagen de regresi√≥n lineal 1.jpeg"/>

<figcaption>Img 1: Ejemplo de fen√≥meno lineal</figcaption>

</figure>

[Modelado de Relaciones:]{style="color: red;"}

La motivaci√≥n fundamental de la regresi√≥n es modelar la relaci√≥n entre
variables. En muchos casos, existe la necesidad de entender c√≥mo una
variable dependiente cambia en funci√≥n de una o m√°s variables
independientes. Por ejemplo, en la econom√≠a, podr√≠a ser necesario
entender c√≥mo las tasas de inter√©s afectan el gasto del consumidor.

[Predicci√≥n y Estimaci√≥n:]{style="color: red;"} Los modelos de regresi√≥n
tambi√©n se utilizan para hacer predicciones y estimaciones. Dado un
conjunto de datos hist√≥ricos, los modelos de regresi√≥n pueden ayudar a
predecir valores futuros de la variable dependiente en funci√≥n de los
valores de las variables independientes. Esto es especialmente √∫til en
campos como la meteorolog√≠a, la finanzas y el marketing.

<figure>

<img src="Imagen de regresi√≥n lineal 2.jpeg"/>

<figcaption>Img 2: Prediciendo la tendencia hist√≥rica del
S&P</figcaption>

</figure>

[Control y Optimizaci√≥n:]{style="color: red;"} En algunos casos, se
utilizan modelos de regresi√≥n para optimizar y controlar procesos. Por
ejemplo, en la manufactura, los modelos de regresi√≥n pueden usarse para
identificar las condiciones ideales que conducen a la m√°xima eficiencia
o calidad del producto.

[Validaci√≥n de Teor√≠as:]{style="color: red;"} En la investigaci√≥n
cient√≠fica y social, los modelos de regresi√≥n pueden usarse para validar
o refutar teor√≠as existentes. Al comparar los resultados del modelo con
las expectativas te√≥ricas, se puede evaluar la validez de las hip√≥tesis.

[Gesti√≥n de Riesgos:]{style="color: red;"} Los modelos de regresi√≥n
tambi√©n se utilizan para evaluar riesgos y tomar decisiones informadas.
En finanzas, por ejemplo, se pueden utilizar para evaluar el riesgo de
inversi√≥n y la exposici√≥n a diferentes factores del mercado.

**Ejemplo**

El modelo CAPM (Capital Asset Pricing Model) es un modelo de valoraci√≥n
de activos financieros desarrollado por William Sharpe que permite
estimar su rentabilidad esperada en funci√≥n del riesgo sistem√°tico. Su
desarrollo est√° basado en diversas formulaciones de Harry Markowitz
sobre la diversificaci√≥n y la teor√≠a moderna de portafolios. Dos puntos
claves son:

1.  El modelo CAPM es utilizado para calcular la rentabilidad que un
    inversionista debe exigir al realizar una inversi√≥n en un activo
    financiero en funci√≥n del riesgo que est√° asumiendo.
2.  El modelo CAPM establece una relaci√≥n lineal entre el rendimiento
    esperado de un activo y su riesgo sistem√°tico, medido por su
    $\beta_i$.

La f√≥rmula del modelo CAPM es la siguiente:

$E(r_i)=r_f+\beta_i*(E(r_m)-r_f)$

Donde:

$E(r_i)$ es la tasa de rentabilidad esperada de un activo concreto. $rf$
es la rentabilidad del activo sin riesgo. $\beta_i$ es la medida de la
sensibilidad del activo respecto a su benchmark. $E(r_m)$ es la tasa de
rentabilidad esperada del mercado en que cotiza el activo.

<figure>

<img src="Imagen de regresi√≥n lineal 3.jpeg"/>\

<figcaption>Img 3: Modelo CAMP para valoraci√≥n de activos
financieros</figcaption>

</figure>

[An√°lisis de Causa y Efecto:]{style="color: red;"} Los modelos de
regresi√≥n pueden ayudar a establecer relaciones causa-efecto entre
variables. Esto es √∫til para comprender c√≥mo los cambios en una variable
influyen en otras variables y viceversa.

<figure>

<img src="Imagen de regresi√≥n lineal 4.png"/>

<figcaption>Img 4: Predicci√≥n de rendimientos de cultivos</figcaption>

</figure>

En resumen, la motivaci√≥n detr√°s de los m√©todos de regresi√≥n radica en
la necesidad de entender, modelar, predecir y cuantificar las relaciones
entre variables en una variedad de campos. Estos m√©todos permiten tomar
decisiones informadas, realizar an√°lisis profundos y obtener informaci√≥n
valiosa a partir de los datos disponibles.

# 1. INTRODUCCI√ìN NO FORMAL AL MODELO LINEAL SIMPLE

En este cap√≠tulo introduciremos el modelo lineal simple partiendo de
datos reales del precio del aguacate y el precio del d√≥lar. M√°s que una
presentaci√≥n formal o matem√°tica del modelo, nuestro objetivo ser√°
construirlo paso a paso usando las ideas estad√≠sticas detr√°s del m√©todo.
Confiamos en que esto le al lector una idea m√°s precisa de c√≥mo √©ste
modelo usa los datos dados para relacionar las variables de inte≈ïes, y
que comprenda sus fortalezas pero tambi√©n sus limitaciones.

## 1.1 Lectura de datos

A continuaci√≥n, cargaremos dos tablas de datos que contienen informaci√≥n
sobre el precio del dolar, tambi√©n conocida como tasa representativa del
mercado, y otra con la informaci√≥n del precio del aguacate Hass tomada
de una fuente gubernamental y de la p√°gina del SIPSA.

Los enlaces para acceder a informaci√≥n relacionada son:

<https://www.agronet.gov.co/estadistica/Paginas/home.aspx?cod=11>
<https://www.bde.es/webbe/es/estadisticas/temas/tipos-cambio.html>

```{r}
# Cargue y limpieza de datos.

dolar <- read.csv2("datasets/Tasa_de_Cambio_Representativa_del__Mercado_-Historico.csv") %>% 
  clean_names()
hass <- read.xlsx("datasets/Hass_Precios_Historicos.xlsx") %>% clean_names()
```

## 1.2 Creaci√≥n de funciones.

Se programa una funci√≥n de flextable que mejorar√° la impresi√≥n de tablas
de resumen. Para usarla s√≥lo bastar√° invocar la funci√≥n ftable() al
final de la sentencia (usando %\>% tidyverse) o con el ftable()
encapsular o encerrar la tabla que se quiere ajustar.

```{r}
# Funciones

## Funci√≥n para crear flextable
ftable <- function(x) {
  x %>% 
    flextable() %>% 
    theme_vanilla() %>%
    color(part = "footer", color = "#666666") %>%
    color( part = "header", color = "#FFFFFF") %>%
    bg( part = "header", bg = "#2c7fb8") %>%
    fontsize(size = 11) %>%
    font(fontname = 'Calibri') %>%
    # Ajustes de ancho y tipo de alineaci√≥n de las columnas
    set_table_properties(layout = "autofit") %>% 
    # width(j=1, width = 3) %>%
    align(i = NULL, j = c(2:ncol(x)), align = "right", part = "all")
}
```

Como ejemplo del uso de la funci√≥n:

*Sin flextable*

```{r}
dolar %>% 
  head(5) 
```

*Con flextable*

```{r}
dolar %>% 
  head(5) %>% 
  ftable()
```

## 1.3 Depuraci√≥n de los datos

### Dolar

Como en todo proceso de an√°lisis de datos, es necesario realizar una
depuraci√≥n y ajuste de los datos. Para este caso, fue necesario trabajar
sobre la variable de la fecha y el valor.

```{r}
# Vamos a sacar un valor promedio de cada variable por mes
#Para el dolar vamos a tomar el campo vigenciadesde

dolar <- dolar %>% 
  mutate(fecha = as.Date(vigenciadesde, format = "%d/%m/%y")) %>% 
  mutate(mes = month(fecha), anio = year(fecha)) %>% 
  mutate(valor = as.double(str_replace(valor,",",""))) %>% 
  group_by(anio,mes) %>%
  summarise(precio_dolar = mean(valor), .groups = "drop") 
```

A continuaci√≥n se muestra una fracci√≥n de los datos depurados.

```{r}
dolar %>%
  datatable()
```

### Aguacate

```{r}
hass %>% 
  head(5) %>% 
  ftable()
```

```{r}
# üñáÔ∏è Se genera el vector de meses para usarlo m√°s abajo con la funci√≥n match.
meses <- c("Enero", "Febrero", "Marzo", "Abril", "Mayo",
           "Junio", "Julio", "Agosto", "Septiembre", "Octubre",
           "Noviembre", "Diciembre")

hass <- hass %>% 
  # Otra forma de sacar el anio.
  #mutate(anio = substring(fecha,nchar(fecha)-4,nchar(fecha))) %>% 
  #mutate(espacio = grepl(", ",fecha))
  mutate(mes = sapply(strsplit(fecha, " "), "[", 2),
         anio = sapply(strsplit(fecha, " "), "[", 5)) %>% 
  mutate(anio = as.double(anio)) %>% 
  # üñá Se utiliza la funci√≥n match con el vector meses.
  mutate(mes = match(mes,meses)) %>% 
  group_by(anio,mes) %>%
  summarise(precio_aguacate_kg = mean(precio_kg), .groups = "drop") 
```

```{r}

hass %>% datatable()

```

### Datos unidos

```{r}

hass_dolar <- dolar %>% 
  right_join(hass, by = c("mes","anio")) 

hass_dolar %>% 
  head(5) %>% 
  ftable()
```

## 1.4 Diagrama de dispersi√≥n.

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ theme_light()
```

C√≥mo se puede notar uno intuye que hay alguna relaci√≥n entre ambas
variables en la medida en que parece suceder que cuando el precio del
d√≥lar aumenta, tambi√©n lo hace el precio de aguacate. La relaci√≥n no es
perfecta pues no vemos una curva suave que sea capaz de pasar por todos
los puntos, m√°s bien debemos reconocer que hay ciertas variaciones en el
proceso. Estas variaciones pueden deberse a factores no contemplados por
el modelo, despu√©s de todo no podemos esperar que el precio del aguacate
dependa exclusivamente del precio del d√≥lar.

## 1.5 Generando un modelo s√∫per simplificado (modelo ingenuo)

Ahora vamos a generar el modelo m√°s sencillo que podamos imaginar para
predecir el precio del aguacate, al cual llamaremos un modelo ingenuo.
La raz√≥n del apelativo es que el modelo "ingenuamente" supondr√° que es
posible predecir el precio que tendr√° el aguacate con el promedio
hist√≥rico.

La siguiente gr√°fica muestra la linea horizontal que simboliza el
promedio hist√≥rico del precio del aguacate. Tambi√©n debe notar que
predecir basado en esta l√≠nea es despreciar cualquier aporte de la
variable precio del d√≥lar en la predicci√≥n, es decir, es un modelo que
no explota la relaci√≥n (sea esta d√©bil o sea esta fuerte) entre el
precio del aguacate y su variable explicativa precio del d√≥lar.

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ 
  geom_hline(yintercept = mean(hass_dolar$precio_aguacate_kg), color = "red")+
  geom_text(aes(x= 5700, y = mean(hass_dolar$precio_aguacate_kg),
                label = paste("Precio promedio hist√≥rico: $",
                              round(mean(hass_dolar$precio_aguacate_kg), 2))),
            hjust = 1.2, vjust = -0.2, color = "red"
  ) +
  theme_light()
```

Observe que el modelo ingenuo no ser√≠a tan inadecuado en dos
circunstancias:

-   Si los datos se encuentran muy cercanos al precio hist√≥rico de
    manera consistente sin importar en que valor del precio del d√≥lar me
    posicione. Lo que vendr√≠a a indicar un escenario donde moverme a lo
    largo de diferentes valores del precio del d√≥lar no genera ning√∫n
    patr√≥n de distanciamiento frente al valor hist√≥rico de referencia.

-   Si los datos del precio del aguacate se alejan de la l√≠nea hist√≥rica
    de referencia pero el precio del d√≥lar tampoco puede seguirlos, es
    decir, si la relaci√≥n entre precio del d√≥lar y precio del aguacate
    se parece a una nube de puntos sin ning√∫n patr√≥n o tendencia
    evidente. En este caso cualquier cosa que supongamos de la relaci√≥n
    entre precio de d√≥lar y precio del aguacate ser√° producto de nuestra
    imaginaci√≥n ü§¶ü§¶

En estos dos escenarios es mejor retener el modelo ingenuo a falta de
una variable que de verdad explique lo que pasa con el precio del
aguacate.

Ahora bien, ninguno de los dos escenarios anteriores parece ser el
nuestro, m√°s bien ac√° se nota que el precio del d√≥lar si est√° acompasado
con el precio del aguacate. Sin embargo antes de entrar en la b√∫squeda
de esas relaciones es importante notar dos cosas:

-   El modelo ingenuo es mi modelo de referencia, esto significa que si
    un modelo predictivo construido para predecir el precio del aguacate
    es mejor, lo tendr√° que ser respecto a este modelo ingenuo.

-   Es posible medir el desajuste actual de mi modelo ingenuo tomando la
    distancia de cada punto a la recta horizontal de promedio hist√≥rico,
    esto es:

## 1.6 Calidad de ajuste del modelo ingenuo

Sea $e_i = y_i - \bar{y}$ las diferencia entre el dato del precio de
aguacate $y_i$ y el promedio hist√≥rico $\bar{y}$. Como tengo muchas
diferencias, ser√° necesario definir una m√©trica resumen, por
conveniencia elijamos la suma de los cuadrados de las diferencias,
debido a que si sumamos las diferencias, √©stas se me compensar√°n, pues
diferencias negativas cancelar√°n las positivas, y no quiero esto. M√°s
bien me interesa que ambas sumen a mi medida de desajuste. Llamemos a
esta medida la suma de cuadrados del error:

$$SCE_{\text{ingenuo}} = \sum^{n}_{i=1}e_i¬≤=\sum^{n}_{i=1}(y_i - \bar{y})¬≤$$

En la definici√≥n de la cantidad quisimos usar el sub√≠ndice "ingenuo"
para nombrar al SCE, esto con el fin de hacer √©nfasis que tal m√©trica se
asocia al modelo, si el modelo cambia a otro tipo de modelo el SCE
cambiar√° tambi√©n: ser√° la distancia de cada $y_i$ a la curva definida
por el otro modelo. Veremos esto m√°s adelante. No obstante ya podemos
sacar nuestras primeras conclusiones:

1.  Entre m√°s grandes sean cada una de las distancias $e_i$
    individuales, m√°s grande ser√° SCE.
2.  Todas las distancias contribuyen con igual importancia al SCE,
    excepto en lo que se refiere a su magnitud no hay porque pensar que
    un distanciamiento de $u$ unidades sea m√°s importante que otro de
    las mismas $u$ unidades. Esto puede ser obvio pero no lo es cuando,
    por ejemplo, queremos castigar m√°s las distancias que se dan en la
    zona central del gr√°fico que las que se dan en los extremos del
    gr√°fico.
3.  El SCE es sensible a la cantidad de datos, raz√≥n por la cual entre
    m√°s datos (es decir entre m√°s grande sea n), m√°s grande ser√° SCE.
    Esto dificulta seriamente la posibilidad de comparar modelos que
    fueron calculados sobre una cantidad distinta de puntos.

De acuerdo con lo anterior modifiquemos un poco nuestra definici√≥n de
desajuste, as√≠:

$$MSE_{\text{ingenuo}} =  \color{red}{\frac{1}{n}}\sum^{n}_{i=1}(y_i - \bar{y})¬≤$$

Es decir, nuestra medida de desajuste ser√° el promedio de la suma de las
diferencias cuadradas de cada dato $y_i$ respecto a su media $\bar{y}$,
dicho de otra manera la varianza de la variable $Y$ a predecir!!!!

A continuaci√≥n invitamos al estudiante a calcular la varianza de la
variable a predecir, es decir, el desajuste del modelo ingenuo.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.

# üí° Recordemos que la varianza de los residuales respecto del modelo ingenuo, es la misma varianza de la variable a predecir.



```

Observe el siguiente gr√°fico en el que hemos pintado las distancias que
hay entre cada dato $y_i$ observado y el promedio hist√≥rico (valor
predicho $\bar{y}$ por el modelo ingenuo)

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ 
  geom_hline(yintercept = mean(hass_dolar$precio_aguacate_kg))+
  geom_segment(aes(xend=precio_dolar, yend=mean(precio_aguacate_kg)),
               col='red', lty='dashed')+
  theme_light()
```

## 1.7 Construyendo un modelo lineal simple con los datos

Este es el momento de comenzar a usar la informaci√≥n extra con la que
contamos, esto es, el precio del d√≥lar, el cual se convertir√° en una
variable predictora del precio del aguacate. En estad√≠stica no hay una
√∫nica forma de hacer esto, pero lo usual es partir de especificaciones
muy sencillas. Veamos:

Sea $Y$ el precio del aguacate y sea $X$ el precio del d√≥lar.
Investiguemos si este modelo sencillo nos funciona:

$$y = \beta_{0} + \beta_{1} * x + e \text{ con } e \text{~}N(0, \sigma^2)$$

## 1.8 Algunas observaciones sobre las fortalezas y limitaciones del modelo

Expliquemos qu√© implica la ecuaci√≥n anterior haciendo ciertas
anotaciones de gran relevancia pr√°ctica para entender **c√≥mo los
estad√≠sticos piensan al momento de plantear modelos**:

[Observaci√≥n genial 1:]{style="color: red;"}

Para un x fijo, es decir un x dado, es posible calcular el valor de $y$
con el modelo anterior, donde t√°citamente estamos diciendo que el valor
de $y$ depender√° de $x$ en forma exacta, excepto por una perturbaci√≥n
aleatoria $e$ que representar√° la parte del valor de $y$ que no puede
ser capturada por el t√©rmino $\beta_{0} + \beta_{1} * x$.

¬øCu√°ndo suceder√° que el valor de $y$ pueda ser exactamente capturado por
el t√©rmino $\beta_{0} + \beta_{1} * x$? Cuando $y$ dependa en forma
lineal exacta del valor de $x$. Esta es una condici√≥n demasiado fuerte
que rara vez ocurre en la pr√°ctica, por eso agregamos la perturbaci√≥n
$e$ como una forma de modelar la **incertidumbre** en la determinaci√≥n
de $y$ dado un valor de $x$. Esto no soluciona todos los problemas pero
ayuda a obtener un modelo relativamente m√°s realista.

[¬øCu√°les son los riesgos que implica asumir a priori un comportamiento
espec√≠fico para el error?]{style="color: red;"}

[Observaci√≥n genial 2:]{style="color: red;"}

La perturbaci√≥n $e$ puede recibir varios nombres en estad√≠stica, la
encontrar√°s nombrada como: perturbaci√≥n, choque, error, o residual. Lo
importante no es el nombre que reciba, sino las condiciones que vamos a
suponer sobre sus valores.

En particular no vamos a exigir que $e$ tenga valores predecibles, muy
al contrario vamos a exigir que sea un valor aleatorio, precisamente
porque est√° modelando incertidumbre. Pero esto no implica que no podamos
poner ciertas condiciones sobre el **tipo de aleatoriedad deseada para**
$e$. En este caso supondremos que $e$ se distribuye como una variable
aleatoria normal con media $\mu=0$ y desviaci√≥n est√°ndar $\sigma$.
Conviene aclarar que esto es una suposici√≥n, no implica que
efectivamente los errores vayan a cumplir tal supuesto.

[Observaci√≥n genial 3:]{style="color: red;"}

¬øPor qu√© suponemos normalidad para $e$? Porque si $e$ es en efecto una
especie de componente incierto en el valor de $y$, entonces muy
seguramente ser√° la suma de muchos efectos independientes que lo est√°n
provocando. En nuestro contexto estos efectos inciertos pueden ser:

-   $F_1$: Cambios en la productividad de los cultivos.
-   $F_2$: Condiciones climatol√≥gicas.
-   $F_3$: Precios de los insumos agr√≠colas.
-   $F_4$: Capacidad de negociaci√≥n de los productores.
-   $F_5$: Presencia o no de subsidios estatales.
-   $F_6$: Precios de productos complementarios o sustitutos.

...

-   $F_k$: precio de la gasolina.

Y un gran etc√©tera.

Es decir, existen innumerables factores, distintos al precio del d√≥lar
que impiden que el precio del aguacate pueda ser determinado de forma
exacta s√≥lo por observar el valor del d√≥lar. En estad√≠stica se ha
estudiado que cuando no estamos midiendo los dem√°s factores que afectan
el valor de una variable, y dejamos que estos contribuyan de forma
desacoplada e independiente a dicho valor, el efecto general $e$ que
resume el efecto global de todos los factores a la vez, suele
comportarse bajo la **distribuci√≥n normal** sencillamente porque hay un
teorema en matem√°ticas, conocido como el Teorema del L√≠mite Central que
b√°sicamente as√≠ lo garantiza, al postular que: ***la suma de*** $k$
***efectos aleatorios independientes tiende a adoptar el comportamiento
normal conforme la cantidad k de efectos crece***.

Es importante destacar que el Teorema del L√≠mite Central tiene algunas
condiciones y suposiciones, como la independencia de las variables
aleatorias y que la suma debe efectuarse sobre una cantidad $k$ de ellas
suficientemente grande. √âste es uno de los conceptos fundamentales en
estad√≠stica y es ampliamente utilizado en la teor√≠a y la pr√°ctica de
esta disciplina.

[Observaci√≥n para nada genial 4 üò•üò¢:]{style="color: red;"}

Excepto por el componente aleatorio $e$, el modelo restringe la relaci√≥n
de $Y$ y $X$ al universo de relaciones lineales. Existir√° una posible
relaci√≥n por cada par de valores $\beta_0$ y $\beta_1$ que decidamos
elegir. Pero por m√°s que nos esforcemos en modificarlos siempre
conducir√°n a relaciones representadas por l√≠neas rectas, de ah√≠ que el
modelo asuma el nombre de regresi√≥n lineal.

Si queremos capturar otras posibles dependencias de $Y$ respecto a $X$
podemos generalizar la relaci√≥n usando $y = f(x) + e$ donde $f(x)$ puede
ser una nueva especificaci√≥n funcional con otra estructura deseada, por
ejemplo un polinomio o cualquier otra funci√≥n que querramos definir. Lo
importante es que detr√°s de la definici√≥n de $f(x)$ haya alguna
justificaci√≥n producto de haber analizado los datos en b√∫squeda de
relaciones que capturen bien la dependencia.

[Observaci√≥n genial final:]{style="color: red;"}

Son los datos observados para cada valor de $y$ y $x$ los que nos deben
informar sobre el par de valores $\beta_0$ y $\beta_1$ que crean la
relaci√≥n lineal que mejor representa nuestra nube de puntos, pero de
nada sirven los valores observados si no definimos una m√©trica que nos
informe sobre la calidad del ajuste.

## 1.9 Introducci√≥n a las medidas de ajuste para modelos no ingenuos

De la observaci√≥n final del apartado anterior nos queda una inquietud:
¬øC√≥mo decidimos el par de valores a asignar a los par√°metros del modelo
lineal si no tenemos un criterio para poder saber cu√°l es el mejor
modelo?

Para salir de este embrollo, reconozcamos que nunca en un problema real
sujeto a incertidumbre una recta pasar√° exactamente por todos los
puntos, raz√≥n por la cual aparecer√°n componentes de error $e_i$ por cada
$y_i$ y $x_i$ observado. Vimos que esto ocurri√≥n con el modelo ingenuo y
por supuesto ocurre tambi√©n para un modelo no ingenuo. La idea es
entonces reducir la magnitud de estos errores al m√≠nimo posible y el par
de valores $\beta_1$ y $\beta_2$ que as√≠ lo logren ser√° nuestra elecci√≥n
√≥ptima de cara al objetivo de minimizaci√≥n del error.

Otro criterio de ajuste que se podr√≠a usar es el de maximizar la
probabilidad de ocurrencia de los valores observados bajo el supuesto de
que el modelo usa valores $\beta_0$ y $\beta_1$ previamente
especificados. De esta manera si un par de valores hace menos probable
haber obtenido nuestros datos observados deber√°n descartarse.

¬øPero c√≥mo podemos medir la probabilidad de ocurrencia de nuestros datos
observados una vez damos valores espec√≠ficos para los betas? Esta es una
cuesti√≥n que se tratar√° m√°s adelante cuando discutamos los **m√©todos de
estimaci√≥n de par√°metros**, nombre con el que se conoce al procedimiento
estad√≠stico encaminado a elegir los mejores betas para un conjunto de
datos observados.

Por lo pronto vamos a proceder de manera m√°s intuitiva, dejando de lado
el formalismo matem√°tico, y vamos a ejemplificar un posible modelo
ajustado usando valores $\beta_0=700$ y $\beta_1 = 1.2$. Hemos elegido
estos valores por simple inspecci√≥n visual as√≠ que no esperamos que sean
√≥ptimos en ning√∫n sentido, veamos:

```{r}

# Calculamos las predicciones de "y" usando el modelo
b0 = 700
b1 = 1.2
x= hass_dolar$precio_dolar
y_pred = b0 + b1*x

hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg ))+
  geom_point() +
  geom_line( aes( x= x, y = y_pred), col = "red")
```

Aunque no estamos seguros de la optimalidad de los betas elegidos, el
ajuste basado en este modelo es a simple vista decente, pero no
deber√≠amos depender de esto. Es necesario usar las herramientas que R
nos ofrece para estimar los betas. Antes de pasar a su uso ciego,
revisemos en qu√© consideraciones estad√≠sticas se basan estos m√©todos.
Para ello definamos primero los errores cometidos por el modelo, y m√°s
a√∫n, la medida de desempe√±o que usaremos para decidir cu√°l es la mejor
elecci√≥n de los betas. Usaremos entonces la m√©trica MSE(Mean Squared
Errors) dada por:

$$MSE_{\text{NO ingenuo}} =  \frac{1}{n}\sum^{n}_{i=1}e_i¬≤$$

Por otro lado podemos decir que $e_i = y_i - \hat{y}$, donde la cantidad
$\hat{y}$ representa el valor de la variable $Y$ que arroja el modelo,
el cual es por regla general diferente al verdadero valor observado de
$Y$, es decir, los errores son las diferencia de lo observado $y_i$ y lo
predicho por el modelo basado en la recta $\hat{y}$, por lo tanto:

$$MSE_{\text{NO ingenuo}} =  \frac{1}{n}\sum^{n}_{i=1}(y_i - \hat{y})¬≤$$

Y m√°s espec√≠ficamente:

$$MSE_{\text{NO ingenuo}} =  \frac{1}{n}\sum^{n}_{i=1}(y_i - (\beta_0 + \beta_1*x_i))¬≤$$
$$MSE_{\text{NO ingenuo}} =  \frac{1}{n}\sum^{n}_{i=1}(y_i - \beta_0 - \beta_1*x_i)¬≤$$

Con esta expresi√≥n ya se hace patente que $MSE_{\text{NO ingenuo}}$ es
funci√≥n de los pares de valores $(x_i, y_i)$ a los que se debe ajustar
la recta, as√≠ como de los valores que elijamos para los betas.

Sin embargo, como los pares $(x_i, y_i)$ ya vienen dados por nuestra
base de datos o, hablando en t√©rminos m√°s generales, por la informaci√≥n
recaudada para la construcci√≥n del modelo, s√≥lo tendremos opci√≥n de
variar los betas en aras de minimizar la cantidad
$MSE_{\text{NO ingenuo}}$. Dicho de otra manera los pares de valores
$(x_i, y_i)$ **se comportan como constantes** a lo largo de mi proceso
de minimizaci√≥n.

Dado lo anterior, es buena idea definir una funci√≥n MSE en R que permita
calcular, para diferentes elecciones de los betas, la medida de
desempe√±o. Esta medida de desempe√±o es conocida tambi√©n en la literatura
como funci√≥n de p√©rdida, queriendo indicar con ello una p√©rdida de
ajuste del modelo a los datos. Es importante aclarar que no existe una
√∫nica elecci√≥n para la funci√≥n de p√©rdida de un modelo predictivo, otras
alternativas t√≠picas son la mediana de las desviaciones absolutas
(MEDA), la media de los errores absolutos (MAE), entre otros. Sin
embargo en regresi√≥n lineal es usual usar MSE porque su expresi√≥n como
cuadrado de errores permite usar teor√≠a de inferencia basada en la
distribuci√≥n F cuando los errores se asumen normales. Veremos esto m√°s a
detalle cuando discutamos pruebas de hip√≥tesis sobre el modelo de
regresi√≥n.

## 1.10 Una relaci√≥n importante entre los betas del modelo lineal

Retomando nuestro modelo original

$$y = \beta_{0} + \beta_{1} * x + e \text{ con } e \text{~}N(0, \sigma^2)$$

Tomando valor esperado en ambos lados de la ecuaci√≥n tenemos que:

$$E(y) = E(\beta_{0} + \beta_{1} * x + e)$$ Pero como $\beta_0$ y
$\beta_1$ son constantes, tenemos que:

$$E(y) = E(\beta_0) + E(\beta_1*x) + E(e)$$ As√≠ que:

$$E(y) = \beta_0 +\int_{-\infty}^{\infty}\beta_1 *xf(x)dx + E(e)$$
$$E(y) = \beta_0 + \beta_1*\int_{-\infty}^{\infty}xf(x)dx + E(e)$$ Y
como $\int_{-\infty}^{\infty}xf(x)dx$ es por definici√≥n $E(x)$ y adem√°s
$E(e) = 0$ por dise√±o (de acuerdo con el supuesto usado para la
distribuci√≥n del error), entonces:

$$E(y) = \beta_0 + \beta_1*E(x)$$ Dicho de otra manera, la recta del
modelo debe pasar por el punto $(E(x), E(y))$ !!

La recta debe pasar por el centroide de la nube de puntos.

Ciertamente no conocemos el valor esperado ni de la variable predictora
X ni de la variable respuesta Y, pero buenos estimados basados en
informaci√≥n muestral ser√≠an $\bar{x}$ y $\bar{y}$. Es decir, no podremos
saber con precisi√≥n los valores $\beta_0$ y $\beta_1$, pero nos podemos
conformar con buenas estimaciones que cumplan la condici√≥n:

$$\bar{y} = \color{red}{\widetilde\beta_0} + \color{red}{\widetilde\beta_1}\bar{x}$$

El coloreado y la tilde ancha para los betas pretenden enfatizar que
estos valores son estimados de los correspondientes $\beta_0$ Y
$\beta_1$ del modelo te√≥rico subyacente. En conclusi√≥n: la mejor recta
estimada deber√° pasar por $(\bar{x}, \bar{y})$, y por lo tanto hay una
relaci√≥n atando a los betas estimados:

$$\begin{equation} \label{eq:ec0}
  \color{red}{\widetilde\beta_0} = \bar{y} - \color{red}{\widetilde\beta_1}\bar{x}
\end{equation}
$$ Es decir que dando valores a $\color{red}{\widetilde\beta_1}$
podremos obtener valores para $\color{red}{\widetilde\beta_0}$ y en
√∫ltimas dibujar diversas rectas en el plano, con el fin de establecer
cu√°l de ellas minimiza el $MSE_{\text{NO ingenuo}}$.

## 1.11 Una animaci√≥n que ilustra la estimaci√≥n de betas

A continuaci√≥n presentamos un c√≥digo en R que permite comprender c√≥mo
funcionan num√©ricamente los m√©todos de estimaci√≥n de betas para un
modelo lineal simple, espec√≠ficamente cuando se usa como m√©todo de
estimaci√≥n el criterio de minimizaci√≥n del promedio de los cuadrados de
los errores:

```{r}
# Extraemos las variables de inter√©s
x = hass_dolar$precio_dolar
y = hass_dolar$precio_aguacate_kg

# Calculamos medias muestrales para las variables X y Y

x_barra <- mean(x)
y_barra <- mean(y)

# Definimos posibles valores para b1, y por consiguiente para b0
b1_values <- seq(-1, 3, by = 0.08)
b0_values <- y_barra - b1_values * x_barra

# Fijamos l√≠mites en X y Y para cada gr√°fico
xmin = min(x) - 50
xmax = max(x) + 50
ymin = min(y) - 50
ymax = max(y) + 50

# Construimos una funci√≥n para calcular el MSE en funci√≥n de x_i, y_i, b1 y b0
MSE <- function(x, y, b0, b1) {
  errors <- y - (b0 + b1*x)
  squared_errors <- errors^2
  mse <- mean(squared_errors)
  return(mse)
}

# Crear un dataframe para almacenar los resultados
list_mse <- vector()

# Calcular el MSE para diferentes valores de B1
for (i in 1:length(b1_values)) {
  mse <- MSE(x, y, b0_values[i], b1_values[i])
  list_mse[i] <- mse
}

mse_df = data.frame(
  B0 = b0_values,
  B1 = b1_values,
  mse = list_mse,
  color_point = 'green'
)

# Capturando el rengl√≥n donde ocurre el m√≠nimo mse
pos_min <- which(mse_df$mse==min(mse_df$mse))

# Impriendo las filas cercanas al m√≠nimo
mse_df$color_point[(pos_min - 4):(pos_min + 4)] <- rep('red',9)
ftable(mse_df[(pos_min - 8):(pos_min + 8), ])
```

¬øQu√© hemos logrado hasta ac√°?

-   Demostrar que el valor de $MSE_{\text{NO ingenuo}}$ depende de los
    betas elegidos para el modelo.
-   Demostrar c√≥mo el valor de $\color{red}{\widetilde\beta_0}$ est√°
    atado con el de $\color{red}{\widetilde\beta_1}$ si de verdad
    queremos que la recta estimada respete la condici√≥n de que
    $e \text{~}N(0, \sigma^2)$ y por lo tanto de que $E(e)=0$.
-   Generar un dataframe que calcula los valores de
    $MSE_{\text{NO ingenuo}}$ para cada par de valores elegidos para
    $\color{red}{\widetilde\beta_0}$ y $\color{red}{\widetilde\beta_1}$,
    dando valores a $\color{red}{\widetilde\beta_1}$
-   Estimamos que el $\color{red}{\widetilde\beta_1}$ √≥ptimo est√° cerca
    del valor 0.94, y que el $\color{red}{\widetilde\beta_0}$ √≥ptimo
    est√° cerca del valor 1452.

Ahora graficaremos una curva que muestre esta relaci√≥n:

```{r results = 'hide'}

if (!file.exists("beta_vs_mse.gif")){
  # Crear la animaci√≥n
  animation <- ggplot(mse_df, aes(B1, mse)) +
    geom_line(aes( x= B1, y = mse, color = color_point), linetype = "dashed") +
    geom_point(aes(color = color_point), size = 3) +
    labs(title = "Valores de MSE para cada posible B1, respetando E(e) = 0") +
    geom_text(data = mse_df, 
              aes(label = paste("MSE: ", round(mse,0))), x= 1, y = 2e6, color='black'
    ) +
    geom_text(data = mse_df, 
              aes(label = paste("MSE m√≠nimo: ", round(min(mse),0))),
              x= 1, y = min(mse_df$mse) - 1e5, color='red') +
    scale_color_manual(values = c("red" = "red", "green" = "green"),
                       labels = c("Sub√≥ptima", "√ìptima"),
                       name = "Soluciones") +
    theme_minimal() +
    transition_reveal(B1)

  animate(animation, duration = 10, fps = 2, renderer = gifski_renderer())
  anim_save("beta_vs_mse.gif")
}
```

<figure>

<img src="beta_vs_mse.gif"/>

<figcaption>Img 1: Relaci√≥n entre B1 y MSE</figcaption>

</figure>

Ahora veamos el efecto que tiene la elecci√≥n de los betas en la recta
regresora, e incluyamos pausas en la animaci√≥n para detenernos en el
momento en que encontremos la recta √≥ptima.

```{r results = 'hide'}

if (!file.exists("beta_estimation.gif")){
  
# Las pausas en la animaci√≥n son fotogramas con informaci√≥n repetida correspondiente
# al rengl√≥n √≥ptimo

fotogramas_pausa <- data.frame(
  B0 = rep(b0_values[pos_min], 2*nrow(mse_df)),
  B1 = rep(b1_values[pos_min], 2*nrow(mse_df)),
  mse = rep(list_mse[pos_min], 2*nrow(mse_df)),
  color_point = rep('red',2*nrow(mse_df))  
)

mse_df <- rbind(mse_df, fotogramas_pausa)
mse_df <- mse_df[order(mse_df$B1), ]
mse_df$estado <- seq(1,nrow(mse_df))

# Construimos el dataframe de predicciones para cada estado, con el cual poder
# graficar los residuales en cada fotograma

df_predicciones <- data.frame(B0_pred=numeric(0),
                              B1_pred=numeric(0),
                              x_from_pred=numeric(0),
                              y_from_pred=numeric(0),
                              y_pred_mod=numeric(0),
                              estado = numeric(0))
for (i in 1:nrow(mse_df)){
  df_nuevo <-data.frame(
    BO_pred = rep(mse_df[i, 'B0'], length(x)),
    B1_pred = rep(mse_df[i, 'B1'], length(x)),
    x_from_pred = x,
    y_from_pred = y,
    y_pred_mod = mse_df[i, 'B0'] + mse_df[i,'B1']*x,
    estado = rep(mse_df[i, 'estado'], length(x))
  ) 
  df_predicciones <- rbind(df_predicciones, df_nuevo)
}

# Creamos el gr√°fico base

base_plot <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +
  geom_point() +
  geom_point(x=mean(x), y=mean(y), color='green', size=2.5) +
  geom_abline(aes(slope = 0, intercept = y_barra), 
              linetype = "dashed", color = "blue") +
  labs(title = "Estimaci√≥n de Betas por Minimizaci√≥n del MSE") +
  theme_minimal()

# Creamos la animaci√≥n

animation <- base_plot +
  geom_abline(aes(slope = B1, intercept = B0, color = color_point), data=mse_df) +
  geom_text(aes(label = paste("MSE: ", round(mse,0))), x= 2000, y = 6000, data=mse_df,
            color = "red")+
  geom_segment(data = df_predicciones,
               aes(x=x_from_pred, y=y_from_pred, xend=x_from_pred, yend=y_pred_mod),
               col = "violet", lty='dashed') +
  transition_states(estado, transition_length = 2, state_length = 1) +
  scale_color_manual(values = c("red" = "red", "green" = "green"),
                     labels = c("Sub√≥ptima", "√ìptima"),
                     name = "Rectas regresoras") +
  enter_fade() +
  exit_fade()

# Guardamos la animaci√≥n en un archivo GIF
animate(animation, duration = 20, fps = 2, renderer = gifski_renderer())
anim_save("beta_estimation.gif", animation)
}
```

<figure>

<img src="beta_estimation.gif"/>

<figcaption>Img 1: Estimaci√≥n de betas por minimizaci√≥n de
MSE</figcaption>

</figure>

## 1.12 Usando funciones en R para estimar par√°metros

Finalmente hemos llegado al punto en que usaremos las funciones de R
para estimar par√°metros del modelo de regresi√≥n lineal. Esto se har√°
bajo el enfoque de optimizaci√≥n, para ello conviene darle una mirada al
siguiente problema:

Estime un par de valores $(\widetilde\beta_0,\widetilde\beta_1)$ tal que
minimice la expresi√≥n $MSE = f(\beta_0,\beta_1)$, donde:

$$f(\beta_0,\beta_1) = \frac{1}{n}\sum^{n}_{i=1}(y_i - \beta_0 - \beta_1*x_i)¬≤$$
Tomando en cuenta que no hay restricciones aplicadas a los betas.

Para resolver el problema primero efectuemos algunas simplificaciones
tomando partida de la siguiente identidad algebraica:
$(a+b+c)¬≤ = a¬≤+b¬≤+c¬≤+2ab+2ac+2bc$, de tal manera que:

$$f(\beta_0,\beta_1) = \frac{1}{n}\sum^{n}_{i=1}(y_i¬≤+\beta_0¬≤+\beta_1¬≤x_i¬≤-2y_i\beta_0-2y_i\beta_1x_i+2\beta_0\beta_1x_i)$$

Ahora distribuyamos la suma y separemos aparte los t√©rminos que dependen
de su √≠ndice:

$$f(\beta_0,\beta_1) = \frac{1}{n}\left[\sum^{n}_{i=1}y_i¬≤+\beta_0¬≤\sum^{n}_{i=1}1+\beta_1¬≤\sum^{n}_{i=1}x_i¬≤-2\beta_0\sum^{n}_{i=1}y_i-2\beta_1\sum^{n}_{i=1}y_ix_i+2\beta_0\beta_1\sum^{n}_{i=1}x_i\right]$$
Simplificando t√©rminos y distribuyendo la fracci√≥n:

$$f(\beta_0,\beta_1) = \frac{1}{n}\sum^{n}_{i=1}y_i¬≤+\frac{\beta_0¬≤}{n}n+\frac{\beta_1¬≤}{n}\sum^{n}_{i=1}x_i¬≤-\frac{2\beta_0}{n}\sum^{n}_{i=1}y_i-\frac{2\beta_1}{n}\sum^{n}_{i=1}y_ix_i+\frac{2\beta_0\beta_1}{n}\sum^{n}_{i=1}x_i$$
Para finalmente obtener:

$$f(\beta_0,\beta_1) = \beta_0¬≤ +\bar{x¬≤}\beta_1¬≤ -2\bar{y}\beta_0-\left[\frac{2}{n}\sum^{n}_{i=1}x_iy_i\right]\beta_1+2\bar{x}\beta_0\beta_1+\bar{y¬≤} $$
En la expresi√≥n anterior, las cantidades $\bar{x¬≤}$, $-2\bar{y}$,
$-\frac{2}{n}\sum^{n}_{i=1}x_iy_i$, $2\bar{x}$, $\bar{y¬≤}$ son todas
constantes por depender s√≥lo de los datos que alimentan al modelo, en
ese sentido son coeficientes num√©ricos para los respectivos betas de la
funci√≥n.

LLegados a este punto, podemos sacar las siguientes conclusiones:

-   La funci√≥n $f$ s√≥lo depende de los betas, puesto que los dem√°s
    valores son constantes, es decir los valores $(x_i, y_i)$ son
    n√∫meros consignados en una base de datos, y $n$ es la cantidad de
    pares de n√∫meros consignados. Todos los coeficientes depender√°n de
    operaciones sobre estos n√∫meros.

-   De acuerdo con lo anterior la funci√≥n $f$ es una funci√≥n
    multivariada, pues depende de dos variables: $\beta_0$ y $\beta_1$.

-   Existen m√©todos muy sencillos para resolver un problema de
    optimizaci√≥n no restringido cuando la funci√≥n a optimizar es
    convexa, entonces s√≥lo nos resta demostrar que la funci√≥n anterior
    lo es.

**Definici√≥n de convexidad**

Para demostrar que $f(\beta_0, \beta_1)$ es convexa, debemos verificar
que su matriz hessiana sea semidefinida positiva. La matriz hessiana es
una matriz de segundas derivadas parciales de la funci√≥n.

Primero, calculemos las derivadas parciales de $f$ con respecto a
$\beta_0$ y $\beta_1$:

$$\begin{equation} \label{eq:ec1}
  \frac{\partial f}{\partial \beta_0} = 2\beta_0-2\bar{y}+2\bar{x}\beta_1
\end{equation}
$$ $$\begin{equation} \label{eq:ec2}
  \frac{\partial f}{\partial \beta_1} = 2\bar{x¬≤}\beta_1-\frac{2}{n}\sum^{n}_{i=1}x_iy_i+2\bar{x}\beta_0
\end{equation}
$$ Luego, calculemos las segundas derivadas parciales de $f$ con
respecto a $\beta_0$ y $\beta_1$:

$$\frac{\partial^2 f}{\partial \beta_0^2} = 2 > 0$$
$$\frac{\partial^2 f}{\partial \beta_1^2} = 2\bar{x^2} > 0$$
$$\frac{\partial^2 f}{\partial \beta_0 \partial \beta_1} = 2\bar{x}$$

La matriz hessiana de $f$ es:

$$
H(f) = \begin{bmatrix}
\frac{\partial^2 f}{\partial \beta_0^2} & \frac{\partial^2 f}{\partial \beta_0 \partial \beta_1} \\
\frac{\partial^2 f}{\partial \beta_0 \partial \beta_1} & \frac{\partial^2 f}{\partial \beta_1^2}
\end{bmatrix}
$$

Por lo tanto:

$$
H(f) = \begin{bmatrix} 
2 & 2\bar{x}\\
2\bar{x} & 2\bar{x^2}
\end{bmatrix}
$$

Para que $f(\beta_0, \beta_1)$ sea convexa, la matriz hessiana debe ser
semidefinida positiva, lo que significa que debe cumplir la propiedad:
$$v^ \top Hv>=0 \text{ para }\forall v \in \mathbb{R}¬≤
$$ Es decir, debemos verificar si:

$$\begin{bmatrix} a & b
\end{bmatrix}H\begin{bmatrix}
      a \\ 
      b  \\
     \end{bmatrix}>=0 \text{ para } \forall (a,b) 
$$

Veamos si esto se cumple:

$$
\begin{bmatrix}
  a & b
\end{bmatrix}
\begin{bmatrix} 
  2        &   2\bar{x}  \\
  2\bar{x} &   2\bar{x^2}
\end{bmatrix}
\begin{bmatrix}
  a \\ 
  b  \\
\end{bmatrix}\\=
\begin{bmatrix}
  2a+2b\bar{x} & 2a\bar{x} + 2b\bar{x¬≤}
\end{bmatrix}
\begin{bmatrix}
  a \\ 
  b  \\
\end{bmatrix}\\=2a¬≤+2ab\bar{x}+2ab\bar{x}+2b¬≤\bar{x¬≤}
\\=2a¬≤+4ab\bar{x}+2b¬≤\bar{x¬≤}\\
=2(a¬≤+2ab\bar{x}+b¬≤(\bar{x})¬≤)-2b^2(\bar{x})¬≤+2b¬≤\bar{x¬≤}\\
=2(a+b\bar{x})¬≤+2b¬≤(\bar{x¬≤}-(\bar{x})¬≤)
$$

Ac√° es interesante notar que:

$$\bar{x¬≤}-(\bar{x})¬≤=\frac{1}{n}\left[\sum^{n}_{i=1}x_i¬≤\right]-\bar{x}\bar{x}
=\frac{1}{n}\left[\sum^{n}_{i=1}x_i¬≤-\bar{x}(n\bar{x})\right]\\
=\frac{1}{n}\left[\sum^{n}_{i=1}x_i¬≤-\bar{x}\sum^{n}_{i=1}x_i\right]
=\frac{1}{n}\left[\sum^{n}_{i=1}(x_i¬≤-\bar{x}x_i)\right] \\
=\frac{1}{n}\left[\sum^{n}_{i=1}((x_i¬≤-2\bar{x}x_i)+\bar{x}x_i)\right]
=\frac{1}{n}\left[\sum^{n}_{i=1}\left((x_i¬≤-2\bar{x}x_i+\bar{x}¬≤)+(\bar{x}x_i-\bar{x}¬≤)\right)\right]\\
=\frac{1}{n}\left[\sum^{n}_{i=1}\left((x_i-\bar{x})¬≤+\bar{x}(x_i-\bar{x})\right)\right]
=\frac{1}{n}\sum^{n}_{i=1}(x_i-\bar{x})¬≤+\bar{x}\left[\sum^{n}_{i=1}\frac{x_i}{n}-\sum^{n}_{i=1}\frac{\bar{x}}{n}\right]\\
=\sigma¬≤_{x}+\bar{x}\left(\bar{x}-\bar{x}\sum^{n}_{i=1}\frac{1}{n}\right)\\
=\sigma¬≤_{x}+\bar{x}(\bar{x}-\bar{x}*1)=\sigma¬≤_{x}$$

Obteniendo finalmente

$$\begin{equation} \label{eq:sigma_x}
  \bar{x¬≤}-(\bar{x})¬≤ = \sigma¬≤_{x}
\end{equation}
$$

Luego:

$$v^ \top Hv = 2(a+b\bar{x})¬≤+2b¬≤\sigma¬≤_{x} \text{ con } \text{ }v=(a,b)$$
Y como ambos sumandos son no negativos se tiene que
$$v^ \top Hv >= 0 \text{ para }\forall v\in \mathbb{R}¬≤$$ Es decir, la
matriz hessiana es semidefinida positiva y por lo tanto
$f(\beta_0,\beta_1)$ es convexa en el espacio de par√°metros $\beta_0$ y
$\beta_1$. Esto implica que el problema de regresi√≥n lineal, que busca
minimizar esta funci√≥n de errores, tiene un m√≠nimo global y puede ser
resuelto de manera eficiente mediante m√©todos de optimizaci√≥n convexa.

Una condici√≥n necesaria y suficiente para que
$(\widetilde\beta_0,\widetilde\beta_1)$ sea un m√≠nimo global es que
$\nabla{} f(\widetilde\beta_0,\widetilde\beta_1)=0$

Por lo tanto retomando $\ref{eq:ec1}$ y $\ref{eq:ec2}$ el problema de
minimizaci√≥n queda resuelto hallando la soluci√≥n al sistema de
ecuaciones:

$$\begin{equation} \label{eq:ec3}
    2\widetilde\beta_0-2\bar{y}+2\bar{x}\widetilde\beta_1=0
  \end{equation}  
$$ $$\begin{equation} \label{eq:ec4}
    2\bar{x¬≤}\widetilde\beta_1-\frac{2}{n}\sum^{n}_{i=1}x_iy_i+2\bar{x}\widetilde\beta_0=0
  \end{equation}
$$

De modo que despejando $\widetilde\beta_0$ en $\ref{eq:ec3}$ tenemos lo
siguiente:

$$\begin{equation} \label{eq:ec5}
  \widetilde\beta_0 = \bar{y} - \widetilde\beta_1\bar{x}
\end{equation}
$$ Esto en concordancia con $\ref{eq:ec0}$, y adem√°s haciendo reemplazos
en $\ref{eq:ec4}$ obtenemos:

$$
  2\bar{x¬≤}\widetilde\beta_1-\frac{2}{n}\sum^{n}_{i=1}x_iy_i+2\bar{x}(\bar{y}-\widetilde\beta_1\bar{x})=0\\
  2\bar{x¬≤}\widetilde\beta_1-2\bar{x}¬≤\widetilde\beta_1 = \frac{2}{n}\sum^{n}_{i=1}x_iy_i-2\bar{x}\bar{y}\\
  (\bar{x¬≤} - \bar{x}¬≤)\widetilde\beta_1 = \frac{1}{n}\sum^{n}_{i=1}x_iy_i-\bar{x}\bar{y}
$$ Ahora, usando el hecho de que el lado derecho de la ecuaci√≥n anterior
es por definici√≥n $cov(x,y)$, es decir, la covarianza entre la variable
regresora $x$ y la variable respuesta $y$, as√≠ como la expresi√≥n dada en
$\ref{eq:sigma_x}$ tenemos:

$$\begin{equation} \label{eq:ec6}
    \widetilde\beta_1 = \frac{cov(x,y)}{\sigma_x¬≤}
  \end{equation}
$$ Las ecuaciones $\ref{eq:ec5}$ y $\ref{eq:ec6}$ son muy informativas
en su estructura, la primera indica que la recta regresora debe pasar
por el centroide de la nube de puntos: $(\bar{x}, \bar{y})$ y la segunda
indica que la pendiente de la recta es proporcional al grado de relaci√≥n
lineal que tenga la variable $x$ con la variable $y$ medida a trav√©s de
la covarianza.

Estos sencillos c√°lculos son realizados por R a trav√©s de la funci√≥n
"lm", as√≠:

```{r}

# b1 = cov(x,y) / (sigma_x)¬≤
b1 <- cov(x,y) / var(x)
print(b1)

# bo = y_barra - b1*x_barra
b0 <- mean(y) - b1*mean(x)
print(b0)

residuales <- y - (b0 + b1*x)
n <- length(x)

sigma_2 <- sum(residuales^2)/(n-2)
print(sigma_2)

# Confirmamos usando la funci√≥n lm, a continuaci√≥n la documentaci√≥n

# lm(formula, data, subset, weights, na.action,
#    method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
#    singular.ok = TRUE, contrasts = NULL, offset, ...)

mod1 <- hass_dolar %>% lm(precio_aguacate_kg ~ precio_dolar,.)
print(mod1$coefficients)
resumen <-  summary(mod1)
resumen$sigma

parametros <- data.frame(metodo = c("manual","lm"), 
                         beta_0 = c(b0,mod1$coefficients[1]),
                         beta_1 = c(b1,mod1$coefficients[2]),
                         sigma_2 = c(sigma_2, (resumen$sigma)^2))



parametros %>% 
  ftable()

#print(mod1$)
# Generando el modelo con R Base ser√≠a as√≠:
# mod1 <- lm(promedio_aguacate_kg ~ promedio_dolar, hass_dolar)

```

Podemos notar entonces que ambos m√©todos son consistentes entre si, algo
que no es de extra√±ar pues internamente R implementa estos m√©todos en
sus librer√≠as.

## 1.13 Representaci√≥n funcional del modelo y comprobaci√≥n de supuesto de normalidad.

Hemos llegado al punto en que ya tenemos todos los elementos necesarios
para definir formalmente el modelo estimado sobre nuestros datos.
Podemos ahora concluir que el mejor modelo para describir la relaci√≥n
entre la variable precio del aguacate ($y$) y precio del d√≥lar($x$),
pensado desde la perspectiva de minimizaci√≥n de cuadrados del error y
estando restringidos a la familia de modelos de regresi√≥n lineal con
componente de error normal es:

$y = `r round(b0,2)` + `r round(b1,2)`x + e \text{ con e~N(0,`r round(sigma_2,1)`)}$

Pero, aunque nos hemos esforzado por encontrar las mejores estimaciones
de los betas para minimizar la funci√≥n de p√©rdida, esto no garantiza que
nuestro modelo induzca errores normales, por tal motivo conviene recordar lo siguiente:

üí≠ Cuando planteamos el modelo, tuvimos que suponer
algunas cosas, entre ellas, que la distribuci√≥n de los errores ser√≠a
*normal*. Por lo tanto, ahora que tenemos constru√≠do nuestro modelo, es
necesario que validemos la suposici√≥n hecha.

ü§îüí≠ ¬øC√≥mo hacerlo ‚ùî

Consideremos la siguiente tabla comparativa que informa sobre las ventajas y desventajas de las tres pruebas de normalidad (Kolmogorov-Smirnov, Lilliefors y Shapiro-Wilk) m√°s usadas en estad√≠stica

```{r}

tabla_normalidad <- data.frame(
  "Prueba" = c("Kolmogorov-Smirnov (KS)", "Prueba de Lilliefors", "Prueba de Shapiro-Wilk (SW)"),
  "Ventajas" = c(
    "- Puede utilizarse para cualquier distribuci√≥n te√≥rica.\n- Es una prueba no param√©trica vers√°til.\n - Adecuada para muestras grandes.",
    "- Es espec√≠fica para evaluar la normalidad.\n- Puede ser m√°s adecuada para muestras peque√±as.\n - Utiliza estad√≠sticas de prueba que se ajustan a la distribuci√≥n est√°ndar.",
    "- Es especialmente potente para muestras peque√±as.\n- Dise√±ada espec√≠ficamente para evaluar la normalidad.\n- Sensible a desviaciones de la normalidad en cualquier parte de la distribuci√≥n."
  ),  "Desventajas" = c(
    "- Puede ser menos poderosa en muestras peque√±as.\n - No es espec√≠fica para la distribuci√≥n normal.",
    "- Restringida a la comprobaci√≥n de normalidad. \n - Menos potente en muestras grandes.\n- No es tan vers√°til como KS para otras distribuciones.",
    "- M√°s compleja de entender y aplicar.\n- No proporciona estimaciones de par√°metros.\n- Puede ser menos adecuada para muestras muy grandes.\n- No es tan vers√°til como KS para otras distribuciones."))

# Imprimir la tabla con formato

tabla_normalidad %>% 
  ftable() %>% 
  align(i = NULL, j = c(2:3),
        align = "left", part = "all")
```

Recordemos que la elecci√≥n de la prueba depende de varios factores,
incluyendo el tama√±o de la muestra, el *conocimiento previo sobre la
distribuci√≥n de los datos* y los objetivos del an√°lisis. Ninguna prueba
es perfecta y es importante considerar el contexto y las caracter√≠sticas
espec√≠ficas de tus datos al seleccionar una prueba de normalidad.

ü§îüí≠ Y si el texto anterior dice: *"el conocimiento previo sobre la
distribuci√≥n de los datos"* ü§îüí≠ ¬øSer√° necesario representar la
distribuci√≥n en un gr√°fico? ¬øC√≥mo funciona la prueba Lilliefors? A continuaci√≥n describiremos los c√°lculos implicados:

```{r}

# Paso 1: Ordenamos los datos de menor a mayor
n <- length(mod1$residuals)
df_residuals <- data.frame(residuals = mod1$residuals)
df_residuals <- df_residuals %>% 
                arrange(residuals)

# Paso 2: Estimamos los par√°metros para la distribuci√≥n normal te√≥rica a partir de los datos

mean_residuals <- mean(df_residuals$residuals)
sd_residuals <- sd(df_residuals$residuals)

# Paso 3: Calculamos la funci√≥n acumulada para cada valor de residual
cdf_teorica <- pnorm(df_residuals$residuals,
                      mean = mean_residuals,
                      sd = sd_residuals)

# Paso 4: Calculamos la m√°xima diferencia absoluta entre la densidad emp√≠rica y la te√≥rica

# M√©todo 1: Construir la funci√≥n emp√≠rica usando f√≥rmulas de R (ecdf) y luego tomar diferencias
#           absolutas entre valor emp√≠rico y te√≥rico (enfoque KS)
cdf_empirica <- ecdf(df_residuals$residuals)
diferencias_absolutas <- abs(cdf_empirica(df_residuals$residuals) - cdf_teorica)
pos_estadistico_m1 <- which.max(diferencias_absolutas)
estadistico_m1 <- diferencias_absolutas[pos_estadistico_m1]

print(paste0("Este es el valor del estad√≠stico: ", estadistico_m1," y esta es la posici√≥n en ",
             "la que ocurre la m√°xima diferencia ", pos_estadistico_m1))

# M√©todo 2: Construir manualmente los c√°lculos que permiten hallar la m√°xima diferencia
#           entre la emp√≠rica y la te√≥rica usando la definici√≥n de la emp√≠rica como
#           funci√≥n a trozos üìà

e_plus <- seq(1:n)/n
e_minus <- seq(0:(n-1))/n

D_plus <- e_plus - cdf_teorica
D_minus <- cdf_teorica - e_minus

pos_max_d_plus <- which.max(D_plus)
pos_max_d_minus <- which.max(D_minus)

estadistico_m2 <- max(D_plus[pos_max_d_plus], D_minus[pos_max_d_minus])
pos_estadistico_m2 <- function() {
  if(max(D_plus)>=max(D_minus)) {
    return(pos_max_d_plus)
    }else{
      return(pos_max_d_minus)
    }
  }

print(paste0("Este es el valor del estad√≠stico: ", estadistico_m2," y esta es la posici√≥n en ",
             "la que ocurre la m√°xima diferencia ", pos_estadistico_m2()))

# Comprobamos los resultados invocando la funci√≥n de R que realiza estos c√°lculos autom√°ticos

Lilliefors_test <- lillie.test(df_residuals$residuals)
print(Lilliefors_test)
estadistico_l <- Lilliefors_test$statistic
print(paste0("Este es el estad√≠stico de Lilliefors usando paqueter√≠a de R: ", estadistico_l))
```

Hasta ac√° hemos verificado que los c√°lculos realizados manualmente coinciden con el c√≥digo implementado por la librer√≠a nortest de R, y m√°s espec√≠ficamente en el c√≥digo fuente de la funci√≥n lillie.test. Ahora ilustremos algo de estos pasos usando una gr√°fica: 
```{r}
x_seg <- df_residuals$residuals[pos_estadistico_m1]
y_sup_seg <- cdf_teorica[pos_estadistico_m1]
y_inf_seg <- cdf_empirica(x_seg)

cat("x_seg: ", format(x_seg, digits=5), 
    " y_inf_seg: ", format(y_inf_seg, digits=5),
    " y_sup_seg: ", format(y_sup_seg, digits=5),
    sep='')

datos_para_grafico <- data.frame(
  residuales = df_residuals$residuals
  )

ggplot(datos_para_grafico, aes(x = residuales)) +
#  geom_density(color = "blue", fill = "lightblue", alpha = 0.5) +
  stat_function(fun = pnorm, args = list(mean = mean_residuals,
                                         sd = sd_residuals),
                color = "red", size = 1) +
  stat_ecdf(color = "green", size = 1) +
  geom_segment(aes(x = x_seg, y = y_inf_seg, xend = x_seg,
                   yend = y_sup_seg),
               color = "purple", size = 1, linetype = "solid") +
  geom_point(aes(x = x_seg, y = y_inf_seg),
             color = "purple",size = 3) +
  geom_point(aes(x = x_seg, y = y_sup_seg),
             color = "purple", size = 3) +
  annotate("text", x = x_seg , y = (y_inf_seg + y_sup_seg)/2, 
           label = paste0("M√°xima Separaci√≥n=",
                          format(estadistico_m1,
                                 digits=5)," ü§îüí≠", sep=''),
           color = "purple", size = 3, hjust = -0.05, vjust = 0) +
  coord_cartesian(xlim = c(min(datos_para_grafico$residuales),
                           max(datos_para_grafico$residuales))) +
  xlab("Residuales del modelo") +
  ylab("Probabilidad acumulada") +
  labs(title = "Esquema de funcionamiento de la prueba de Lilliefors") +
  theme_minimal()

```

Nos queda la siguiente reflexi√≥n sobre la gr√°fica anterior, ¬øPor qu√© elegir la m√°xima diferencia absoluta entre las dos curvas en lugar de un promedio de las diferencias a lo largo de todo el eje? ¬øPor qu√© no usar una media ponderada de los residuales al cuadrado?

Comparemos los resultados contra la prueba de Shapiro WIlk y Kolmogorov
```{r}
# Probando normalidad del error. 

shapiro.test(mod1$residuals)
ks.test(mod1$residuals, "pnorm")
```
Hemos notado diferencias con la prueba de Kolmogorov, ¬øCu√°l es la raz√≥n?, podr√°s encontrar en el siguiente art√≠culo en el que Lilliefors introduce por primera vez su prueba:

Lilliefors, H. (June 1967), "On the Kolmogorov--Smirnov test
for normality with mean and variance unknown", Journal of the American
Statistical Association, Vol. 62. pp. 399--402.

Una aclaraci√≥n respecto a que esta prueba es menos potente ‚ùó  en comparaci√≥n con la prueba KS est√°ndar, esto significa que es menos sensible para detectar desviaciones de la normalidad en muestras grandes, especialmente si los datos no siguen una distribuci√≥n estrictamente normal. La prueba KS est√°ndar es m√°s robusta en este sentido, pero por lo mismo m√°s conservador. En el contexto de modelos de regresi√≥n, no requerimos un cumplimiento estricto de la normalidad sino una semejanza razonable para habilitar el uso del modelo. En ese sentido es preferible usar Lilliefors.üòÄüòé

ü§îüí≠ Otras posibles preguntas son:

- ¬øCu√°les son los riesgos que implica asumir a priori un comportamiento espec√≠fico para el error?
- ¬øSi el modelo no cumple con lo supuesto para el error, qu√© alternativas tengo?
- Al plantear un modelo alternativo que captura mejor el comportamiento del error usando
una distribuci√≥n distinta a la normal ¬øPuedo seguir usando los m√©todos de inferencia tradicionales o qu√© modificaciones debo introducir?
- ¬øPor qu√© es importante para un magister en estad√≠stica conocer sobre m√©todos de simulaci√≥n Montecarlo?

# 2. EL SUPUESTO DE INDEPENDENCIA DEL ERROR

Un supuesto que hemos perdido por un momento en los apartados anteriores es el supuesto de independencia del error. Su consideraci√≥n es importante porque representa un punto de quiebre en el uso de modelos de regresi√≥n vs modelos alternativos como las series de tiempo. Pero ¬øQu√© significa exactamente la independencia?. Consideremos los siguientes escenarios:

- Un modelo que presenta sistem√°ticamente m√°s dificultades para predecir de forma exacta valores altos de la variable respuesta pero es bueno prediciendo valores bajos.

- Un modelo que se construy√≥ sobre datos de un experimento de durezas de metales, donde el mecanismo que sensa la dureza perdi√≥ calibraci√≥n o exactitud despu√©s de tomar la medida de las 100 primeras muestras.

- Un modelo que intento predecir un precio de activo financiero en funci√≥n de un activo de referencia, pero conforme pasa el tiempo el modelo se va desajustando y haciendo menos preciso.

- Un modelo que desconce la presencia de una variable oculta que controla la variaci√≥n acoplada de mi variable respuesta vs mi variable predictora y las hace parecer m√°s correlacionadas de lo que realmente son (regresiones espurias)

Para cada uno de los casos anteriores el investigador tiene que decidir el tipo de recurso gr√°fico o estad√≠stco que le permita inspeccionar si una situaci√≥n de esa naturaleza est√° ocurriendo. Las alternativas son:

[Gr√°ficos de Residuales vs. Valores Ajustados:]{style="color: red;"} Graficar los residuales (diferencia entre los valores observados y los valores predichos) contra los valores ajustados (las predicciones del modelo) es una forma com√∫n de detectar patrones de no independencia. 

ü§îüí≠ ¬øQu√© comportamiento es deseable para este gr√°fico?

Si no hay patrones evidentes en el gr√°fico y los residuales se distribuyen aleatoriamente alrededor de cero, es un indicio de independencia.

[Gr√°fico de Residuales con √≠ndice temporal:]{style="color: red;"} Si tus datos est√°n ordenados en el tiempo (series temporales), un gr√°fico de residuales en el tiempo puede revelar patrones de autocorrelaci√≥n. Deben parecer ruido blanco, es decir, sin patrones visibles. Tambi√©n es conveniente usar gr√°ficos de autocorrelaci√≥n de residuales (ACF) y gr√°ficos de autocorrelaci√≥n parcial de residuales (PACF), ya que estos informan sobre una posible correlaci√≥n entre el error actualmente observado y errores observados en otros momentos del pasado.

[Prueba de Durbin-Watson:]{style="color: red;"} Esta prueba estad√≠stica eval√∫a si existe autocorrelaci√≥n de primer orden en los residuales, mejora el an√°lisis gr√°fico por ser una prueba formal. Un valor de Durbin-Watson cercano a 2 indica independencia, mientras que valores significativamente diferentes de 2 pueden indicar autocorrelaci√≥n. De nuevo es necesario elegir una variable de ordenamiento para los errores encaminada a detectar el tipo de patr√≥n de inter√©s. 

ü§îüí≠ ¬øC√≥mo hacer esto?

- Ordene los errores seg√∫n magnitud del valor predicho (ajustado)
- Ordene los errores seg√∫n el tiempo.
- Ordene los errores seg√∫n el orden de experimentaci√≥n.
- No ordene por la posici√≥n del registro en el data frame a menos qu√© entienda qu√© significa ese orden y qu√© aporte da al an√°lisis.
- Ordene por una variable de inter√©s en la que quiera inspeccionar si los residuales se ven afectados por la magnitud de tal variable. 

üë¶üë¥üëßüë®üë®‚Äçüëµ

**Ejemplo:** Puede considera revisa si el consumo cal√≥rico puede explicar residuales de un modelo simplificado en el que el peso pretenda explicar la estatura. üîéüïµÔ∏è Aunque este ejemplo parece obvio ilustra una idea muy importante, porque nos da un criterio para ingresar una nueva variable a un modelo de regresi√≥n: Ingr√©sela cuando dicha variable se relaciona bien con los residuales generados por la primera versi√≥n de mi modelo!!!

[Prueba de Ljung-Box:]{style="color: red;"} Esta prueba se utiliza para evaluar la autocorrelaci√≥n en los residuales en varios retardos. Si los residuales son independientes, las autocorrelaciones en los retardos deber√≠an ser cercanas a cero. Por varios retardos entendemos inspeccionar relaci√≥n no s√≥lo con el valor del residual inmediatamente anterior, sino contra cualquier otro ubicado en un momento m√°s anterior. üîé Es una mejora a la prueba de Durbin-Watson en la medida en que no limita la autocorrelaci√≥n al retardo anterior.

Estas son algunas de las formas comunes de comprobar la independencia de los errores en un modelo de regresi√≥n lineal. Si encuentras evidencia de autocorrelaci√≥n o patrones en los residuales, puede ser necesario considerar modelos m√°s avanzados, como üìâ üö™**modelos autorregresivos o modelos de series temporales**, para capturar adecuadamente la estructura de dependencia en los datos.

**En conclusi√≥n**:

En regresi√≥n lineal asumimos la no correlaci√≥n de los errores. En nuestro caso el precio del aguacate y el precio del d√≥lar son variables evolucionando en el tiempo, as√≠ que quisi√©ramos que tras el transcurrir del tiempo nuestro modelo no se deteriore. Una forma en que esto puede ocurrir es que alguna autocorrelaci√≥n temporal provoque residuales que van variando en sus propiedades conforme el tiempo pasa. Por tal motivo necesitamos t√©cnicas como las anteriores para probar independencia.

Por ejemplo puede ocurrir que el error se vaya volviendo m√°s grande en promedio con el paso del tiempo o vaya creciendo en dispersi√≥n, o cualquier otra anomal√≠a que impida suponer que tenemos un modelo estable para todo momento del tiempo. Los gr√°ficos de dispersi√≥n t√≠picos ilustran la asociaci√≥n entre la magnitud de la variable predictora y la variable respuesta, pero salvo que los errores se dibujen indexados por la fecha en la que tal error ocurri√≥, ser√° imposible apreciar posibles patrones temporales. Haremos esas inspecciones a continuaci√≥n üîé :

```{r}
hass_dolar %>% 
  ggplot(aes(x= precio_dolar, y= precio_aguacate_kg )) +
  geom_point()+ theme_light()+
  geom_smooth(method='lm', formula=y~x, se=TRUE, col='dodgerblue1')
```

# Varianza de los residuales del modelo

# Obteniendo los coeficientes.

```{r}
mod1$coefficients
```

# Valores ajustados - Fitted values.

```{r}
mod1$fitted.values %>% 
  head(20)
```

# Residuales

```{r}
mod1$residuals %>% 
  head(20)

# Los valores ajustados y los residuales tambi√©n se pueden recuperar usando las funciones fitted( ) y residuals( ). Consulte la ayuda de estas funciones para conocer otros detalles.

```

Calcular la varianza de los residuales.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.



```

# Porcentaje de variabilidad no explicada por el modelo.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.




```

# Diagrama de dispersi√≥n con los puntos originales

## Creaci√≥n de la columna de predicci√≥n

En este chunk trabajamos con el modo de escritura Tidyverse, aunque se
muestra c√≥mo ser√≠a con R base.

```{r}

#hass_dolar$predicciones <- predict(mod1)

hass_dolar <- hass_dolar %>% 
  mutate(predicciones = predict(mod1))


```

```{r}


hass_dolar %>% 
  ggplot(aes(x = precio_dolar, y = precio_aguacate_kg)) +
  geom_smooth(method = "lm", se = FALSE, color="lightblue") +
  geom_segment(aes(xend=precio_dolar, yend=predicciones),
               col='red', lty='dashed') +
  geom_point() +
  geom_point(aes(y=predicciones), col='red') +
  theme_light()


#Con R Base
# ggplot(datos, aes(x=Edad, y=Resistencia)) +
#   geom_smooth(method="lm", se=FALSE, color="lightgrey") +
#   geom_segment(aes(xend=Edad, yend=predicciones), col='red', lty='dashed') +
#   geom_point() +
#   geom_point(aes(y=predicciones), col='red') +
#   theme_light()

```

# REVISI√ìN B√ÅSICA DE CONCEPTOS - Simulaci√≥n.

Vamos a simular un modelo de regresi√≥n cuya especificaci√≥n funcional es
la siguiente:

$$ y = \beta_0 + \beta_1 * x + e $$

Con $e \text{~} N(0,\sigma^2)$

Como se puede notar, todo modelo teorico se compone de dos t√©rminos:

1)  El valor E(y\|x)= E_y_x (valor esperado de y dado x)
2)  El componente aleatorio tambi√©n llamado error.

Aunque en el modelo anterior hemos elegido una estructura lineal para
representar E(y\|x) en realidad podemos elegir cualquier otra estructura
alternativa, siempre que respetemos la linealidad en los betas.

```{r}

bo = 2
b1_1 = 3
b1_2 = 0.5
x <- seq(1,10)

# Estructura para el valor esperado de y dado x ()
# Esta estructura admitir√≠a otras formas funcionales

# El valor esperado es deterministico.
E_y_x_1 <- bo + b1_1 * x

E_y_x_2 <- bo + b1_1 * x + b1_2 * x^2


# el valor observado o valor real, es aleatorio.



y_obs_1 <- E_y_x_1 + rnorm(10, 0, 4)

y_obs_2 <- E_y_x_2 + rnorm(10, 0, 4)

datos_simulados <- data.frame(
  x = x,
  x_2 = x^2,
  E_y_x_1 = E_y_x_1,
  E_y_x_2 = E_y_x_2,
  y_obs_1 = y_obs_1,
  y_obs_2 = y_obs_2,
  y_pred_1 = predict(lm(y_obs_1 ~ x)),
  y_pred_2 = predict(lm(y_obs_2 ~ x+ x^2)))


```

# Dibujamos un gr√°fico de dispersi√≥n

```{r}
datos_simulados %>% 
  ggplot(aes(x = x, y = y_obs_1)) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE, color="lightblue") +
  geom_point(col = "green")+
  geom_point(aes(y=y_obs_2), col='red')+ 
  geom_smooth(method="lm", se= FALSE ,formula=y_obs_2~poly(x, 2),color = "blue")+
  ylab("Y")
```

# DISCUTIENDO SOBRE LA LINEALIDAD DE LOS BETAS

En esta secci√≥n vamos a aclarar el asunto de que los BETAS sean
lineales.

La linealidad en este contexto hace referencia a que no tengan potencias
y que los **coeficientes sean independendientes entre ellos**.

Supongamos un modelo con coeficientes repetidos.

$$E(y|x) = \beta_0 + \alpha * x + \alpha * x^2 $$

Debemos factorizarlos para evitar estimarlos por separado y producir
inconsistencias,esto debido a que ambos alpha (coeficientes) son
dependientes.Es decir, reescribimos:

$$E(y|x) = \beta_0 + \alpha (x + x^2) $$

Donde el componente x + x¬≤ ser√° el vector de valores de la variable
predictora, que puede pensarse como una variable nueva:

$$ \tilde{x} = x + x^2$$

Y por lo tanto pensar al modelo como:

$$ E(y|x) = \beta_0 + \alpha * \tilde{x}$$

Esta transformaci√≥n garantiza la creaci√≥n de un nuevo modelo lineal
respecto a $\beta_0$ y $\alpha$.

Es importante anotar que el dataframe que alimentar√° el modelo deber√°
tener computada la variable auxiliar $\tilde{x}$, la cual debe ser
pasada a la funci√≥n encargada de estimar par√°metros.

Esto corresponde a crear una nueva variable con los c√°lculos mencionados
$\tilde{x} = x + x^2$. Sin embargo, para graficar es conveniente usar la
variable original.

```{r}

# Se define una semilla para generar los aleatorios.
set.seed(77)

alpha <- 0.7

E_y_x_3 = bo + alpha * (x + x^2)
y_obs_3 = E_y_x_3 + rnorm(10, 0, 4)

x_auxiliar = x + x^2

datos_simulados_2 <- data.frame(
  x = x,
  x_2 = x^2,
  x_auxiliar = x_auxiliar,
  E_y_x_3 = E_y_x_3,
  y_obs_3 =  y_obs_3,
  y_pred_3_1 = predict(lm(y_obs_3 ~ poly(x, 2))),
  y_pred_3_2 = predict(lm(y_obs_3 ~ x_auxiliar )))


modelo1a <-  lm(y_obs_3 ~ poly(x, 2))
modelo1b <-  lm(y_obs_3 ~ poly(x, 2, raw = TRUE))
modelo2 <-  lm(y_obs_3 ~ x_auxiliar )

```

Notese que los coeficientes arrojados por el modelo1a al invocar la
funci√≥n poly sin par√°metro de raw=TRUE, no son atribuibles o no est√°n
asociados a los t√©rminos $1,x,x^2$, es decir, **No es v√°lido escribir**:

$$E(y|x)= `r round(modelo1a$coefficients[1], 2)` * 1 + `r round(modelo1a$coefficients[2], 2)` * x + `r round(modelo1a$coefficients[3], 2)` * x^2$$

Esta ecuaci√≥n no ser√≠a v√°lida porque al remplazar para $x=5$ se obtiene
$E(y|5)=$
`r round(modelo1a$coefficients[1] * 1 + modelo1a$coefficients[2] * 5 + modelo1a$coefficients[3] * 5^2, 2)`,
mientras que al remplazar en el modelo con el par√°metro raw = TRUE, se
obtendr√≠a: $E(y|5)=$
`r round(modelo1b$coefficients[1] * 1 + modelo1b$coefficients[2] * 5 + modelo1b$coefficients[3] * 5^2, 2)`.

Observe que para el valor $x=5$ los valores observados de $y$ est√°n
cercanos a 24 por lo que una predicci√≥n arrojando un valor cercano a
836, est√° claramente desfasada. Observe el gr√°fico m√°s abajo para
comprender lo explicado.

Esto se debe a que los coeficientes anclados al modelo_1a est√°n
dise√±ados para ser usados sobre la siguiente base de polinomios
ortogonales:

-   $P_1(x)= a$
-   $P_2(x)=m*x - c$
-   $P_3(x)= k_1 + k_2 *x + k_3 * x ^2$

Donde $a,m,c,k_1,k_2,k_3$ son par√°metros estimados que lamentablemente
no est√°n siendo arrojados por el modelo.

A continuaci√≥n se muestra el resumen de cada uno de los modelos,
utilizando la funci√≥n summary().

```{r}


summary(modelo1a)
summary(modelo1b)
summary(modelo2)




```

```{r}

datos_simulados_2 %>% 
  ggplot(aes(x = x , y = y_obs_3))+
  geom_point(col = 'green')+
  geom_point(aes(y = y_pred_3_1), col = 'blue')+
  geom_point(aes(y = y_pred_3_2), col = 'red')


```

```{r eval=FALSE, include=FALSE}

# TAREA,
# Buscar la descomposici√≥n ortogonal de los vectores.

u1 = 1/sqrt(2)
u2 = sqrt(3/2) * (5-1)
u3 = sqrt(5/2) * ((3/2)*(5^2) - 3*5 + 1)

#E_y_5 = coef1 * u1 + coef2*u2 + coef3*u3
E_y_5 = modelo1a$coefficients[1] * u1 + modelo1a$coefficients[2]*u2 + modelo1a$coefficients[3]*u3

modelo1a$coefficients[1]
modelo1a$coefficients[2]
modelo1a$coefficients[3]

E_y_5

```

# ESTIMACI√ìN DE PAR√ÅMETROS

## Por m√°xima verosimilitud.

## Por m√≠nimos cuadrados.

```{r}


n <- 1000

vector_x <- c(1: n ) 
sigma <- 4 
beta_0 <- 2
beta_1 <- 0.5

# Esperada de y dado x
E_y_x <- beta_0 + (beta_1 * vector_x )

print(E_y_x)

plot(vector_x,E_y_x)



```

```{r}


error <- rnorm(n = n, mean = 0, sd = sigma)

y_observado <- beta_0 + (beta_1 * vector_x ) + error

print(y_observado)

plot(vector_x,y_observado)


```

# Modelo lineal

```{r}

modelo_y_predicho <- lm(y_observado ~ vector_x)

y_predicho <- predict(modelo_y_predicho)

print(y_predicho)


```

# Uni√≥n en un dataframe

```{r}

datos_modelo <- as.data.frame(vector_x) %>% 
  mutate(E_y_x = E_y_x,
         y_observado = y_observado,
         y_predicho = y_predicho)

```

# Gr√°ficando los valores

```{r}

datos_modelo %>% 
  ggplot(  )+
  geom_point( aes( x= vector_x, y = E_y_x), col = "blue")+
  geom_point( aes( x= vector_x, y = y_observado), col = "red")+
  geom_point( aes( x= vector_x, y = y_predicho), col = "brown")+
  geom_line(aes( x= vector_x, y = E_y_x), col = "blue")+
  geom_line(aes( x= vector_x, y = y_predicho), col = "black")

```

# Diferencia entre esperado y el predicho (Error)

```{r}


modelo_y_predicho$coefficients

b_0_estimado <- modelo_y_predicho$coefficients[1]
b_1_estimado <- modelo_y_predicho$coefficients[2]



datos_modelo <- datos_modelo %>% 
  mutate(error_predicho_observado = y_predicho - y_observado) 



```

# Construir funci√≥n

```{r}


estimar_betas <-  function(n,b0,b1,sigma,distribucion){
  
  
  if (distribucion == "normal") {
    
    errores <- rnorm(40,0,4)
    
  } else
    
  { errores <- runif(40,-6.928203,6.928203)  }
  
  
  vector_x <- c(1: n) 
  E_y_x <- b0 + (b1 * vector_x )
  
  #error <- rnorm(n = n, mean = 0, sd = sigma)
  #error <- runif(n = n, min=-6.928203, max = 6.928203)
  
  
  y_observado <- E_y_x + errores
  modelo <- lm(y_observado ~ vector_x)
  y_predicho <- predict(modelo)
  
  b0_estimado <- modelo$coefficients[1]
  b1_estimado <- modelo$coefficients[2]
  
  return(list("b0_estimado" = b0_estimado,
              "b1_estimado" = b1_estimado))
  
}



```

# Usando la funci√≥n

```{r}

resultados1 <- estimar_betas(40,beta_0,beta_1,sigma,"normal")
# resultados2 <- estimar_betas(40,beta_0,beta_1,sigma)
# resultados3 <- estimar_betas(40,beta_0,beta_1,sigma)

print(resultados1)
# print(resultados2)
# print(resultados3)


```

# Creando vector BETAS

```{r}

vector_betas <- function(n,beta_0,beta_1,sigma, n_sim, distribucion) {
  
  vector_b0 <- vector()
  vector_b1 <- vector()
  
  for (i in 1:n_sim) {
    
    resultados <-  estimar_betas(n,beta_0,beta_1,sigma,distribucion)
    
    vector_b0[i] <- resultados$b0_estimado
    vector_b1[i] <- resultados$b1_estimado
    
  }
  
  df_resultados <- data.frame(
    b0 = vector_b0,
    b1 = vector_b1)
  
  
  
  return(df_resultados)
  
}


```

```{r}


df_resultados1 <- vector_betas(40,2,2,4,1000,"normal")
df_resultados2 <- vector_betas(40,2,2,4,1000,"runif")


```

# Histograma comparando distribuciones.

```{r}

histo_b0_normal <- df_resultados1 %>% 
  ggplot()+
  geom_histogram(aes(x = b0))+
  ylim(0,110)

histo_b0_uniforme <- df_resultados2 %>% 
  ggplot()+
  geom_histogram(aes(x = b0))+
  ylim(0,110)

grid.arrange(histo_b0_normal,
             histo_b0_uniforme, 
             ncol = 2)


```

```{r}

histo_b0_normal <- df_resultados1 %>% 
  ggplot()+
  geom_histogram(aes(x = b0))+
  ylim(0,110)

histo_b1_normal <- df_resultados1 %>% 
  ggplot()+
  geom_histogram(aes(x = b1))+
  ylim(0,110)

grid.arrange(histo_b0_normal,
             histo_b1_normal, 
             ncol = 2)


```

```{r include=FALSE}

# Este chunk se configur√≥ para no imprimirse

vector_betas(100,2,2,4,1000,"normal")



```
