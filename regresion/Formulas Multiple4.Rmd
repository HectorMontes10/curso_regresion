---
title: "REGRESIN LINEAL MLTIPLE"
subtitle: "Maestr铆a en Investigaci贸n Operativa y Estad铆stica"
author:
  - "Juli谩n Piedrah铆ta Monroy"
  - "H茅ctor Hern谩n Montes"
output: 
  rmdformats::readthedown:
    css: styles.css
  bookdown::html_document2:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
date: "2023"
editor_options: 
  markdown: 
    wrap: 72
  wrap: 72
chunk_output_type: inline
---

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
```
<div>

<img src="https://media2.utp.edu.co/imagenes/Logo-UTP-Azul.png" alt="UTP" class="watermark"/>

</div>

```{r include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = TRUE,
  message = FALSE,
  options(scipen=999)
)

# Librer铆as
library(tidyverse)
library(janitor)
library(openxlsx)
library(flextable)
library(viridis)
library(scales)
library(DT)
library(lubridate)
library(gridExtra)
library(ggplot2)
library(gganimate)
library(animation)
library(gifski)
library(magick)
library(nortest)
library(fitdistrplus)
library(tseries)
library(lmtest)
library(stats)
library(sandwich)
#install.packages("devtools")
library(devtools)

```



```{r}

devtools::install_github("kassambara/datarium")

data("marketing", package = "datarium")
head(marketing, 4)


```


```{r}

model <- lm(sales ~ youtube + facebook + newspaper, data = marketing)
summary(model)

```

```{r}

summary(model)$coefficient

```

```{r}

model  <- lm(sales ~ youtube + facebook, data = marketing)
summary(model)


```
sales = 3.5 + 0.045\*youtube + 0.187\*facebook

```{r}

confint(model)


```

```{r}

sigma(model)/mean(marketing$sales)


```

$y=x\beta+\varepsilon$

En donde

$y= \begin{bmatrix}y_1 \\y_2 \\\vdots \\y_n\end{bmatrix}, x=\begin{bmatrix}1 & X_{11} & X_{12} & \cdots & X_{1k}\\1 & X_{21} & X_{22} & \cdots & X_{2k}\\\vdots & \vdots & \vdots & \ddots & \vdots \\1 & X_{n1} & X_{n2} & \cdots & X_{nk}\end{bmatrix}$

$\beta=\begin{bmatrix}\beta_1 \\\beta_2 \\\vdots \\\beta_n\end{bmatrix},  \varepsilon=\begin{bmatrix}\varepsilon_1 \\\varepsilon_2 \\\vdots \\\varepsilon_n\end{bmatrix}$

Se desea determinar el vector $\hat{\beta}$ de estimadores de m铆nimos cuadrados que minimice.

$S(\beta) = \sum\limits_{i=1}^{n} (\varepsilon_i^2) = \varepsilon' \varepsilon= (y-X\beta)'(y-X\beta)$

N贸tese que $S(\beta)$ se puede expresar como sigue:

$S(\beta) = y'y - \beta'X'y - y'X\beta + \beta'X'X\beta = y'y-2\beta'X'y + \beta'X'X\beta$

ya que $\beta'X'y$  es una matriz de 1x1, es decir, un escalar, y que su transpuesta $(\beta'X'y)'=y'X\beta$ es el mismo escalar. Los estimadores de m铆nimos cuadrados deben satisfacer:

$\left.\frac{\partial S}{\partial \beta}\right|_{\hat{\beta}}  = -2X'y + 2X'X\beta  = 0$

Que simplifica a:

\begin{equation}X'X\beta  = X'y \label{eq:ref1}\end{equation}

As铆, el estimador de $\beta$ por m铆nimos cuadrados es:


\begin{equation}\beta' = (X'X)^{-1}X'y\label{eq:ref2}\end{equation}


Al escribir (\ref{eq:ref1}) con detalle se obtiene.


$\begin{bmatrix}n & \sum\limits_{i=1}^{n} x_{i1} & \sum\limits_{i=1}^{n} x_{i2} & \cdots & \sum\limits_{i=1}^{n} x_{ik}\\ \sum\limits_{i=1}^{n} x_{i1} & \sum\limits_{i=1}^{n} x_{i1}^2 & \sum\limits_{i=1}^{n} x_{i1} x_{i2} & \cdots & \sum\limits_{i=1}^{n} x_{i1} x_{ik}\\\vdots & \vdots & \vdots & \ddots & \vdots \\\sum\limits_{i=1}^{n} x_{ik} & \sum\limits_{i=1}^{n} x_{ik}x_{i1} & \sum\limits_{i=1}^{n} x_{ik}x_{i2} & \cdots & \sum\limits_{i=1}^{n} x_{ik}^2\end{bmatrix} \begin{bmatrix}\hat{\beta_0}\\\hat{\beta_1}\\\vdots \\\hat{\beta_k}\end{bmatrix}=\begin{bmatrix}\sum\limits_{i=1}^{n} y_i\\\sum\limits_{i=1}^{n} x_{i1}y_i\\\vdots \\\sum\limits_{i=1}^{n} x_{ik}y_i\end{bmatrix}$


El modelo ajustado de regresi贸n que corresponde a los niveles de las variables regresoras $x'=[1,x_1,x_2,...,x_k]$ es:

$\hat{y} = x' \hat{\beta} = \hat{\beta_0} + \sum\limits_{j=1}^{k} \hat{\beta_j}x_j$

El vector de valores ajustados $\hat{y_i}$ que corresponden a los valores observados $y_i$ es:

$\hat{y} = X\hat{\beta} = X(X'X)^{-1}X'y = Hy$

La diferencia entre el valor observado $y_i$, y el valor ajustado $\hat{y_i}$ correspondiente es el **residual** $e_i = y_i - \hat{y_i}$. Los n residuales se pueden escribir c贸modamente con notaci贸n matricial como sigue:

$e = y - \hat{y}$

Hay otras maneras de expresar el vector de residuales $e$, que pueden ser 煤tiles, como:

$e= y - X\hat{\beta}=y-Hy = (I - H) y$

... Ac谩 continuar铆a el ejemplo 3.1 Datos del tiempo de entrega.

Estimaci贸n de $\sigma^2$

Como en la regresi贸n lineal simple, se puede desarrollar un estimador de $\sigma^2$ a partir de la suma de cuadrados de residuales.

$SS_{Res} \sum\limits_{i=1}^{n} (y_i - \hat{y_i})^2$
$= \sum\limits_{i=1}^{n} e_i^2$
$=e'e$

Se sustituye $e= y - X\hat{\beta}$ y se obtiene:

$SS_{Res} = (y - X\hat{\beta})' (y - X\hat{\beta})$
$ = y'y - \hat{\beta'}X'y - y'X\hat{\beta} + \hat{\beta'}X'X\hat{\beta}$
$ = y'y - 2\hat{\beta'}X'y + \hat{\beta'}X'X\hat{\beta}$

Como $X'X\hat{\beta} = X'y$, la 煤ltima ecuaci贸n se transforma en:

$SS_{Res} = y'y - \hat{\beta'}X'y$

El **cuadrado medio residual**, o **cuadrado medio de residuales** es:

$MS_{Res} = SS_{Res} / n - p$

$MS_{Res} = \frac{SS_{Res}}{n - p}$

Tambien existe la demostraci贸n de que el valor esperado de $MS_{Res}$ es $\sigma^2$, por lo que un estimador insesgado de $\sigma^2$ es:
$\hat{\sigma}^2 = MS_{Res}$

Como se dijo en el caso de regresi贸n lineal simple, el estimador de $\sigma^2$ **depende del modelo**


# Prueba de hip贸tesis en la regresi贸n lineal m煤ltiple

## Prueba de la significancia de la regresi贸n.

Las hip贸tesis pertinentes son:

$H_0: \beta_1=\beta_2= ... = \beta_k=0$
$H_1: \beta_j \neq 0$  al menos para una j

El rechazo de la hip贸tesis nula implica que al menos uno de los regresaores $x_1, x_2, ..., xk$ contribuye al modelo en forma significativa.

El procedimiento de prueba es una generalizaci贸n del an谩lisis de varianza que se us贸 en la regresi贸n lineal simple. La suma total de cuadrados $SS_T$ se divide en una suma de cuadrados debidos a la regresi贸n, $SS_R$, Y A UNA SUMA DE CUADRADOS DE RESIDUALES, $SS_{Res}$. As铆,

$SS_T = SS_R + SS_{Res}$

De acuerdo con la definici贸n del estad铆stico F.

$F_0 = \frac{SS_R/k}{SS_{Res}/(n - k -1)} = \frac{MS_R}{MS_{Res}}$

Tiene la distribuci贸n $F_{k,n-k-1}$. Es demostrable que:

$E(MS_{Res}) = \sigma^2$

$E(MS_{R}) = \sigma^2 + \frac{ \beta^*麓X'_c X_c \beta^*}{k\sigma^2}$


Siendo $\beta^*$ = $(\beta_1,\beta_2... \beta_k)$ y $X_c$ es la matriz "centrada" del modelo, definida por:


Claro, aqu铆 tienes una matriz de 6 filas por 3 columnas donde cada elemento es "x-x":


$\begin{bmatrix}x_{11}-\bar{x}_{1} & x_{12}-\bar{x}_{2} & . . . & x_{1k}-\bar{x}_{k} \\ x_{21}-\bar{x}_{1} & x_{22}-\bar{x}_{2} & . . . & x_{2k}-\bar{x}_{k} \\\vdots & \vdots & & \vdots \\ x_{i1}-\bar{x}_{1} & x_{i2}-\bar{x}_{2} & . . . & x_{ik}-\bar{x}_{k} \\\vdots & \vdots & & \vdots \\ x_{n1}-\bar{x}_{1} & x_{n2}-\bar{x}_{2} & . . . & x_{nk}-\bar{x}_{k} \\end{bmatrix}$

Estos cuadrados medios esperados indican que si el valor observado de $F_0$ es grande, es probable que al menos una $\beta_j \neq 0$. Tambi茅n se puede demostrar que si al menos una $\beta_j \neq 0$, entonces $F_0$ tiene una distribuci贸n F no central, con $k$ y $n-k-1$ grados de libertad, y par谩metro de no centralidad definido por:

`atenci贸n con esto porque me parece que hay un error en el libro con los betas de la siguiente f贸rmula.

$\lambda = \frac{\beta^{*}麓X'_c X_c \beta^*}{\sigma^2}$

Este par谩metro de no centralidad...
Por consiguiente, para probar la hip贸tesis $H_0: \beta_1=\beta_2= ... = \beta_k=0$, se calcula el estad铆stico de prueba $F_0$ y se rechaza $H_0$ si:

$F_0 > F_{\alpha,k,n-k-1}$

El prodecimiento de prueba se resume normalmente en una **tabla de an谩lisis de varianza**.
Un f贸rmula de c谩lculo para $SS_{R}$ se deduce partiendo de:

$SS_{Res} = y'y - \hat{\beta'}X'y$

y ya que:

$SST = \sum\limits_{i=1}^{n} y_i^2 - \frac{(\sum\limits_{i=1}^{n} y_i)^2}{n} = y'y - \frac{(\sum\limits_{i=1}^{n}y_i)^2}{n}$

Se puede escribir la ecuaci贸n anterior en la forma:

$SS_{Res} = y'y - \frac{(\sum\limits_{i=1}^{n}y_i)^2}{n} - \begin{bmatrix} \hat{\beta'} X'y - \frac{(\sum\limits_{i=1}^{n}yi)^2}{n}\\\end{bmatrix}$

o bien,

$SS_{Res} = SS_T - SS_R$

Por consiguiente, la **suma de cuadrados de la regresi贸n** es:

$SS_{R} = \hat{\beta'}X'y - \frac{(\sum\limits_{i=1}^{n}yi)^2}{n}$

La **suma de cuadrados de residuales**, o **suma residual de cuadrados** es:

$SS_{Res} = y'y - \hat{\beta'}X'y$

y la **suma total de cuadrados** es:

$SS_T = y'y - \frac{(\sum\limits_{i=1}^{n}yi)^2}{n}$



$R^2$ y $R^2$ ajustada

$R^2_{Adj} = 1 - \frac{SS_{Res} / (n-p)}{SS_T / (n-1)}$

En vista de que $SS_{Res}/(n-p)$ es el cuadrado medio de residuales y $SS_T / (n-1)$ es constante, independientemente de cu谩ntas variables hay en el modelo,
$R^2_{Adj}$ s贸lo aumentar谩 al agregar una variable al modelo si esa adici贸n reduce el cuadrado medio residual.

# Pruebas sobre coeficientes individuales de regresi贸n.

$H_0: \beta_j = 0$
$H_1: \beta_j \neq 0$

Si no se rechaza $H_0: \beta_j = 0$, quiere decir que se puede eliminar el regresor $x_j$ del modelo. El **estad铆stico de prueba** para esta hip贸tesis es:

$t_0 = \frac{\hat{\beta_j}}{\sqrt{\hat{\sigma}^2 C_{jj}}} = \frac{\hat{\beta_j}}{se(\hat{\beta_j})}$

Donde $C_{jj}$ es el elemento diagonal de $(X'X)^{-1}$ que corresponde a $\hat{\beta}_j$. Se rechaza la hip贸tesis nula $H_0:\beta_j=0$ si:

$|t_0|>t_{\alpha/2,n-k-1}$. N贸tese que 茅sta es en realidad una **prueba parcial** o **marginal**, porque el coeficiente de regresi贸n $\hat{\beta_j}$ depende de todas las dem谩s variables regresoras $x_i (i \neq j)$, que hay en el modelo. As铆, se trata de una prueba de la **contribuci贸n** de $x_j$ **dados los dem谩s regresores del modelo**


# Predicci贸n de nuevas observaciones

Con el modelo de regresi贸n se pueden predecir observaciones futuras de $y$ que correspondan a determinados valores de las variables regresoras, por ejemplo $x_{01},x_{02},...,x_{0k}$. Si $x_0' = [1,x_{01},x_{02},...,x_{0k}]$, entonces un **estimado puntual de la observaci贸n futura** $y_0$ en el punto $x_{01},x_{02},...,x_{0k}$ es:

$\hat{y}_0 = x'_0 \hat{\beta}$

Un **intervalo de predicci贸n de** $100(1-\alpha)$ **por ciento** para esta futura observaci贸n es:

$\hat{y}_0 - t_{\alpha/2,n-p} \sqrt{\hat{\sigma}^2 (1 + x'_0 (X'X)^{-1}x_0)} \leq y_0 \leq \hat{y}_0 + t_{\alpha/2,n-p} \sqrt{\hat{\sigma}^2 (1 + x'_0 (X'X)^{-1}x_0)}$

Es una generalizaci贸n de intervalo de predicci贸n para una futura observaci贸n en la regresi贸n lineal simple.


# Balanceo

El lugar de los puntos en el espacio de $x$ tiene importancia potencial en la determinaci贸n de las propiedades del modelo de regresi贸n. En particular, los puntos alejados o remotos tienen un impacto desproporcionado sobre los estimados de los par谩metros, los errores est谩ndar, valores predichos y estad铆sticas de resumen del modelo. La matriz de sombrero...

$H = X(X'X)^{-1}X'$

Desempe帽a un papel importante en la identificaci贸n de observaciones influyentes. Como se dijo antes, $H$ determina las varianzas y covarianzas de $\hat{y}$ y de $e$, porque $Var(\hat{y}) = \sigma^2H$ y $Var(e) = \sigma^2(I-H)$. Los elementos de $h_{ij}$ de la matriz $H$ pueden ser vistos como la cantidad de **balanceo** o palanqueo ejercido por la *i_茅sima* observaci贸n $y_i$ sobre el *i_茅simo* valor ajustado de $\hat{y}_i$.


Con frecuencia, la atenci贸n se dirige hacia los elementos diagonales $h_{ii}$ de la matriz H sombrero, que se pueden expresar como:

$h_{ii} = x_i'(X'X)^{-1}x_i$

Sucede que el tama帽o promedio de los elementos de la diagonal es $\hat{h} = p/n$ porque $\sum\limits_{i=1}^{n}h_{ii} = rango(H) = rango(X) = p$, y por tradici贸n se supone que toda observaci贸n para la cual la diagonal del sombrero es m谩s del doble del promedio $2p/n$ est谩 suficientemente alejada del resto de los datos como para considerarse un **punto de balanceo**. Como los elementos diagonales de la matriz de sombrero s贸lo examinan el lugar de la observaci贸n en el espacio de x, algunos analistas prefieren examinar los residuales estudentizados, o los residuales $R$ de Student **junto con** las $h_{ii}$...

# Medidas de influencia: La D de Cook

Mencionando la preferencia de tener en cuenta el lugar del punto en el espacio de $x$ y tambi茅n la variable de respuesta, al medir la influencia. Cook ha sugerido una forma de hacerlo, con una medida de la distancia, elevada al cuadrado, entre el estimado por m铆nimos cuadrados basado en los n puntos $\hat{\beta}$, y el estimado obtenido eliminando el i_茅simo punto, por ejemplo $\hat{\beta}_{(i)}$. Esta medida de la distancia se puede expresar como sigue, en forma general:

$D_i(M,c) = \frac{(\hat{\beta_{(i)}}-\hat{\beta})'M(\hat{\beta_{(i)}}-\hat{\beta})}{c}, i = 1,2, ... , n$ 

Las opciones comunes de $M$ y $c$ son $M = X'X$ y $c = pMS_{Res}$, por lo que la ecuaci贸n se transforma en:

$D_i (X'X, pMS_{Res}) \equiv D_i = \frac{(\hat{\beta_{(i)}}-\hat{\beta})'X'X(\hat{\beta_{(i)}}-\hat{\beta})}{pMS_{Res}}, i = 1,2, ... , n$

Los puntos con grandes valores de $D_i$ tienen gran influencia sobre el estimado de $\hat{\beta}$ por m铆nimos cuadrados.

La magnitud de $D_i$ se suele evaluar compar谩ndola con $F_{\alpha,p,n-p'}$ Si $D_i = F_{0.5,p,n-p'}$ entonces al eliminar el punto $i$ se mover铆a $\hat{\beta}_{(i)}$ hacia la frontera de una regi贸n de confianza aproximada de 50% para $\beta$, bas谩ndose en el conjunto completo de datos. Es un desplazamiento grande e indica que el estimado por m铆nimos cuadrados es sensible al i-茅simo punto de datos. Como $F_{0.5,p,n-p'} \approx 1 $, se suelen considerar como influyentes los puntos para los que $D_i > 1$. En el caso ideal ser铆a bueno que cada estimado $\hat{\beta_{(i)}}$ permaneciera dentro de
los l铆mites de la regi贸n de confianza de 10 o de 20%. Esta recomendaci贸n de corte se basa en la semejanza de Di con la ecuaci贸n del elipsoide de confianza de la teor铆a normal. La medida de distancia $D_i$ no es una estad铆stica F. Sin embargo, usar el corte igual a una unidad funciona muy bien en la pr谩ctica.

La estad铆stica $D_i$ se puede reexpresar como sigue:

$D_i = \frac{r_i^2}{p}\frac{Var(\hat{y_i})}{Var(\hat{e_i})} = \frac{r_i^2}{p} \frac{h_{ij}}{(1-h_{ij})}, i= 1,2,...,n$

As铆 se ve que, adem谩s de la constante $p$, la $D_i$ es el producto del cuadrado del i-茅simo residual estudentizado por $\frac{h_{ij}}{(1-h_{ij})}$.

Se puede demostrar que esta relaci贸n es la distancia del vector $x_i$ al centroide de los datos restantes. As铆, $D_i$ est谩 formada por un componente que refleja lo bien que se ajusta el modelo a la i-茅sima observaci贸n $yi$, y un componente que mide lo alejado que el punto est谩 del resto de los datos.

Cualquiera de los componentes (o ambos), pueden contribuir a un valor grande de $D_i$. As铆, en $D_i$ se combinan la magnitud del residual para la i_茅sima observaci贸n y la ubicaci贸n de ese punto en el espacio de $x$, para evaluar su influencia.

Ya que $X\hat{\beta_{(i)}} - X\hat{\beta} = \hat{y}_{(i)} - \hat{y}$, otra forma de expresar la medida de distancia de Cook es:

$D_i = \frac{(\hat{y}_{(i)}-\hat{y})'(\hat{y}_{(i)}-\hat{y})}{pMS_{Res}}$

As铆, otra forma de interpretar la distrancia de Cook es el cuadrado de la distancia euclidiana (sin considerar $pMS_{Res}$) que se mueve el vector de los valores ajustados cuando se elimina la i_茅sima observaci贸n.











