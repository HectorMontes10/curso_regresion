---
title: "SERIES DE TIEMPO - FORMULARIO"
subtitle: "Con base en libro de Morettin."
author:
  - "Héctor Hernán Montes"
  - "Julián Piedrahíta Monroy"
output: 
  rmdformats::readthedown:
    css: styles.css
  bookdown::html_document2:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
date: "2023"
editor_options: 
  markdown: 
    wrap: 72
  wrap: 72
chunk_output_type: inline
---

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
```
<div>

<img src="https://media2.utp.edu.co/imagenes/Logo-UTP-Azul.png" alt="UTP" class="watermark"/>

</div>

```{r include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = TRUE,
  message = FALSE,
  options(scipen=999)
)

# Librerías
library(tidyverse)
library(janitor)
library(openxlsx)
library(flextable)
library(viridis)
library(scales)
library(DT)
library(lubridate)
library(gridExtra)
library(ggplot2)
library(gganimate)
library(animation)
library(gifski)
library(magick)
library(nortest)
library(fitdistrplus)
library(tseries)
library(lmtest)
library(stats)
library(sandwich)
#install.packages("devtools")
library(devtools)

```


# Procesos Autoregresivos

Decimos que $\left\{X_t,t \in \mathbb{Z}\right\}$ es un proceso autoregresivo de orden $p$, y escribimos $X_t \sim\ AR(p)$, si satisface la ecuación de diferencias.

\begin{equation}X_t - \mu = \phi_1 (X_{t-1} - \mu) + \phi_2(X_{t-2}-\mu)+ ... + \phi_p(X_{t-p}-\mu)+ \varepsilon_t\label{eq:2.24}\end{equation}


Donde $\mu,\phi_1,...\phi_p$ son parámetros reales y $\varepsilon \sim\ RB(0,\sigma^{2})$ siendo RB = Ruido Blanco, que quiere decir que el proceso autoregresivo tiene como condición que el error sea Ruido Blanco. Nótese que $E(X_t)= \mu$ y si escribimos el proceso en la forma:

$$X_t = \phi_0 + \phi_1 X_{t-1} + ... \phi_p X_{t-p} + \varepsilon_t$$

Entonces:

$$\mu = E(X_t) = \frac{\phi_0}{1-\phi_{1} - ... - \phi_p}$$

Definamos el operador retroactivo B a través de $B^{s}X_t = X_{t-s},s \geq 1$. Entonces (\ref{eq:2.24}) puede ser escrita.

\begin{equation}\phi(B)\tilde{X}_t = \varepsilon_t\label{eq:2.25}\end{equation}

Donde $\phi(B) = 1 - \phi_1B - \phi_2B^2 - ... -\phi_pB^{p}$ es el operador autoregresivo de orden $p$ y $\tilde{X}_t = X_t - \mu$. Suponga $\mu = 0$ en adelante.
Un caso particular importante es el proceso AR(1).


\begin{equation}$$X_t = \phi X_{t-1} + \varepsilon_t$$\label{eq:2.26}\end{equation}.

Como $\phi(B) = 1 - \phi B$. A través de sustituciones sucesivas obtenemos.

$$X_t = \sum\limits_{j=0}^{r} \phi^j \varepsilon_{t-1} + \phi^{r+1}X_{t-r-1}$$

Se $X_t$ fuera estacionario, con la varianza finita $\sigma^2 X$, entonces.

$$E[X_t - \sum\limits_{j=0}^{r}\phi^j \varepsilon_{t-j}]^2 = \phi^{2r+2}E[X_{t-r-1}^2] = \phi^{2r+2}\sigma_{X}^2$$

Si $|\phi| < 1$, $\phi^{2(r+1)} \rightarrow 0$, cuando $r \rightarrow \infty$, por lo tanto sobre esta suposición, podemos escribir.

\begin{equation}X_t = \sum\limits_{j=0}^{\infty}\phi^j \varepsilon_{t-j}\label{eq:2.27}\end{equation}

Donde la convergencia es una media cuadratica. Luego, la condición $|\phi|<1$ es suficiente para $X_t$ ser estacionario. Multiplicando ambos miembros de (\ref{eq:2.26}) por $X_{t-\tau}$ y tomando la esperanza, obtenemos.

$$\gamma_{\tau} = \phi \gamma_{\tau-1} = ... = \phi^{\tau}\gamma_0$$
Mas de (\ref{eq:2.27}), obtenemos:


\begin{equation}\gamma_0 = \sigma^2{X} = \sigma^2 \sum\limits_{j=0}^{\infty}\phi^{2j} = \frac{\sigma^2}{1-\sigma^2}\label{eq:2.28}\end{equation}

De lo que sigue:

$$\gamma_{r} = \frac{\sigma^2}{1-\phi^2}\phi^{\tau}, \tau \geq 0$$

Como $\gamma_\tau$ es simétrica, podemos escribir finalmente la f.a.c.v (función de autocovarianza) de un proceso AR(1) como:

\begin{equation}\gamma_\tau = \frac{\sigma^2}{1 - \phi^2} \phi^{\lvert \tau \rvert}, \quad \tau \in\mathbb{Z}\label{eq:2.29}\end{equation}

La f.a.c de $X_t$ es obtenida de (\ref{eq:2.29}), es decir,


\begin{equation}\rho_\tau = \frac{\gamma_\tau}{\gamma_0} = \phi^{\lvert \tau \rvert}, \quad \tau \in \mathbb{Z}\label{eq:2.30}\end{equation}


# Processos de medias móviles

Decimos que $\{X_t, t \in \mathbb{Z}\}$ es un proceso de medias móviles de orden $q$, denotado como $MA(q)$, si satisface la ecuación de diferencias.

\begin{equation}X_t = \mu + \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q},\label{eq:2.36}\end{equation}

Donde $\mu,\theta_1, ..., \theta_q$ son constantes reales y $\varepsilon_t \sim\ RB(0,\sigma^2)$.

Nótese que $X_t$ es estacionario, con media $\mu$, y como $\varepsilon_t$ son no correlacionados, podemos obtener fácilmente la varianza del proceso.

\begin{equation} \sigma^2_X = \sigma^2 \left(1 + \theta^2_1 + \ldots + \theta^2_q\right)\label{eq:2.37}\end{equation}

Suponga $\mu=0$. En cuanto a la función de autocovarianza, tenemos.


$$\gamma_\tau = E\{X_t X_{t-\tau}\} = \gamma_\varepsilon(\tau) - \sum_{k=1}^q \theta_k \gamma_\varepsilon(k - \tau)$$

$$-\sum_{k=1}^q \theta_{\mathcal{l}} \gamma_\varepsilon(\tau + l) + \sum_{k=1}^q \sum_{l=1}^q \theta_k \theta_{l} \gamma_\varepsilon(\tau + l - k)$$

Donde estamos denotando por $\gamma_{\varepsilon}(\tau)$ una f.a.c.v (función de autocovarianza) de $\varepsilon_t$. Resulta entonces,

\begin{equation} \gamma_\tau = \begin{cases} \sigma^2 (-\theta_\tau + \theta_1 \theta_{\tau+1} + \ldots + \theta_q \theta_{q-\tau}), & \text{si } \tau = 1, \ldots, q \\ 0, & \text{si } \tau > q \\\gamma_{-\tau}, & \text{si } \tau < 0\end{cases}\label{eq:2.38}\end{equation}

De (\ref{eq:2.37}) y (\ref{eq:2.38}) obtenemos una f.a.c (función de autocorrelación) del proceso $MA(q)$

\begin{equation}\rho_\tau = \begin{cases}\frac{-\theta_\tau + \theta_1 \theta_{\tau+1} + \ldots + \theta_q \theta_{q-\tau}}{1 + \theta_1^2 + \ldots + \theta_q^2}, & \text{si } \tau = 1, \ldots, q \\0, & \text{si } \tau > q \\\rho_{-\tau}, & \text{si }\tau < 0\end{cases}\label{eq:2.39}\end{equation}

Observamos, entonces, que la f.a.c.v (o f.a.c) de un proceso $MA(q)$ se anula para $|\tau| > q$. En particular, para un proceso $MA(1)$,

\begin{equation}X_t = \varepsilon_t - \theta \varepsilon_{t-1}\label{eq:2.40}\end{equation}

Obtenemos

$$\text{Var}(X_t) = \sigma^2_X = \sigma^2 (1 + \theta^2)$$,

\begin{equation}\rho_\tau =\begin{cases}\frac{-\theta}{1 + \theta^2}, & \text{si } \tau = \pm 1 \\0, & \text{si } \lvert \tau\rvert > 1\end{cases}\label{eq:2.41}\end{equation}

Definiendose el operador de medias móviles de orden $q$ por:

$\theta(B) = 1 - \theta_1B - \theta_2B^2 - \ldots - \theta_qB^q$

El proceso (\ref{eq:2.36}) puede ser escrito

\begin{equation}X_t = \theta(B)\varepsilon_t\label{eq:2.42}\end{equation}

En particular, para el proceso $MA(1)$ tenemos $\theta(B)=1-\theta B$, de modo que podemos escribir.

$X_t = (1 - \theta B)\varepsilon_t$

De donde, formalmente sigue:

$\varepsilon_t = (1 - \theta B)^{-1}X_t = (1 + \theta B + \theta^2 B^2 + \ldots)X_t$,

Por lo que tenemos:

\begin{equation}X_t = -\theta X_{t-1} - \theta^2 X_{t-2} - \ldots + \varepsilon_t\label{eq:2.43}\end{equation}

Si $|\theta| < 1$, la serie del lado derecho de (\ref{eq:2.43}) converge. En esta ecuación, expresamos $X_t$ como un proceso autoregresivo de orden infinito. Decimos que $|\theta| < 1$ es una condición de invertibilidad para el proceso MA(1). En general, el proceso (\ref{eq:2.36}) puede escribirse de la siguiente manera:

\begin{equation} X_t = \sum\limits_{j=1}^{\infty}\pi_j X_{t-j} + \varepsilon_t \label{eq:2.44}\end{equation},

Si se satisface la siguiente condición de invertibilidad: todas las raíces de $\theta(B) = 0$ deben estar fuera del círculo unitario. Ver Box, Jenkins y Reinsel (1994) para obtener más detalles.

La relación (\ref{eq:2.44}) puede ser escrita:

\begin{equation}\pi (B)X_t = \varepsilon_t$\label{eq:2.45}\end{equation},

donde $\pi(B) = 1 - \pi_1B - \pi_2B^2 - \ldots$, de modo que $\pi(B) = \theta(B)^{-1}$. Por lo tanto, los coeficientes $\pi_j$ pueden obtenerse a partir de la identidad $\theta(B)\pi(B) = 1$.

\begin{equation} X_t = \varepsilon_t - 0.8 \varepsilon_{t-1}, \varepsilon_t \sim \text{i.i.d. }\mathcal{N}(0,1)\label{eq:2.46}\end{equation}


# Tabla: Comportamiento de las f.a.c y las f.a.c.p de un proceso ARIMA(p,d,q)


| Orden | (1,d,0) | (0,d,1) |
| :--- | :--- | :--- |
| Comportamiento de $p_k$| Disminuye exponencialmente |Solamente $\rho_1 \ne 0$  |
| Comportamiento de $\phi_{kk}$| Solamente $\phi_{11} \ne 0$ |Disminución exponencial dominante|
| Estimaciones iniciales| $\phi = \rho_1$ | $\rho_1 = -\frac{\theta}{1 + \theta^2}$ |
| Dominio o región admisible| $-1 < \phi < 1$ | $-1 < \theta < 1$ |

| Orden | (2,d,0) | (0,d,2) |
| :--- | :--- | :--- |
| Comportamiento de $p_k$| Mezcla de exponenciales u ondas  sinusoidales  amortiguadas | Solamente $\rho_1 \ne 0$ y $\rho_2 \ne 0$  |
| Comportamiento de $\phi_{kk}$| Solamente $\phi_{11} \ne 0$ y $\phi_{22} \ne 0$ |Dominada por una mezcla de exponenciales o   sinusoidales amortiguadas.|
| Estimaciones iniciales| $\begin{cases}\phi_1 = \frac{\rho_1 (1 - \rho_2)}{1 - \rho_1^2} \\\phi_2 = \frac{\rho_2 - \rho_1^2}{1 - \rho_1^2}\end{cases}$ | $\begin{cases}\rho_1 = \frac{-\theta_1 (1 - \theta_2)}{1 + \theta_1^2 + \theta_2^2}, \\\rho_2 =\frac{-\theta_2}{1 + \theta_1^2+\theta_2^2}\end{cases}$ |
| Dominio o región admisible| $\begin{cases}-1 < \phi_2 < 1 \\\phi_2 - \phi_1 < 1 \\\phi_2 + \phi_1 < 1\end{cases}$ | $\begin{cases}-1 < \theta_2 < 1,\\\theta_2 - \theta_1 < 1, \\\theta_2 + \theta_1 < 1\end{cases}$|

| Orden | (1,d,1) |
| :--- | :--- |
| Comportamiento de $p_k$ | Decae exponencialmente después del retraso (lag) 1|
| Comportamiento de $\phi_{kk}$ | Dominada por disminución exponencial después del retraso (lag) 1|
| Estimaciones iniciales |$\rho1 = \frac{(1 - \phi \theta)(\phi - \theta)}{1 + \theta^2 - 2 \phi \theta}, \rho_2 = \rho_1\phi$|
| Dominio o región admisible |$-1 < \phi <1 , -1 < \theta < 1$|



# Identificación de las series de tiempo según la tabla.


```{r}

# Comenzar a generar series de tiempo simuladas con cada uno de los casos que están en la tabla.
# Para construir con las series simuladas, los diagramas de autocorrelación y a través de la inspección de los afc, saber en qué lugar de la tabla me ubico.


# phi <- 0.7  # Coeficiente autorregresivo (AR)
# theta <- -0.4  # Coeficiente de promedios móviles (MA)
# n <- 100  # Número de observaciones a simular
# # Simula el proceso ARMA
# ar_puro <- arima.sim(model = list(order = c(1, 0, 0), ar = phi), n=n)
# ma_puro <- arima.sim(model = list(order = c(0, 0, 1), ma = theta), n=n)
# arma <- arima.sim(model = list(order = c(1, 0, 1), ar = phi, ma = theta), n=n)
# arima <- arima.sim(model = list(order = c(1, 1, 1), ar = phi, ma = theta), n=n)

# par(mfrow = c(2, 2))
# 
# plot(ar_puro, type = 'l', main = "Simulación de un Proceso AR(1)")
# plot(ma_puro, type = 'l', main = "Simulación de un Proceso MA(1)")
# plot(arma, type = 'l', main = "Simulación de un Proceso ARMA(1,1)")
# plot(arima, type = 'l', main = "Simulación de un Proceso ARIMA(1,1,1)")



```


# Descripción e identificación gráfica de las series de tiempo.


## Orden (1,d,0)


Una serie de tiempo de orden (1,d,0) se obtiene aplicando una diferencia de orden "d" una vez y luego añadiendo un componente autorregresivo de orden 1. En términos más simples, esto significa que la serie original no es estacionaria, pero después de aplicar una diferencia de primer orden (diferenciación) se convierte en una serie estacionaria. Luego, se agrega un componente autorregresivo de orden 1 que se basa en los valores previos de la serie.

Una serie de tiempo de orden (1,d,0) se puede describir con la siguiente ecuación:

$Y_t = \phi * Y_{(t-1)} + \varepsilon_t$


Para simular una serie de tiempo de este tipo, se pueden seguir estos pasos:

# Paso 1: 

Generar una serie temporal ARIMA de orden (1,0,0) que no esté diferenciada. Esto se hace utilizando la función **arima.sim** y especificando el modelo ARIMA deseado.

# Paso 2: 
Aplica la diferencia de primer orden "d" veces para hacer que la serie sea de orden (1,d,0).

A continuación, un ejemplo:

```{r}



# Paso 1: Generar una serie ARIMA de orden (1,0,0)
set.seed(77)  # Establecer una semilla para la reproducibilidad
n <- 100        # Número de observaciones
phi <- 0.7      # Parámetro AR(1)

# Genera una serie ARIMA de orden (1,0,0)
arima_order <- c(1, 1, 0)
ts_arima <- arima.sim(model = list(ar = phi, d = 1), n = n)

# ✔️
ts_arima_2 <- arima.sim(model =  list(order = arima_order, ar = phi), n = n)
# ✔️
ts_arima_vector <- ts_arima
ts_arima_vector_2 <- ts_arima

ts_arima <- ts_arima %>% as.data.frame()
ts_arima_2 <- ts_arima_2 %>% as.data.frame()

```


```{r}

#plot(ts_arima, main = paste("Serie de tiempo de orden (1,", d, ",0)"))

ts_arima %>% 
  mutate(Tiempo = as.numeric(row_number())) %>% 
  ggplot(aes(x = Tiempo, y = x)) +
  geom_line() +  # Línea para representar la serie de tiempo
  labs(x = "Tiempo", y = "Valor de x") +  # Etiquetas de los ejes
  ggtitle( paste("Serie de tiempo ARIMA1 de orden (1,1,0)")) # Título del gráfico



ts_arima_2 %>% 
  mutate(Tiempo = as.numeric(row_number())) %>% 
  ggplot(aes(x = Tiempo, y = x)) +
  geom_line() +  # Línea para representar la serie de tiempo
  labs(x = "Tiempo", y = "Valor de x") +  # Etiquetas de los ejes
  ggtitle( paste("Serie de tiempo ARIMA2 de orden (1,1,0)")) # Título del gráfico

```


```{r}

# Paso 2: Aplica la diferencia de primer orden "d" veces
d <- 2  # Número de diferenciaciones
ts_diff <- diff(ts_arima_vector, differences = d)

ts_diff <-  ts_diff %>% as.data.frame()


```

```{r}

ts_diff %>% 
  mutate(Tiempo = as.numeric(row_number())) %>% 
  ggplot(aes(x = Tiempo, y = x)) +
  geom_line() +  # Línea para representar la serie de tiempo
  labs(x = "Tiempo", y = "Valor de x") +  # Etiquetas de los ejes
  ggtitle( paste("Serie de tiempo de orden (1,", d, ",0)")) # Título del gráfico


```


En este ejemplo, hemos generado una serie de tiempo ARIMA de orden (1,0,0) con un parámetro AR(1) de 0.7 y luego aplicamos la diferencia dos veces para convertirla en una serie de tiempo de orden (1,2,0). Se puede ajustar el valor de "d" a gusto. La serie resultante es estacionaria después de aplicar la diferencia.


## Orden (0,d,1)

Una serie de tiempo de orden (0,d,1) se obtiene aplicando una diferencia de orden "d" una vez y luego añadiendo un componente de media móvil de orden 1. En términos más simples, significa que la serie original no es estacionaria, pero después de aplicar una diferencia de primer orden (diferenciación) se convierte en una serie estacionaria. Luego, se agrega un componente de media móvil de orden 1 que influye en los valores futuros de la serie.

Una serie de tiempo de orden (0,d,1) se puede describir con la siguiente ecuación:

$Y_t = Y_{(t-d)} +  \varepsilon_t + \theta *  \varepsilon_{t-1}$


Para simular una serie de tiempo de este tipo, se pueden seguir estos pasos:

# Paso 1: 

Generar una serie temporal ARIMA de orden (0,0,1) que no esté diferenciada. Esto se hace utilizando la función **arima.sim** y especificando el modelo ARIMA deseado.

# Paso 2: 
Aplica la diferencia de primer orden "d" veces para hacer que la serie sea de orden (0,d,1).

A continuación, un ejemplo:

```{r eval=FALSE, include=FALSE}

# Paso 1: Generar una serie ARIMA de orden (1,0,0)
set.seed(77)  # Establecer una semilla para la reproducibilidad
n <- 100        # Número de observaciones
#phi <- 0.7      # Parámetro AR(1)
error <- rnorm(n)

error_vector <- error

error %>% as.data.frame()

```


```{r eval=FALSE, include=FALSE}

#plot(ts_arima, main = paste("Serie de tiempo de orden (1,", d, ",0)"))

ts_arima %>% 
  mutate(Tiempo = as.numeric(row_number())) %>% 
  ggplot(aes(x = Tiempo, y = x)) +
  geom_line() +  # Línea para representar la serie de tiempo
  labs(x = "Tiempo", y = "Valor de x") +  # Etiquetas de los ejes
  ggtitle( paste("Serie de tiempo de orden (1,0,0)")) # Título del gráfico

```


```{r eval=FALSE, include=FALSE}

# Paso 2: Aplica la diferencia de primer orden "d" veces
d <- 2  # Número de diferenciaciones
ts_diff <- diff(ts_arima_vector, differences = d)

ts_diff <-  ts_diff %>% as.data.frame()


```

```{r eval=FALSE, include=FALSE}

ts_diff %>% 
  mutate(Tiempo = as.numeric(row_number())) %>% 
  ggplot(aes(x = Tiempo, y = x)) +
  geom_line() +  # Línea para representar la serie de tiempo
  labs(x = "Tiempo", y = "Valor de x") +  # Etiquetas de los ejes
  ggtitle( paste("Serie de tiempo de orden (1,", d, ",0)")) # Título del gráfico


```



# Ejercicios de práctica

## Ejemplo - Suponga el modelo AR(2)

$(1 - \phi_1 B - \phi_2 B^2)X_t = \phi_0 + \varepsilon_t$

Tenemos que:

$X_{T+h} = \phi_1 X_{T+h-1} + \phi_2 X_{T+h-2} + \phi_0 + \varepsilon_{T+h}$

Luego:

(i) Para $h=1$, tenemos $\hat{X}_T(1) = \phi_1X_T + \phi_2X_{T-1}+\phi_0$;
(ii) Para $h=2$, tenemos $\hat{X}_T(2) = \phi_1\hat{X}_T(1) + \phi_2X_{T}+\phi_0$;


(iii) Para h > 2, tenemos $\hat{X}_T(h) = \phi_1\hat{X}_T(h-1) + \phi_2\hat{X_T}{h-2}+\phi_0$


Escribiendo el modelo en la forma de medias moviles (infinito),se puede probar que el error de previsión es dado por.

$e_T(h) = \hat{X}_T(h) - \hat{X_T}(h)= \varepsilon_{T+h} + \psi_1\epsilon_{t+h-1} + \ldots + \psi_{h-1}\epsilon_{T+1}$

En el que los pesos $\psi_j$ vienen de $\psi(B) = \varphi^{-1}(B)\theta(B)$








