---
title: "SERIES DE TIEMPO"
subtitle: "Maestr√≠a en Investigaci√≥n Operativa y Estad√≠stica"
author: 
  - "Juli√°n Piedrahita Monroy"
  - "H√©ctor Hern√°n Montes"
output: 
  rmdformats::readthedown:
    css: styles.css
  bookdown::html_document2:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
date: "2023"
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: inline
---

<div>

<img src="https://media2.utp.edu.co/imagenes/Logo-UTP-Azul.png" alt="UTP" class="watermark"/>

</div>

```{=html}
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
```

```{r include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = TRUE,
  message = FALSE
)

# Librer√≠as
library(tidyverse)
library(janitor)
library(openxlsx)
library(flextable)
library(viridis)
library(scales)
library(DT)
library(lubridate)
library(gridExtra)
library(stats)
library(ggplot2)
library(gganimate)
source("funciones_personalizadas.R")
```

# 1. MOTIVACI√ìN

Partiendo desde lo financiero, la econom√≠a, e incluyendo el √°rea agr√≠cola y social, las series de tiempo permiten abordar una gran cantidad de problemas y situaciones desde la ciencia de datos. Las series de tiempo son una de las herramientas m√°s importantes para an√°lisis de las din√°micas sociales, entre otras, y la existencia de nuevas paqueter√≠as en lenguajes de programaci√≥n como R han permitido el abordaje m√°s comprensible de las mismas por parte de la comunidad acad√©mica e investigadores de todas las √°reas.

Por ejemplo, series de datos de:

- Epidemiolog√≠a: con datos sobre la propagaci√≥n de una enfermedad.
- Medicina: la medici√≥n a lo largo del tiempo de la presi√≥n de la sangre, permite evaluar la implementaci√≥n de medicamentos. (Shumway & Stoffer, 2017, 1)
Las im√°genes por resonancia magn√©tica funcional de patrones de series temporales de ondas cerebrales podr√≠an usarse para estudiar c√≥mo reacciona el cerebro a ciertos est√≠mulos en diversas condiciones experimentales.(Shumway & Stoffer, 2017, 1)

Las series de tiempo son la apertura al mundo que andan los expertos en ciencias de datos, para hacer predicciones sobre tendencias econ√≥micas, pron√≥sticos meteorol√≥gicos e incluso plataformas de contenido audiovisual. Las series de tiempo permiten las pistas para entender lo que ha sucedido en el pasado y lo que podr√≠a suceder en el futuro. La aplicaci√≥n de series de tiempo, permite ver patrones, tendencias y comportamientos que no son tan visibles a simple vista.

Imagina que somos inversionistas tratando de decidir en qu√© acciones invertir. Conociendo c√≥mo analizar series de tiempo, podemos estudiar el comportamiento hist√≥rico de esas acciones y hacer predicciones informadas sobre su desempe√±o futuro. O tal vez como cient√≠ficos que quieren entender el cambio clim√°tico. Al analizar series de tiempo de temperaturas, niveles de CO2 y otros datos, podemos identificar patrones y evaluar el impacto de las acciones humanas en el medio ambiente.

Comprender sobre series de tiempo es obtener una llave para desbloquear el potencial de los datos en movimiento. Permite tomar decisiones m√°s inteligentes en √°reas que van desde las finanzas hasta la salud p√∫blica y la planificaci√≥n urbana. Es una gran habilidad que nos convierte en mejores cient√≠ficos de datos.

As√≠, revisemos que el an√°lisis de un fen√≥meno social, financiero, econ√≥mico, entre otros genera inquietudes sobre la medida o variable de inter√©s en el fen√≥meno:

- ¬øEst√° en crecimiento? 
- ¬øEst√° cayendo?
- ¬øEst√° repitiendo un fen√≥meno estacional?
- ¬øSe est√° comportando aleatoriamente?
- ¬øEst√° retornando a un valor estable que podemos pronosticar?
- ¬øC√≥mo la din√°mica del fen√≥meno se relaciona y reacciona a las din√°micas en otro fen√≥meno? 

Las respuestas a estas preguntas, hacen que consideremos nuestra data como parte de procesos din√°micos y nos lleva a utilizar t√©cnicas como las que presentamos. (Box-Steffensmeier et al., 2014, 1)

# 2. LECTURA DE DATOS

Bajo este enfoque estimaremos el precio del aguacate usando sus propios precios pasados en lugar de usar la variable precio del d√≥lar.

Primero leeemos el dataset de aguacate y hacemos ediciones para asegurar que las fechas se encuentren en formato ISO (yyyy-mm-dd)

```{r}
# Cargue y limpieza de datos.
# üñáÔ∏è Se genera el vector de meses para usarlo m√°s abajo con la funci√≥n match.
meses <- c("Enero", "Febrero", "Marzo", "Abril", "Mayo",
           "Junio", "Julio", "Agosto", "Septiembre", "Octubre",
           "Noviembre", "Diciembre")

hass <- read.xlsx("datasets/Hass_Precios_Historicos.xlsx") %>% clean_names()
hass_dia <- hass %>%
  mutate(dia = sapply(strsplit(fecha, " "), "[", 3),
         mes = sapply(strsplit(fecha, " "), "[", 2),
         anio = sapply(strsplit(fecha, " "), "[", 5)) %>% 
  mutate(anio = as.double(anio)) %>% 
  # üñá Se utiliza la funci√≥n match con el vector meses.
  mutate(mes = match(mes,meses)) %>%
  mutate(fecha = ymd(paste(anio, mes, dia, sep = "-")))

dolar <- read.csv2("datasets/Tasa_de_Cambio_Representativa_del__Mercado_-Historico.csv") %>% 
  clean_names()
```

Leemos el dataset de dolar, asegurando tambi√©n fechas en formato ISO, y agregando por mes.

```{r}
# Vamos a sacar un valor promedio de cada variable por mes
# Para el dolar vamos a tomar el campo vigenciadesde
dolar <- dolar %>% 
  mutate(fecha =  format(as.Date(vigenciadesde, format = "%d/%m/%y"),
                         format = "%Y-%m-%d")) %>% 
  mutate(mes = month(fecha), anio = year(fecha)) %>% 
  mutate(valor = as.double(str_replace(valor,",",""))) %>%
  mutate(fecha= ymd(paste(anio, mes, 1, sep = "-"))) %>% 
  group_by(fecha) %>%
  summarise(precio_dolar = mean(valor), .groups = "drop")
print(dolar)
```

Construimos el dataset de aguacate agregado por mes

```{r}
hass_mes <- hass_dia %>% 
  mutate(fecha = ymd(paste0(anio,"-",mes,"-","01"))) %>%
  group_by(fecha) %>%
  summarise(precio_aguacate_kg = mean(precio_kg), .groups = "drop") 
```

Unimos los datasets agregados por mes

```{r}
hass_dolar <- dolar %>% 
  right_join(hass_mes, by = c("fecha"))
```

# 3. ALGUNOS CONCEPTOS GENERALES

[Variable aleatoria:]{style="color: red;"} Una variable aleatoria X es una funci√≥n de \[f : \Omega \rightarrow \mathbb{R}\] con $\Omega$ el conjunto de resultados posibles de un experimento aleatorio. En otras palabras un mapeo desde el espacio de descripciones muestrales a los n√∫meros reales con el fin de asociar a los resultados del experimento un valor num√©rico con el cual habilitar operaciones matem√°ticas. 

[Proceso estoc√°stico:]{style="color: red;"} Sea $\mathcal{T}$ un conjunto arbitrario. Un proceso estoc√°stico es una familia $\{X(t), \text{ t} \in \tau \}$, tal que, para cada $t \in \mathcal{T}, X(t)$ es una variable aleatoria.  Es decir, es una secuencia de variables aleatorias ordenadas por un √≠ndice, t√≠picamente tomado del conjunto de los n√∫meros enteros, o del conjunto de los n√∫meros reales.

[Proceso estoc√°stico en tiempo discreto:]{style="color: red;"} Es un proceso estoc√°stico con un √≠ndice temporal discreto. Significando que $\tau \in \mathbb{Z}$


[Especificaci√≥n de un proceso estoc√°stico:]{style="color: red;"} Sea $t_1, t_2, t_3,...,t_n$ elementos cualquiera de $\mathcal{T}$ y consideremos

$$\begin{equation} \label{eq:ec1} F(x_1, x_2, x_3, ..., x_n; t_1, t_2, t_3, ..., t_n)=P\{X(t_1)<=x_1, ..., X(t_n)<=x_n\}\end{equation}$$
Entonces un proceso estoc√°stico $\{X(t),t‚àà\mathcal{T}\}$ estar√° especificado si conocemos las distribuciones finito-dimensionales en ($\ref{eq:ec1}$), para todo n>=1. Esto significa que para $n=1$ conocemos las distribuciones uni-dimensionales de la variable aleatoria $X(t_1), t_1 \in \mathcal{T}$, para n=2 conocemos las distribuciones bi-dimensionales de las variables aleatorias $(X(t_1),X(t_2)), t_1, t_2 \in \mathcal{T}$ y as√≠ sucesivamente. Las funciones de distribuci√≥n ($\ref{eq:ec1}$) deben satisfacer las dos condiciones siguientes:

- (i) (Condici√≥n de simetr√≠a): Para cualquier permutaci√≥n $j_1, j_2,...,j_n$ de √≠ndices $1,2,3,...,n$, tenemos:

$$\begin{equation} \label{eq:ec2}F(x_{j_1},x_{j_2}, ..., x_{j_n}; t_{j_1},t_{j_2}, ..., t_{j_n}) = F(x_1, x_2,..., x_n; t_1, t_2, ..., t_n)\end{equation}$$

- (ii) (Condici√≥n de compatibilidad): Para $m<n$, 

$$\begin{equation} \label{eq:ec3}F(x_1,..., x_m, \infty, ...,\infty ; t_1,...,t_m,...,t_n) = F(x_1,..., x_m; t_1,...,t_m)\end{equation}$$
El lado izquierdo de ($\ref{eq:ec2}$) debe ser entendido como:

$\lim_{x_{m+1}, ..., x_n \to \infty}F(x_1,...,x_m,...,x_n;t_1,...,t_n)$

Se puede demostrar que cualquier conjunto de funciones de distribuci√≥n de la forma ($\ref{eq:ec1}$), satisfaciendo las condiciones ($\ref{eq:ec2}$) y ($\ref{eq:ec3}$) define un proceso estoc√°stico X_{t} sobre $\mathcal{T}$. Este teorema es conocido como **teorema de extensi√≥n de Kolmogorov**.

[Definici√≥n de un proceso estoc√°stico estrictamente estacionario:]{style="color: red;"} Un proceso estoc√°stico $\{X(t), \text{ t} \in \mathcal{T} \}$ se dice estrictamente estacionario si todas las distribuciones finito dimensionales tal como se definen en la ecuaci√≥n ($\ref{eq:ec1}$) permanecen siendo las mismas sobre traslaciones en el tiempo, es decir:

$$F(x_1, x_2, x_3, ..., x_n; t_1+\tau, t_2+\tau, t_3+\tau, ..., t_n+\tau)=F(x_1, x_2, x_3, ..., x_n; t_1, t_2, t_3, ..., t_n)$$
Algunas consecuencias de lo anterior son:

- $E\{X(t)\} = \mu(t) = \mu$
- $Var\{X(t)\} = \sigma¬≤(t) = \sigma¬≤$
- $\gamma(t_1, t_2) = \gamma(t_1-t_2, 0) = Cov\{X(t_1-t_2), X(0)\}$ o equivalentemente $\gamma{\tau} = Cov\{X(t), X(t+\tau)\} = Cov\{X(0), X(\tau)\}$ para t, $\tau \in \mathcal{T}$

[Definici√≥n de un proceso estoc√°stico d√©bilmente estacionario:]{style="color: red;"} Un proceso estoc√°stico $\{X(t), \text{ t} \in \mathcal{T} \}$ se dice d√©bilmente estacionario (o estacionario en segundo orden) si y s√≥lo si:

- (i) $E\{X(t)\}=\mu(t)=\mu$, constante, para todo $t \in \mathcal{T}$
- (ii) $E\{X^2(t)\}<\infty$, para todo $t \in \mathcal{T}$
- (iii) $\gamma(t_1,t_2)=Cov\{X(t_1),X(t_2)\}$ es una funci√≥n s√≥lo de |t_1-t_2|

A partir de ahora en esta gu√≠a estaremos interesados s√≥lo en procesos de este tipo, los cuales denominaremos simplemente como procesos estacionarios. 

[Definici√≥n de proceso estoc√°stico gaussiano:]{style="color: red;"} Un proceso estoc√°stico $\{X(t), \text{ t} \in \mathcal{T} \}$ se dice Gaussiano, si para cualquier conjunto $t_1,t_2,..., t_n$ de $\mathcal{T}$, las variables aleatorias $X(t_1), X(t_2),..,X(t_n)$ tienen una distribuci√≥n normal n-variada. 

Como un proceso normal n-variado con varianzas finitas queda determinado por sus medias y su estructura de covarianzas, si el es estacionario de segundo orden entonces el ser√° estrictamente estacionario.

En lo que sigue, usaremos la siguiente notaci√≥n: si el par√°metro $t$ fuera discreto, esto es $t \in \mathbb{Z} = \{0, \pm1, \pm2, ...\}$, el proceso ser√° descrito como $\{X_t, t \in  \mathbb{Z}\}$, y en caso de que $t$ sea continuo, esto es $t \in \mathbb{R}$, el proceso ser√° denotado por $\{X(t), t \in  \mathbb{R}\}$. La misma convenci√≥n aplicamos para los momentos. Por ejemplo: una funci√≥n de autocovarianza para proceso discreto ser√° denotado como $\gamma_\tau$ y una funci√≥n de autocovarianza para proceso continuo como $\gamma(\tau)$.

[Definici√≥n de un proceso ruido blanco discreto:]{style="color: red;"} Decimos que $\{\varepsilon_t, t \in  \mathbb{Z}\}$ es un ruido blanco discreto si las variables aleatorias $\varepsilon_t$ son no correlacionadas, esto es, $Cov\{\varepsilon_{t},\varepsilon_{s}\}=0, t \ne s$

Un proceso de este tipo ser√° adem√°s estacionario si $E(\varepsilon_t)=\mu$ y $Var(\varepsilon_t)={\sigma^2}_{\varepsilon}$ para todo $t$. Se sigue entonces que la funci√≥n de autocovarianza de $\varepsilon$ esta dada por:

$\gamma_t = Cov(\varepsilon_n, \varepsilon_{n+\tau})={\sigma^2}_{\varepsilon}$ si $\tau=0$, de lo contrario ser√° 0. 


**Reflexiones en lenguaje informal**

- Un proceso estoc√°stico quedar√° suficientemente definido si definimos la distribuci√≥n de probabilidad n-dimensional conjunta para cada $n>=1$, es decir, tomando $n$ variables del proceso estoc√°stico a la vez, ser√≠a necesario especificar la relaci√≥n estad√≠stica de esas n variables a la vez para cada eleci√≥n de n. En nuestro contexto implica especificar c√≥mo se espera que el precio de hoy se relacione con el precio de ayer, o c√≥mo se espera que el precio de hoy se relacione con el de ayer y adem√°s con el de antier. Y as√≠ sucesivamente para cualquier grupo finito de $n$ precios que decida tomar a consideraci√≥n.

- Un proceso estoc√°stico estacionario se puede interpretar como aquel cuyas propiedades estad√≠sticas permanencen constantes a lo largo del tiempo y s√≥lo cambiaran en funci√≥n del grupo de $n$ variables que decida obervar conjuntamente, pero no de la posici√≥n de esas n variables en el tiempo. Por ejemplo, suponiendo que n=1, esto implicar√≠a que las propiedades estad√≠sticas de cada variable tomada individualmente no se ven alteradas conforme el tiempo avance, en particular cada variable ser√° estable en media y en varianza. Lo que significa que puedo reunir las observaciones individuales en cada momento del tiempo para estimar la media constante y la varianza de toda la secuencia, en la media en que en cada punto temporal estoy asumiendo que la observaci√≥n que registr√© proviene de poblaciones estad√≠sticas con la misma media y la misma varianza. Esto siempre se cumplir√°? Por supuesto que no, es una suposici√≥n que imponemos a los datos para poder hacer inferencia con un √∫nico set de valores hist√≥ricos observados. 

- Como la condici√≥n de estacionariedad estricta dificilmente se puede comprobar en la pr√°ctica, como alternativa relajamos un poco las condiciones para exigencia de estacionariedad, por ejemplo podemos exigir estacionariedad d√©bil consistente en pedir s√≥lo homogeneidad en media, en varianza y en covarianza de segundo orden a la serie de tiempo bajo estudio. 

- Si la serie observada no parece cumplir estacionariedad podemos tomar la serie de variaciones porcentuales o podemos tomar la serie diferenciada $d$ veces. Por diferenciada entendemos tomar la serie de incrementos de un tiempo a otro, esto es: $X(t)-X(t-1)$. Estas transformaciones suelen reestablecer la estacionariedad. 

# 4. DEFINICI√ìN DE MODELOS ARIMA

## 4.1 Ecuaci√≥n general del modelo

En este curso vamos a discutir √∫nicamente los modelos de la familia **ARMA(p,q)**(Autoregressive Movil Average) y **ARIMA(p,d,q)**(Autoregressive Integrated Movil Average), los cuales tiene la siguiente estructura:

- ARMA(p,q):

$X_t - \mu = \phi_1(X_{t‚àí1} - \mu) + \phi_2(X_{t‚àí2} - \mu)+... + \phi_p(X_{t‚àíp}-\mu) + \varepsilon_t - \theta_1\varepsilon_{t‚àí1} - \theta_2\varepsilon_{t‚àí2}-...-\theta_q\varepsilon_{t‚àíq}$

Donde $\mu$ ser√≠a la media general del proceso es decir $\mu = E(X_t)$ y $\varepsilon_{t} \sim RB(0,{\sigma}_{\varepsilon})$.

Equivalentemente se podr√≠a proponer tambi√©n la siguiente especificaci√≥n:

$X_t = c_0 +\phi_1X_{t‚àí1} + \phi_2X_{t‚àí2} +... + \phi_pX_{t‚àíp} + \varepsilon_t - \theta_1\varepsilon_{t‚àí1} - \theta_2\varepsilon_{t‚àí2} - ... - \theta_q\varepsilon_{t‚àíq}$

Esta segunda formulaci√≥n contendr√≠a intercepto $c_0$, pero tomando valor esperado a ambos lados es claro que:

$$E(X_t) = c_0 + \phi_1E(X_{t‚àí1}) +  \phi_2E(X_{t‚àí2}) + ...+\phi_pE(X_{t‚àíp})$$

Luego:

$$\mu = c_0 + \phi_1\mu +  \phi_2\mu + ...+\phi_p\mu\\\mu=\frac{c_0}{1-\phi_1-\phi_2-...-\phi_p}$$

Es decir que si la ecuaci√≥n se escribe centrada respecto a $\mu$ no requiere intercepto pero en caso contrario requiere un intercepto $c_0$ cumpliendo la ecuaci√≥n anterior. 

- ARIMA(p,d,q):

El proceso ARIMA no es m√°s que una generalizaci√≥n para el proceso ARMA la cual incluye un t√©rmino de diferenciaci√≥n:

$\Delta^d X_t \sim ARMA(p,q)$

Puede notar que de forma comprimida hemos escrito $\Delta X_t$ para hacer referencia a la operaci√≥n $X_t-X_{t-1}$. Si sucede que despu√©s de aplicar una diferenciaci√≥n a la serie procedemos a repetir de nuevo el proceso de diferenciaci√≥n, escribimos en forma comprimida $\Delta^2 X_t$, para indicar que se aplica una nueva diferenciaci√≥n sobre la serie ya diferenciada. Al n√∫mero $d$ de veces que se haya diferenciado la serie lo llamamos el grado de integraci√≥n $d$ de la serie.

Aunque existen otras especificaciones mejoradas de modelos basados en la idea de los modelos ARMA y ARIMA, es importante en un curso introductorio a series de tiempo concentrarnos en entender primero los modelos cl√°sicos antes de pasar a propuestas m√°s sofisticadas como los modelos ARMA-GARCH o ARIMA-GARCH.

En el modelo ARMA anterior es importante aclarar lo siguiente:

- Tenemos una variable ${X_t}$ la cual asumimos que se puede modelar usando su historia. En la historia de dicha variable podemos considerar tres componentes: 

1. Los valores propiamente dichos de la variable en un momento pasado: $X_{t‚àí1}, ... X_{t‚àíp}$

2. Los choques aleatorios que ocurrieron en momentos pasados: $\varepsilon_{t‚àí1} + ... + \varepsilon_{t‚àíq}$

3. El choque aleatorio para el momento actual $\varepsilon_t$

Los dos √∫ltimos elementos pretenden capturar las variaciones en la variable que no se pueden explicar por la estructura de dependencia temporal de la variable misma sino por causas externas que producen variaciones no sistem√°ticas en los valores de la variable.

- Para los errores asumimos como es usual la normalidad $\varepsilon\sim N(0,{\sigma_{\varepsilon}}^2)$, la independencia de los errores y la homocedasticidad

- El modelo estar√° bien especificado si una vez ajustado a los datos se nos confirman los supuestos anteriores sobre los residuales observados.

- El par√°metro $p$ se debe determinar en forma conveniente e indica la cantidad de rezagos de la variable a incluir como variables predictoras.

- El par√°metro $q$ se debe determinar en forma conveniente e indica la cantidad de errores anteriores a considerar como predictores del valor actual.

- $\sigma_\varepsilon$ ser√° otro par√°metro a estimar y hablar√° de la varianza a largo plazo de todo mi proceso. 

- $\mu$ ser√° otro par√°metro a estimar en caso de que especifique el proceso no centrado. 

- La especificaci√≥n funcional del modelo ARMA que acabamos de dar no coincide con la que R usa, en R los coeficientes $\theta_1,\theta_2,\theta_q$ se consideran con signo "+" en la ecuaci√≥n anterior, por consiguiente las estimaciones que R nos entrega para tales coeficientes siempre tendr√°n signo opuesto a nuestra convenci√≥n actual.

## 4.2 Supuestos importantes

Adicional al supuesto de normalidad de los errores, independencia y homocedasticidad, conviene mencionar dos supuestos adicionales que se usan frecuentemente dentro de la formulaci√≥n de modelos de series de tiempo.

El primero es el supuesto de estacionariedad, recapitulamos su definici√≥n informal:

**Estacionariedad**: Un proceso estoc√°stico se considera estacionario en sentido d√©bil si las propiedades estad√≠sticas de segundo orden del proceso no cambian a lo largo del tiempo. Como consecuencia de esto dichos procesos poseen las siguientes caracter√≠sticas:

- Media constante: La media de las observaciones es constante para todos los puntos en el tiempo.
- Varianza constante: La varianza de las observaciones es constante a lo largo del tiempo.
- Autocorrelaci√≥n constante: La correlaci√≥n entre observaciones qque distan entre s√≠ una determinada cantidad de tiempo, cambia con respecto a la distancia en tiempo pero no con respecto a los tiempos en si mismos. Es decir, es decir la covarianza entre $X(t_2), X(t_1)$ se espera igual a la de $X(t_{20}), X(t_{19})$. La covarianza entre $X(t_3),X(t_1)$ no es por regla general que la de $X(t_3),X(t_2)$ porque no corresponden a tiempos a igual distancia. 

En la literatura estad√≠stica se distinguen dos tipos de estacionariedad:

- Estacionariedad estricta: Todas las distribuciones conjuntas son id√©nticas.
- Estacionariedad d√©bil: La que ya se mencion√≥, donde las estad√≠sticas relevantes (media, varianza, autocorrelaciones) son constantes o estables, dependen de la distancia en tiempo entre las variables pero no de los tiempos espec√≠ficos considerados.

En el caso del modelo ARMA existir√°n ciertas condiciones que garantizar√°n su estacionariedad d√©bil. Para que ello se impondra≈Ñ ciertas condiciones sobre los coeficientes $\phi_i$ del componente autorregresivo del modelo. M√°s adelante daremos detalles. 

Por el contrario el modelo ARIMA ser√° siempre **NO** estacionario. Sin embargo, de acuerdo con la ecuaci√≥n que define al proceso ARIMA, √©ste puede pensarse como un tipo de proceso que requiere $d$ diferenciaciones para transformarse en un proceso ARMA. Como ya dijimos, por diferenciaci√≥n entendemos la operaci√≥n consistente en restar el valor actual $X_t$ de la serie con su valor un periodo antes $X_{t-1}$. Esta operaci√≥n se suele proponer en series de tiempo porque es una estrategia que suele ser √∫til para lograr estacionariedad. Por lo tanto, si queremos obtener un proceso ARIMA conveniente para pron√≥stico, ser√° suficiente con que garanticemos que su proceso ARMA asociado ya es estacionario despu√©s de las diferenciaciones aplicadas. 

Finalmente conviene mencionar un segundo supuesto que se usa en series de tiempo: el supuesto de ergodicidad, a continuaci√≥n una definici√≥n no formal:

**Ergodicidad:** La ergodicidad es un concepto m√°s profundo que se refiere a la condici√≥n de un proceso estoc√°stico en el que las propiedades del proceso pueden estimarse a partir de una sola realizaci√≥n (una sola trayectoria) del proceso. En otras palabras, si un proceso es erg√≥dico, una muestra finita de datos se comportar√° de manera similar a la poblaci√≥n completa de datos. Como consecuencia de esto se permite la extracci√≥n de informaci√≥n estad√≠stica significativa a partir de una sola trayectoria del proceso. La ergodicidad es especialmente relevante en el contexto de procesos estoc√°sticos no estacionarios, donde las propiedades estad√≠sticas pueden cambiar con el tiempo. Tenga en cuenta que estacionariedad no es sin√≥nimo de ergodicidad, podemos tener procesos estacionarios que sin embargo no son erg√≥dicos. 

Para entender mejor este punto, considera los siguientes ejemplos:

**Ejemplo 1**:

Un **proceso de Wiener** tambi√©n conocido como **movimiento Browniano** o **caminata aleatoria** que es no estacionario y no erg√≥dico (sus incrementos son estacionarios pero no lo es el proceso en s√≠ mismo).

Llamaremos movimiento browniano est√°ndar (o proceso de Weiner) a un proceso continuo $W=\{W(t), t>=0\}$ tal que:

- (a) W(0) = 0
- (b) Para cualesquiera instantes $0<=t_1<=t_2<=...<=t_k<=1$ las variables aleatorias $W(t_2) - W(t_1), W(t_3) - W(t_2),..., W(t_k) - W(t_{k-1})$,  son independiente. Es decir los saltos o incrementos que le ocurren al proceso desde un tiempo $t_{k-1}$ a otro posterior $t_k$ son independientes. 
- (c) Para cualesquiera $s,t$, y $\tau$ no negativos las variables aleatorias $W(t)-W(s)$ y $W(t+\tau)-W(s+\tau)$ tienen la misma distribuci√≥n. 
- (d) Para todo t>0, $W(t) \sim N(0.t)$
- (e) Las trayectorias de W(t) son continuas con probabilidad 1

Simulemos e ilustremos esta situaci√≥n. 

```{r}
if (!file.exists("brownian_motion.gif")){
  
# Funci√≥n para simular una trayectoria del movimiento browniano
simular_mov_browniano<- function(n_steps) {
  dt <- 1
  t <- seq(1, n_steps * dt, by = dt)
  x <- cumsum(rnorm(n_steps))
  data.frame(t, x)
}

# N√∫mero de trayectorias a simular
n_simulations <- 10

# N√∫mero de pasos en cada trayectoria
n_steps <- 1000

# Simulamos las trayectorias del movimiento browniano
simulaciones_browniano <- lapply(1:n_simulations, 
                                 function(i) simular_mov_browniano(n_steps))

# Conformamos un dataframe con los resultados
df_sim_browniano = data.frame(
  t=numeric(),
  x=numeric(),
  simulacion=numeric()
)
for (i in 1:length(simulaciones_browniano)){
  df_inc_browniano <- data.frame(
    t=simulaciones_browniano[[i]]$t,
    x=simulaciones_browniano[[i]]$x,
    simulacion=rep(i,length(simulaciones_browniano[[i]]$x))
    )
  df_sim_browniano <- rbind(df_sim_browniano, df_inc_browniano)
}

# Creamos un gr√°fico animado para Browniano
p_b <- ggplot() +
  geom_line(aes(x = t, y = x,
                color = factor(simulacion)),
            data = df_sim_browniano) +
  labs(title = "Simulaciones de Movimiento Browniano") +
  theme_minimal() +
  transition_states(simulacion, transition_length = 2, state_length = 1)

#Guardamos el gif animado
anim_save("brownian_motion.gif", animate(p_b), renderer = gifski_renderer())
}
```

<figure>
<img src="brownian_motion.gif"/>
<figcaption>Img 1: Proceso no estacionario y no erg√≥dico</figcaption>
</figure>

Observe que el proceso explota en varianza, es decir su varianza crece conforme el tiempo pasa (no estacionariedad), y observe que las trayectorias son tan diversas de una corrida a otra que la informaci√≥n depositada en una trayectoria poco nos puede informar de lo que ocurrir√° con las dem√°s o con el proceso en general. 

Ahora analicemos el segundo ejemplo:

**Ejemplo 2:**

Una se√±al sinosoidal con ruido blanco aditivo que es estacionaria pero no erg√≥dica (sus propiedades estad√≠sticas se preservan en el tiempo pero una √∫nica trayectoria no ayuda a estimar con precisi√≥n sus propiedades de fase, amplitud y frecuencia)

El proceso queda definido as√≠:

$W=\{W(t), t>=0\}$ tal que:

- (a) $W(t)=Asin(2\pi ft + \phi) + \varepsilon_{t}$ con $\varepsilon_{t} \sim N(0,1)$
- (b) $\varepsilon_{t_1}, \varepsilon_{t_2}$ iid para todo $t_1, t_2$

Simulemos e ilustremos la situaci√≥n:

```{r}
if (!file.exists("sinusoidal_motion.gif")){
# Funci√≥n para estimar una se√±al sinosoidal estacioaria:
simular_sinosoidal <- function(n_steps){
# Par√°metros del proceso
A <- 1          # Amplitud de la se√±al sinusoidal
f <- 0.1        # Frecuencia de la se√±al sinusoidal
phi <- pi/4     # Fase de la se√±al sinusoidal
ruido <- rnorm(n_steps) # Error aditivo

# Generar el proceso
t <- seq(1, n_steps, length.out = n_steps)  # Valores de tiempo
x <- A * sin(2 * pi * f * t + phi) + ruido
data.frame(t,x)
}

# Simulamos las trayectorias del movimiento sinosoidal
simulaciones_sinosoidal <- lapply(1:10,function(i) simular_sinosoidal(100))

#Creamos un dataframe con los resultados
df_sim_sinosoidal = data.frame(
  t=numeric(),
  x=numeric(),
  simulacion=numeric()
)
for (i in 1:length(simulaciones_sinosoidal)){
  df_inc_sinosoidal <- data.frame(
    t=simulaciones_sinosoidal[[i]]$t,
    x=simulaciones_sinosoidal[[i]]$x,
    simulacion=rep(i,length(simulaciones_sinosoidal[[i]]$x))
    )
  df_sim_sinosoidal <- rbind(df_sim_sinosoidal, df_inc_sinosoidal)
}

# Generamos el gr√°fico con la animaci√≥n
p_s <- ggplot() +
  geom_line(aes(x = t, y = x,
                color = factor(simulacion)),
            data = df_sim_sinosoidal) +
  labs(title = "Simulaciones de Movimiento Sinosoidal") +
  theme_minimal() +
  transition_states(simulacion, transition_length = 2, state_length = 1)

#Guardamos el gif animado
anim_save("sinusoidal_motion.gif", animate(p_s), renderer = gifski_renderer())
}
```

<figure>
<img src="sinusoidal_motion.gif"/>
<figcaption>Img 2: Proceso estacionario y no erg√≥dico</figcaption>
</figure>

Note que las trayectorias pueden considerarsen estables en media y varianza(estacionariedad), pero una trayectoria en particular no ofrece informaci√≥n suficiente para estimar con precisi√≥n los par√°metros de fase, amplitud y frecuencia del proceso como un todo. 

## 4.3 Estimaci√≥n de la funci√≥n de autocorrelacion(ACF)

Debido a que estaremos exigiendo estacionariedad d√©bil a las series de tiempo bajo estudio, tiene sentido preguntarnos por un estimador para la estructura de autocovarianza de una serie de tiempo dada. En general usamos la siguiente expresi√≥n para procesos estoc√°sticos en tiempo discreto:

$$\hat{\gamma_{j}}=c_j=\frac{1}{T}\sum_{t=1}^{T-j}[(X_t-\bar{X})(X_{t+j}-\bar{X})], j=0,1,...,T-1$$
Siendo $\bar{X} = \frac{1}{T}\sum_{t=1}^{T} X_t$ la media muestral, como las autocovarianzas verdaderas son funciones pares no tememos decir que $c_{-j}=c_j$. Y para estimar las autocorrelaciones $\rho_j$ usamos:

$$\hat{\rho_j}=r_j=\frac{c_j}{c_0}$$

Donde de nuevo podemos decir que: $r_{-j}=r_j$

Se puede apreciar que $r_j$ es una funci√≥n que depende de $j$ (el rezago considerado), es decir que para cada posible valor pasado de la serie de tiempo alejado $j$ periodos de tiempo del valor actual, tendremos un valor de $r_j$. Al conjunto de todos los valores posibles de $r_j$ se le conoce como la funci√≥n de autocorrelaci√≥n estimada de la serie o ACF. √âsta nos habla de la manera como cada valor de mi variable a predecir se correlaciona con los valores anteriores, es decir, con sus propios rezagos.

A continuaci√≥n un ejemplo de c√≥mo estimar la ACF sobre los datos de aguacate (con periodicidad diaria) usando la funci√≥n acf que ya viene implementada en R.

Esta es la serie de precios diarios:

```{r}
hass_dia %>% 
  mutate(n_registro = as.numeric(rownames(.))) %>% 
  ggplot(aes(x= fecha, y = precio_kg, group = 1))+
  geom_line()
```

Esta es la funci√≥n que me estima la autocorrelaci√≥n:

```{r}
acf_dia <- acf(hass_dia$precio_kg)
data.frame(
  lag = acf_dia$lag,
  acf = acf_dia$acf) %>% head(10) %>% ftable()

# Ac√° construimos las bandas de confianza que informar√°n si la autocorrelaci√≥n
# es significativa.

T<-length(hass_dia$precio_kg)
cuantil <- qnorm(0.975, 0,1)
bc <- cuantil/sqrt(T)

paste0("Declare la autocorrelaci√≥n como significativa si |r|>", round(bc,5))
```

El estudiante curioso puede verificar que esto es equivalente a usar nuestra definici√≥n anterior asi:

```{r}

# ACF usando la definici√≥n
mi_acf <- function(y, k){
  n <- length(y)
  f1 <- y[(1+k):n]-mean(y)
  f2 <- y[1:(n-k)] - mean(y)
  f3 <- sum((y-mean(y))^2)
  r_mod <- sum(f1*f2)/f3
  return(r_mod)
}
paste0("Esta es la autocorrelaci√≥n a 1 lag calculada con mi f√≥rmula: ",
       mi_acf(hass_dia$precio_kg,1), " y esta es la calculada con r: ",
       acf_dia$acf[2])
```
## 4.4 Estimaci√≥n de la funci√≥n de autocorrelacion parcial (PACF)

Box, Jenkins y Reisel(1994) proponen la utilizaci√≥n de otro instrumento para facilitar la correcta identificaci√≥n de un modelo ARMA para una serie de tiempo observada que se basa en la denominada funci√≥n de autocorrelaci√≥n parcial. 

Esta funci√≥n nos habla de la contribuci√≥n que tiene el $k-√©simo$ rezago al valor actual de nuestra variable de inter√©s. Dicho de otra manera, c√≥mo se espera que se correlacione un valor k periodos antes con el valor actual, pero tomando previo control de los k-1 rezagos intermedios que tambi√©n pueden estar afectando el valor actual. Para ello tiene sentido proponer el siguiente modelo de regresi√≥n:

$X_t = \phi_{k1}X_{t-1} + \phi_{k2}X_{t-2} + .... +\phi_{kk}X_{t-k} + \varepsilon_{t}$

Donde el coeficiente $\phi_{kj}$ ser√≠a entonces el $j-√©simo$ coeficiente de un modelo autorregresivo de orden k, mientras que el √∫ltimo coeficiente $\phi_{kk}$ estar√≠a informando espec√≠ficamente del efecto que el $k-√©simo$ rezago tiene sobre el valor actual, una vez hemos descontado expl√≠citamente los efectos de los restantes $k-1$ rezagos intermedios.

Si contaramos con un procedimiento adecuado para estimar $\phi_{kk}$, entonces contar√≠amos con un estimado de la correlaci√≥n parcial al rezago $k$. Procedamos:

**Para k=1**: Estimamos el modelo:

$X_t = \phi_{11}X_{t-1} + \varepsilon_{t}$

Podemos multiplicar la ecuaci√≥n anterior por $X_{t-1}$ y tenemos:

$X_tX_{t-1} = \phi_{11}X_{t-1}X_{t-1} + X_{t-1}\varepsilon_{t}$

Tomando valores esperados en ambos lados de la ecuaci√≥n tenemos:

$E(X_tX_{t-1}) = \phi_{11}E(X_{t-1}X_{t-1}) + E(X_{t-1}\varepsilon_{t})$

Ac√° podemos usar una serie de hechos.

- (i) Podemos asumir sin p√©rdida de generalidad que $E(X_t) = 0$, 
- (ii) Por definici√≥n $E(X_tX_{t-1})=Cov(X_t, X_{t-1})=\gamma_1$ cuando $\mu=0$.
- (iii) $E(X_{t-1}\varepsilon_{t})=0$ pues $\varepsilon_{t} \sim RB(0,{\sigma}_{\varepsilon})$ y se produce independiente del valor previo $X_{t-1}$ as√≠ que est√° incorrelado con valores pasados de $X(t)$. 

Por lo tanto: 

$\gamma_1= \phi_{11}\gamma_0$

Ahora dividiendo entre la varianza $\gamma_0$ del proceso tenemos:

$\rho_1= \phi_{11}$

Remplazando $\rho_1$ por su estimado $r_1$ discutido en la secci√≥n previa, tenemos que:

$\hat{\phi}_{11}=r_1$

**Para k=2**: Estimamos el modelo:

$X_t = \phi_{21}X_{t-1} + \phi_{22}X_{t-2} + \varepsilon_{t}$

Podemos multiplicar la ecuaci√≥n anterior por $X_{t-1}$, y luego hacer lo mismo pero multiplicando por $X_{t-2}$ obteniendo el siguiente sistema de ecuaciones:

- $\rho_1 = \phi_{21} + \phi_{22}\rho_1$ cuando se multiplica por $X_{t-1}$
- $\rho_2 = \phi_{21}\rho_1 + \phi_{22}$ cuando se multiplica por $X_{t-2}$

Reemplazando los $\rho_j$ por sus estimados tenemos:

- $r_1 = \phi_{21} + \phi_{22}r_1$ cuando se multiplica por $X_{t-1}$
- $r_2 = \phi_{21}r_1 + \phi_{22}$ cuando se multiplica por $X_{t-2}$

El cual puede resolverse para $\phi_{22}$ como:

$$\hat{\phi}_{22} = \frac{
\begin{vmatrix}
  1 & r_1 \\
  r_1 & r_2
\end{vmatrix}}
{\begin{vmatrix}
  1 & r_1 \\
  r_1 & 1
\end{vmatrix}}
=\frac{r_2-r_1^2}{1-r_1^2}$$

**Para k=3**: Estimamos el modelo:

$X_t = \phi_{31}X_{t-1} + \phi_{32}X_{t-2} + \phi_{33}X_{t-3} + \varepsilon_{t}$

Podemos multiplicar la ecuaci√≥n anterior por $X_{t-1}$,$X_{t-2}$, y $X_{t-3}$ para obtener el siguiente sistema de ecuaciones:

- $\rho_1 = \phi_{31} + \phi_{32}\rho_1 + \phi_{33}\rho_2$ cuando se multiplica por $X_{t-1}$
- $\rho_2 = \phi_{31}\rho_1 + \phi_{32} + \phi_{33}\rho_1$ cuando se multiplica por $X_{t-2}$
- $\rho_3 = \phi_{31}\rho_2 + \phi_{32}\rho_1 + \phi_{33}$ cuando se multiplica por $X_{t-3}$

El cual, despu√©s de los reemplazos de las estimaciones de los $\rho_i$ por sus respectivos $r_i$ puede resolverse para $\phi_{33}$ como:

$$\hat{\phi}_{33} = \frac{
\begin{vmatrix}
  1 & r_1 & r_1\\
  r_1 & 1 & r_2\\
  r_2 & r_1 & r_3
\end{vmatrix}}
{\begin{vmatrix}
  1 & r_1 & r_2\\
  r_1 & 1 & r_1\\
  r_2 & r_1 & 1\\
\end{vmatrix}}$$

Y en general, podemos deducir que para estimar $\phi_{kk}$ construimos un modelo de regresi√≥n con los k rezagos, y lo multiplicamos convenientemente por cada uno de ellos para obtener que:

$$\hat{\phi}_{kk} = \frac{|P^*_k|}{|P_k|}$$

Donde la matriz $|P_k|$ es una matriz de autocorrelaciones muestrales $r_i$, mientras que la matriz $|P^*_k|$ es la misma matriz $|P_k|$ con su √∫ltima columna reemplazada por el vector de autocorrelaciones muestrales $r_i$. A las cantidades $\hat{\phi}_{kk}$ tabuladas en funci√≥n del valor $k$ elegido, se le conoce como funci√≥n de autocorrelaci√≥n parcial estimada. Y las ecuaciones previamente construidas como las ecuaciones de Yule-Walker. 

Veamos como extraer tales estimados usando sobre la serie de precios de aguacate con periodicidad diaria R

```{r}
pacf_dia <- pacf(hass_dia$precio_kg)
data.frame(
  lag = pacf_dia$lag,
  pacf = pacf_dia$acf) %>% head(10) %>% ftable()

# Realicemos una comprobaci√≥n usando funciones

matriz_autocorrelacion <- function(x, k){
  acf <- acf(x, plot=FALSE)$acf[2:(k+1)]
  if (k==1){
    return(acf[1])
  }else{
  acf_mod <- c(c(1),acf[1:(length(acf)-1)])
  matriz_acf <- matrix(data = rep(0, k*k),
                       nrow = k,
                       ncol = k,
                       byrow = TRUE)
  for (i in (1:k)){
    for (j in (1:k)){
      d <- abs(i-j)
      r <- acf_mod[d+1]
      matriz_acf[i,j] <- r 
    }
  }
  matriz_num <- matriz_acf
  for (p in 1:k){
    matriz_num[p,k] <- acf[p]
  }
  return(det(matriz_num)/det(matriz_acf))
  }
}

phi_33 <-matriz_autocorrelacion(hass_dia$precio_kg, 3)

paste0("Esta es la autocorrelaci√≥n parcial a 1 lag calculada con mi f√≥rmula: ",
       phi_33, " y esta es la calculada con r: ",pacf_dia$acf[3])
```

[Nota importante:]{style="color: red;"} 

En los apartados anteriores hemos calculado las funciones acf y pacf sobre la serie de precios diarios del aguacate. Debido a que acf decae muy lentamente, es indicativo de la presencia de una ra√≠z unitaria en el polinomio autorregresivo que puede estar generando problemas de no estacionariedad. En estos casos ser√° recomendable diferenciar la serie antes de proceder al modelamiento. En particular se sugiere primero intentar con una transformaci√≥n del tipo:

$$ \Delta precio_t = precio_t - precio_{t-1}$$
Y en lugar de modelar la serie $precios_t$, modelamos la serie $\Delta precio_t$. Otra elecci√≥n ampliamente usada en la pr√°ctica es crear una serie como esta:

$$r_t = \Delta precio_t / precio_{t-1} = \frac{precio_t - precio_{t-1}}{precio_{t-1}}$$
La cual representa una serie de precios diferenciada pero luego expresada como variaciones porcentuales (retornos) del precio actual respecto al del d√≠a anterior. 

Cualquiera de las dos transformaciones suelen remover la integraci√≥n y permiten tratar con una serie inicial estacionaria. Si pese a esta primera transformaci√≥n sigue sin lograrse estacionariedad ser√° necesaria una nueva diferenciaci√≥n de la serie obtenida.

Vamos a estacionarizar la serie contemplando los cambios porcentuales

```{r}

df_variaciones <- data.frame(
  variaciones  = (hass_dia$precio_kg - lag(hass_dia$precio_kg)) / lag(hass_dia$precio_kg))

df_variaciones <- df_variaciones %>% 
  filter(!is.na(variaciones))

ggplot(data= df_variaciones, aes(x= seq(1,length(variaciones)),
                                 y = variaciones, group = 1))+
  geom_line()

```
Este gr√°fico ya parece m√°s estable en media y por lo tanto es un mejor candidato para ser estimado usando modelos ARMA. 

[Prueba de ra√≠ces unitarias:üÜï üîç]{style="color: red;"} 

Una prueba de ra√≠ces (tambi√©n conocida como prueba de ra√≠z unitaria) es una prueba estad√≠stica que se utiliza en el an√°lisis de series de tiempo para determinar si una variable tiene ra√≠ces unitarias. Las ra√≠ces unitarias indican que una serie de tiempo no es estacionaria y, por lo tanto, no es adecuada para el modelado de series de tiempo.

Una variable con ra√≠ces unitarias es aquella cuyo proceso estoc√°stico subyacente no ha convergido a una media o varianza constante a lo largo del tiempo. En otras palabras, la serie de tiempo muestra patrones de comportamiento no estacionario.

Las pruebas de ra√≠ces unitarias comunes incluyen:

- Prueba de Dickey-Fuller Aumentada (ADF): Esta es una de las pruebas m√°s utilizadas. Eval√∫a si existe una ra√≠z unitaria en una serie de tiempo univariada. La hip√≥tesis nula en esta prueba es que la serie de tiempo tiene una ra√≠z unitaria, lo que significa que no es estacionaria.

- Prueba de Dickey-Fuller Aumentada con lags (ADF-GLS): Similar a la ADF, pero con correcci√≥n de errores lagged.

- Prueba de Phillips-Perron: Otra prueba com√∫n para la ra√≠z unitaria, que es similar a la ADF.

- Prueba de Kwiatkowski-Phillips-Schmidt-Shin (KPSS): A diferencia de las pruebas anteriores, la KPSS se utiliza para determinar si una serie de tiempo es estacionaria en tendencia en lugar de estacionaria alrededor de una media constante.

La elecci√≥n de la prueba adecuada depende de la naturaleza de los datos y el objetivo del an√°lisis. Si una variable tiene ra√≠ces unitarias, generalmente es necesario realizar transformaciones, como diferenciaci√≥n, para hacer que la serie de tiempo sea estacionaria antes de aplicar t√©cnicas de modelado de series de tiempo, como modelos ARIMA (Media M√≥vil Autorregresiva Integrada) o modelos GARCH (Generalized Autoregressive Conditional Heteroskedasticity).

En resumen, una prueba de ra√≠ces es una herramienta importante en el an√°lisis de series de tiempo para determinar la estacionariedad de una variable y la necesidad de realizar transformaciones antes de modelar los datos.

[Prueba de Dickey Fuller:üÜï üîç]{style="color: red;"} 

La prueba Dickey-Fuller se usa para detectar estad√≠sticamente la presencia de conducta tendencial estoc√°stica en las series temporales de las variables mediante un contraste de hip√≥tesis.

Interpretaci√≥n del rechazo, o no rechazo de la Hip√≥tesis Nula de la prueba.
El contraste ser√° el siguiente:

- $H_0=0$ ‚Üí Existe ra√≠z unitaria, xt $\rightarrow$ La serie no es estacionaria.
- $H_0\ne0$ ‚Üí No existe ra√≠z unitaria,xt  $\rightarrow$ La serie es estacionaria.

Si:
- $œÑ‚àícalculado$ en valor absoluto > $œÑ‚àícr√≠tico$, en valor absoluto: Se rechaza la $H_0$
- $œÑ‚àícalculado$ en valor absoluto < $œÑ‚àícr√≠tico$, en valor absoluto: Se acepta la $H_0$

*Vamos a aplicar la prueba para los precios del aguacate mensuales y sin ninguna transformaci√≥n.*

```{r}
library(urca)
# Vamos a guardar el resultado en un elemento llamado df que hace referencia a la prueba
# Dickey Fuller
df <- ur.df(hass_mes$precio_aguacate_kg, type = "none", lags = 0)
summary(df)
```

De acuerdo con lo anterior, la serie **no es estacionaria**, porque el valor p es 0.7898, por lo tanto no tenemos evidencia suficiente para rechazar la hipotesis nula, es decir que no hay evidencia para confiar en la estacionariedad.

*Ahora vamos a aplicar la prueba para las* **variaciones** *de precios del aguacate*

```{r}
# Vamos a guardar el resultado de nuevo en df que hace referencia a la prueba
# Dickey Fuller
df <- ur.df(df_variaciones$variaciones, type = "none", lags = 0)
summary(df)
```

üèÅ üëç Ahora si encontramos que la variable se ha transformado en una variable estacionaria, puesto que el p valor es menor a 0.05 por lo tanto, tenemos evidencia estad√≠stica suficiente para rechazar la hip√≥tesis nula, reiterando entonces que NO EXISTE RA√çZ UNITARIA y que la variable es estacionaria.

‚ö†Ô∏è** Important:e**

Ac√° estamos partiendo del supuesto de que la serie tiene tendencia estoc√°stica, pero si este supuesto falla y la serie en realidad tiene tendencia deterministica, las conclusiones de la prueba DICKEY FULLER pueden ser enga√±osas.

Porque tendr√≠amos que entrar a analizar:

- ¬øCu√°l es la diferencia entre tendencia estoc√°stica y tendencia deterministica?

Veamoslo con un ejemplo sacado de la Wikipedia:

<figure>
<img src="Tendencia estocastica y deterministica .svg"/>
<figcaption>Img 3: Ejemplo de estacionaria en tendencia vs raiz unitaria </figcaption>
</figure>

El diagrama anterior representa un ejemplo de una potencial ra√≠z unitaria. La l√≠nea roja representa una ca√≠da observada en la producci√≥n, la verde muestra el camino de la recuperaci√≥n si la serie tiene una ra√≠z unitaria. Azul muestra la recuperaci√≥n si no hay ra√≠z unitaria y la serie es estacionaria tendencia. La l√≠nea azul vuelve a cumplir y seguir la l√≠nea de tendencia de puntos, mientras que la l√≠nea verde se mantiene permanentemente por debajo de la tendencia. La  hip√≥tesis de ra√≠z unitaria tambi√©n sostiene que un aumento en la producci√≥n dar√° lugar a niveles de producci√≥n superiores a la tendencia del pasado.

```{r}
acf_variaciones <- acf(df_variaciones$variaciones)
acf_variaciones$acf
T<-length(df_variaciones$variaciones)
cuantil <- qnorm(0.975, 0,1)
bc <- cuantil/sqrt(T)
paste0("Declare la autocorrelaci√≥n como significativa si |r|>", round(bc,5))
```

Hemos agotado el an√°lisis del ACF, ahora pasemos al an√°lis del PACF:

```{r}
pacf_variaciones <-  pacf(df_variaciones$variaciones)
```

Ac√° notamos que el diagrama PACF parece dominado por una mixtura de decaimientos exponenciales y sinusoidales amortiguadas (no exactamente, pero en forma aproximada)

**Conclusi√≥n**:

Tenemos un ACF que se corta despu√©s del rezago 2 y un PACF con comportamiento sinosuoidal que pueden ser asociados a un modelo de tipo MA(2) seg√∫n se muestra en la tabla de identificaci√≥n de modelos del cap√≠tulo 5. 

## 4.5 Caso 1: Modelo Autorregresivo AR(p) puro

Es importante estudiar casos particulares del modelo ARIMA. Por ejemplo, si suponemos que d=q=0, estamos ante el caso de un modelo autorregresivo puro. 

Decimos que $\left\{X_t,t \in \mathbb{Z}\right\}$ es un proceso autoregresivo de orden $p$, y escribimos $X_t \sim\ AR(p)$, si satisface la ecuaci√≥n de diferencias.

\begin{equation}X_t - \mu = \phi_1 (X_{t-1} - \mu) + \phi_2(X_{t-2}-\mu)+ ... + \phi_p(X_{t-p}-\mu)+ \varepsilon_t\label{eq:2.24}\end{equation}


Donde $\mu,\phi_1,...\phi_p$ son par√°metros reales y $\varepsilon \sim\ RB(0,\sigma^{2})$ que quiere decir que el proceso autoregresivo tiene como condici√≥n que el error sea Ruido Blanco. N√≥tese que $E(X_t)= \mu$ y si escribimos el proceso en la forma:

$$X_t = \phi_0 + \phi_1 X_{t-1} + ... \phi_p X_{t-p} + \varepsilon_t$$

Entonces:

$$\mu = E(X_t) = \frac{\phi_0}{1-\phi_{1} - ... - \phi_p}$$

Definamos el operador retroactivo B a trav√©s de $B^{s}X_t = X_{t-s},s \geq 1$. Entonces (\ref{eq:2.24}) puede ser escrita.

\begin{equation}\phi(B)\tilde{X}_t = \varepsilon_t\label{eq:2.25}\end{equation}

Donde $\phi(B) = 1 - \phi_1B - \phi_2B^2 - ... -\phi_pB^{p}$ es el operador autoregresivo de orden $p$ y $\tilde{X}_t = X_t - \mu$. Suponga $\mu = 0$ en adelante. 


[Proceso AR(1):]{style="color: red;"} Un caso particular importante es el proceso AR(1).

\begin{equation}X_t = \phi X_{t-1} + \varepsilon_t\label{eq:2.26}\end{equation}.

Como $\phi(B) = 1 - \phi B$. A trav√©s de sustituciones sucesivas obtenemos.

$$X_t = \sum\limits_{j=0}^{r} \phi^j \varepsilon_{t-j} + \phi^{r+1}X_{t-r-1}$$

Si $X_t$ fuera estacionario, con varianza finita $\sigma^2 X$, entonces.

$$E[X_t - \sum\limits_{j=0}^{r}\phi^j \varepsilon_{t-j}]^2 = \phi^{2r+2}E[X_{t-r-1}^2] = \phi^{2r+2}{\sigma^2}_X$$

Si $|\phi| < 1$, $\phi^{2(r+1)} \rightarrow 0$, cuando $r \rightarrow \infty$, por lo tanto sobre esta suposici√≥n, podemos escribir.

\begin{equation}X_t = \sum\limits_{j=0}^{\infty}\phi^j \varepsilon_{t-j}\label{eq:2.27}\end{equation}

Donde la convergencia es en media cuadr√°tica. Luego, la condici√≥n $|\phi|<1$ es suficiente para que $X_t$ sea estacionario. Multiplicando ambos miembros de (\ref{eq:2.26}) por $X_{t-\tau}$ y tomando la esperanza, obtenemos.

$$\gamma_{\tau} = \phi \gamma_{\tau-1} = ... = \phi^{\tau}\gamma_0$$
Adem√°s de (\ref{eq:2.27}), obtenemos:


\begin{equation}\gamma_0 = {\sigma^2}_X = \sigma^2 \sum\limits_{j=0}^{\infty}\phi^{2j} = \frac{\sigma^2}{1-\sigma^2}\label{eq:2.28}\end{equation}

De lo que se sigue:

$$\gamma_{r} = \frac{\sigma^2}{1-\phi^2}\phi^{\tau}, \tau \geq 0$$

Como $\gamma_\tau$ es sim√©trica, podemos escribir finalmente la f.a.c.v (funci√≥n de autocovarianza) de un proceso AR(1) como:

\begin{equation}\gamma_\tau = \frac{\sigma^2}{1 - \phi^2} \phi^{\lvert \tau \rvert}, \quad \tau \in\mathbb{Z}\label{eq:2.29}\end{equation}

La f.a.c de $X_t$ es obtenida de (\ref{eq:2.29}), es decir,


\begin{equation}\rho_\tau = \frac{\gamma_\tau}{\gamma_0} = \phi^{\lvert \tau \rvert}, \quad \tau \in \mathbb{Z}\label{eq:2.30}\end{equation}

A continuaci√≥n dibujamos dos ejemplos de procesos AR(1), junto con sus respectivas funciones de autocorrelaci√≥n 

<figure>
<img src="Procesos AR(1).png"/>
<figcaption>Img 4: Ejemplos de ACF para procesos AR(1) a) $\phi=0.8$ b) $\phi=-0.8$</figcaption>
</figure>

[Proceso AR(p):]{style="color: red;"}

Ahora analicemos la soluci√≥n para el proceso en (\ref{eq:2.24}) en la forma (\ref{eq:2.27}), esto es:

\begin{equation}X_t = \sum^{\infty}_{j=0}\psi_j\varepsilon_{t‚àíj}\label{eq:2.31}\end{equation}

De (\ref{eq:2.25}) tenemos formalmente que:

$$X_t = \phi(B)^{-1}\varepsilon_t=\psi(B)\varepsilon_t$$
Donde $\psi(B) = 1+\psi_1B+\psi_2B^2+. . ..$ en analog√≠a al caso AR(1), debemos tener que $\sum_{j}{\psi_j}^2<\infty$ para que desde luego (\ref{eq:2.31}) sea una soluci√≥n estacionaria. 

Como $\phi(B)\psi(B) = 1$, los coeficientes $\psi_j$'s pueden ser obtenidos de esta identidad en funci√≥n de los $œÜ_j$‚Äôs. Se puede demostrar que una condici√≥n para que $X_t$ sea estacionario es que todas las raices de $\phi(B)=0$ est√©n por fuera del c√≠rculo unitario. En particular para $p=1$, $\phi(B)=1-\phi B=0$ implica que $B=\phi^{-1}$, lo que conllev√≥ a que se exigiera que $|\phi|<1$. 

Suponiendo un proceso estacionario, multiplicando a ambos miembros de (\ref{eq:2.24}) por $X_{t-\tau}$, y tomando valores esperados, obtenemos, para $\tau>=0$,

\begin{equation}{\sigma^2}_{x} = \frac{\sigma^2}{1-\phi_1\rho_1-\phi_2\rho_2-...-\phi_p\rho_p}\text{ para $\tau=0$,}\label{eq:2.32}\end{equation}

\begin{equation}\gamma_{\tau} = \phi_1\gamma_{\tau-1}+\phi_2\gamma_{\tau-2}+...+\phi_p\gamma_{\tau-p}\text{ para $\tau>0$,}\label{eq:2.33}\end{equation}

Una misma ecuaci√≥n es satisfecha por los $\rho_{\tau}$, bastando con dividir todos los t√©rminos por $\gamma_0$. Una soluci√≥n general de la ecuaci√≥n fue dada por (Miller, 1969):

\begin{equation}\gamma_{\tau} = A_1{G_1}^{\tau}+A_2{G_2}^{\tau}+...+A_p{G_p}^{\tau}\label{eq:2.34}\end{equation}

Donde los ${G_i}$ satisfacen:

$\phi(B)=\prod_{i=1}^{p}(1-G_iB)$

Y como las raices de $\phi(B)=0$ deben estar fuera del c√≠rculo unitario, debemos tener que $|G_i|<1$, para todo $i=1,2,..,p$

Ahora bien si fijamos $\tau=1,2,..,p$ en (\ref{eq:2.33}), obtenemos:

\begin{equation}\Gamma_p\vec{\phi}_p=\gamma_p\label{eq:2.35}\end{equation}

Donde $\Gamma_p = [\gamma_{ij}]$ con $\gamma_{ij}=\gamma_{|i-j|}$, $i,j=1,2,...,p$,
$\vec{\phi}_p = (\phi_1,\phi_2,...,\phi_p)$.

Las ecuaciones (\ref{eq:2.35}) pueden ser utilizadas para obtener estimadores de los par√°metros $\phi_j$'s, sustituyendo la funci√≥n de autocovarianzas por sus estimados. Estos estimadores son llamados estimadores de **Yule-Walker**.

Un an√°lisis de  (\ref{eq:2.34}) nos permite concluir que la funci√≥n de autocovarianza y de autocorrelaci√≥n de un proceso autorregresivo de orden $p$ es una mixtura de exponenciales (correspondientes a las raices $G_i$ que sean reales) y/o senosoidales amortiguadas (correspondientes a los pares de raices complejas conjugadas).

En la siguiente figura tenemos las funciones de autocorrelaci√≥n para dos procesos AR(2), uno con $\phi=0.5, \phi_2=0.3$ y otro con $\phi_1=1,\phi_2=-0.89$.De donde se deduce que:

**Proceso 1**:

$\phi(B)=1-0.5B-0.3B^2$ de modo que necesitamos raices para: $0.3B^2+0.5B-1=0$

As√≠ que:

$B_1 = -2.84027...$ y $B_2=1.1736$

Luego las raices son reales, y caen fuera del c√≠rculo unitario (tienen magnitud mayor a 1). Lo que indica que el proceso ser√° estacionario y la funci√≥n de autocorrelaci√≥n estar√° dominada por un decaimiento exponencial.

**Proceso 2**:

$\phi(B)=1-B+0.89B^2$ de modo que necesitamos raices para: $0.89B^2-B+1=0$

As√≠ que:

$B_1 = 0.561798 - 0.898876 i$ y $B_2 = 0.561798 + 0.898876i$

Luego las raices son complejas conjugadas, y de nuevo caen fuera del c√≠rculo unitario pues $0.5617978^2+0.898876^2>1$, as√≠ que el proceso tambi√©n es estacionario y la funci√≥n de autocorrelaci√≥n estar√° dominada por funciones senosoidales amortiguadas.

<figure>
<img src="Procesos AR(p).png"/>
<figcaption>Img 5: Ejemplos de ACF para procesos AR(2) a) $\phi=0.5, \phi_2=0.3$ b) $\phi_1=1,\phi_2=-0.89$</figcaption>
</figure>

## 4.6 Caso 2: Proceso de medias m√≥viles MA(q) puro

Decimos que $\{X_t, t \in \mathbb{Z}\}$ es un proceso de medias m√≥viles de orden $q$, denotado como $MA(q)$, si satisface la ecuaci√≥n de diferencias.

\begin{equation}X_t = \mu + \varepsilon_t - \theta_1 \varepsilon_{t-1} - \ldots - \theta_q \varepsilon_{t-q},\label{eq:2.36}\end{equation}

Donde $\mu,\theta_1, ..., \theta_q$ son constantes reales y $\varepsilon_t \sim\ RB(0,\sigma^2)$.

N√≥tese que $X_t$ es estacionario, con media $\mu$, y como $\varepsilon_t$ son no correlacionados, podemos obtener f√°cilmente la varianza del proceso.

\begin{equation} \sigma^2_X = \sigma^2 \left(1 + \theta^2_1 + \ldots + \theta^2_q\right)\label{eq:2.37}\end{equation}

Suponga $\mu=0$. En cuanto a la funci√≥n de autocovarianza, tenemos.


$$\gamma_\tau = E\{X_t X_{t-\tau}\} = \gamma_\varepsilon(\tau) - \sum_{k=1}^q \theta_k \gamma_\varepsilon(k - \tau)$$

$$-\sum_{k=1}^q \theta_{\mathcal{l}} \gamma_\varepsilon(\tau + l) + \sum_{k=1}^q \sum_{l=1}^q \theta_k \theta_{l} \gamma_\varepsilon(\tau + l - k)$$

Donde estamos denotando por $\gamma_{\varepsilon}(\tau)$ una f.a.c.v (funci√≥n de autocovarianza) de $\varepsilon_t$. Resulta entonces,

\begin{equation} \gamma_\tau = \begin{cases} \sigma^2 (-\theta_\tau + \theta_1 \theta_{\tau+1} + \ldots + \theta_q \theta_{q-\tau}), & \text{si } \tau = 1, \ldots, q \\ 0, & \text{si } \tau > q \\\gamma_{-\tau}, & \text{si } \tau < 0\end{cases}\label{eq:2.38}\end{equation}

De (\ref{eq:2.37}) y (\ref{eq:2.38}) obtenemos una f.a.c (funci√≥n de autocorrelaci√≥n) del proceso $MA(q)$

\begin{equation}\rho_\tau = \begin{cases}\frac{-\theta_\tau + \theta_1 \theta_{\tau+1} + \ldots + \theta_q \theta_{q-\tau}}{1 + \theta_1^2 + \ldots + \theta_q^2}, & \text{si } \tau = 1, \ldots, q \\0, & \text{si } \tau > q \\\rho_{-\tau}, & \text{si }\tau < 0\end{cases}\label{eq:2.39}\end{equation}

Observamos, entonces, que la f.a.c.v (o f.a.c) de un proceso $MA(q)$ se anula para $|\tau| > q$. En particular, para un proceso $MA(1)$,

\begin{equation}X_t = \varepsilon_t - \theta \varepsilon_{t-1}\label{eq:2.40}\end{equation}

Obtenemos

$$\text{Var}(X_t) = \sigma^2_X = \sigma^2 (1 + \theta^2)$$,

\begin{equation}\rho_\tau =\begin{cases}\frac{-\theta}{1 + \theta^2}, & \text{si } \tau = \pm 1 \\0, & \text{si } \lvert \tau\rvert > 1\end{cases}\label{eq:2.41}\end{equation}

Definiendose el operador de medias m√≥viles de orden $q$ por:

$\theta(B) = 1 - \theta_1B - \theta_2B^2 - \ldots - \theta_qB^q$

El proceso (\ref{eq:2.36}) puede ser escrito

\begin{equation}X_t = \theta(B)\varepsilon_t\label{eq:2.42}\end{equation}

En particular, para el proceso $MA(1)$ tenemos $\theta(B)=1-\theta B$, de modo que podemos escribir.

$X_t = (1 - \theta B)\varepsilon_t$

De donde formalmente se sigue:

$\varepsilon_t = (1 - \theta B)^{-1}X_t = (1 + \theta B + \theta^2 B^2 + \ldots)X_t$,

Por lo que tenemos:

\begin{equation}X_t = -\theta X_{t-1} - \theta^2 X_{t-2} - \ldots + \varepsilon_t\label{eq:2.43}\end{equation}

Si $|\theta| < 1$, la serie del lado derecho de (\ref{eq:2.43}) converge. En esta ecuaci√≥n, expresamos $X_t$ como un proceso autoregresivo de orden infinito. Decimos que $|\theta| < 1$ es una condici√≥n de invertibilidad para el proceso MA(1). En general, el proceso (\ref{eq:2.36}) puede escribirse de la siguiente manera:

\begin{equation} X_t = \sum\limits_{j=1}^{\infty}\pi_j X_{t-j} + \varepsilon_t \label{eq:2.44}\end{equation}

Si se satisface la siguiente condici√≥n de invertibilidad: todas las ra√≠ces de $\theta(B) = 0$ deben estar fuera del c√≠rculo unitario. Ver Box, Jenkins y Reinsel (1994) para obtener m√°s detalles.

La relaci√≥n (\ref{eq:2.44}) puede ser escrita:

\begin{equation}\pi (B)X_t = \varepsilon_t\label{eq:2.45}\end{equation},

donde $\pi(B) = 1 - \pi_1B - \pi_2B^2 - \ldots$, de modo que $\pi(B) = \theta(B)^{-1}$. Por lo tanto, los coeficientes $\pi_j$ pueden obtenerse a partir de la identidad $\theta(B)\pi(B) = 1$.

[Nota importante sobre invertibilidad:]{style="color: red;"}

La condici√≥n de invertibilidad es fundamental en el contexto de un proceso de media m√≥vil (MA) por varias razones importantes:

- Existencia de una representaci√≥n causal: Un proceso MA invertible tiene una representaci√≥n causal, lo que significa que los valores pasados (anteriores en el tiempo) del proceso pueden utilizarse para predecir valores futuros. Esta caracter√≠stica es esencial en aplicaciones donde se necesita prever o modelar valores futuros del proceso.

- Estimaci√≥n de par√°metros: Para estimar los par√°metros de un modelo MA utilizando t√©cnicas de ajuste, como el m√©todo de m√°xima verosimilitud, es fundamental que el proceso sea invertible. Un proceso no invertible puede llevar a estimaciones incorrectas de los par√°metros del modelo, lo que afectar√≠a la calidad de las predicciones.

- Estabilidad del proceso: La invertibilidad est√° relacionada con la estabilidad del proceso. En un proceso invertible, las caracter√≠sticas estad√≠sticas del proceso tienden a ser m√°s estables y predecibles en comparaci√≥n con un proceso no invertible, que puede ser m√°s impredecible y menos estable.

- Validaci√≥n del modelo: La invertibilidad es un criterio importante para validar la adecuaci√≥n de un modelo MA. Un proceso no invertible puede ser una se√±al de que el modelo no es adecuado para representar la serie de tiempo, lo que podr√≠a requerir una revisi√≥n y ajuste del modelo.

- Interpretaci√≥n de los par√°metros: En un proceso invertible, los coeficientes del modelo MA tienen una interpretaci√≥n m√°s clara. Por ejemplo, en un modelo MA(1) invertible, el coeficiente del rezago 1 ($\theta_1$) representa la influencia inmediata de los errores pasados en el valor actual, lo que facilita la interpretaci√≥n y el an√°lisis del proceso.

En resumen, la condici√≥n de invertibilidad es esencial para garantizar que un modelo MA sea adecuado y √∫til para el an√°lisis y la predicci√≥n de una serie de tiempo. Sin esta condici√≥n, la interpretaci√≥n de los par√°metros y la capacidad del modelo para predecir valores futuros pueden verse comprometidas, lo que subraya la importancia de verificar y asegurar que un proceso MA sea invertible.

Consideremos entonces el siguiente ejemplo:

\begin{equation} X_t = \varepsilon_t - 0.8 \varepsilon_{t-1}, \varepsilon_t \sim \text{i.i.d. }\mathcal{N}(0,1)\label{eq:2.46}\end{equation}

Donde tenemos un proceso MA(1) con $\theta_1 = 0.8$. Para este proceso reemplazando en (\ref{eq:2.41}) tenemos que $\rho_1 = \frac{-0.8}{(1+0.8^2)}=-0.49$, mientras que $\rho_{\tau}=0$, $\tau>=2$, y $\rho_{-\tau}=\rho_{\tau}$, por lo tanto tenemos un gr√°fico para el proceso $X_t$ y su funci√≥n de autocorrelaci√≥n como el que se muestra a continuaci√≥n:

<figure>
<img src="Proceso MA(1).png"/>
<figcaption>Img 6: Proceso MA(1) simulado, $\theta_1=0.8$ y su f.a.c.</figcaption>
</figure>

En la gr√°fica puede apreciar como las autocorrelaciones estimadas con base en la √∫nica trayectoria simulada tienden a ser despreciables despu√©s del lag 1, no logrando ser 0 completamente seguramente debido a que se trata de una muestra y no del proceso te√≥rico como un todo. 

Las bandas de confianza dibujadas en la f.a.c indican que en el 95% de los casos, trayectorias simuladas bajo el modelo te√≥rico tendr√°n autocorrelaciones que caen dentro de las bandas, o lo que es lo mismo, que si el proceso subyacente es realmente el proceso te√≥rico, s√≥lo en el 5% de los casos observaremos autocorrelaciones por fuera de esas bandas, de manera que si sistem√°ticamente notamos que las autocorrelaciones est√°n cayendo por fuera de las bandas, seguramente se deba a que el proceso que genera los datos realmente no sigue un modelo te√≥rico de tipo MA(1).

## 4.7 Caso 3: Proceso ARMA(p,q)

C√≥mo se plante√≥ al inicio de este cap√≠tulo un proceso ARMA(p,q) tiene la siguiente estructura:

\begin{equation}X_t - \mu = \phi_1(X_{t‚àí1} - \mu) + \phi_2(X_{t‚àí2} - \mu)+... + \phi_p(X_{t‚àíp}-\mu) + \varepsilon_t - \theta_1\varepsilon_{t‚àí1} - \theta_2\varepsilon_{t‚àí2}-...-\theta_q\varepsilon_{t‚àíq}\label{eq:2.47}\end{equation}

Donde $\varepsilon_t \sim RB(0,\sigma^2)$ de donde se sigue que la media del proceso es $\mu$. Si usamos los operadores autorregresivos y de medias m√≥viles, definidos en las dos secciones anteriores, podemos escribir (\ref{eq:2.47}) de la forma:

\begin{equation}\phi(B)\tilde{X_t}=\theta(B)\varepsilon_t\label{eq:2.48}\end{equation}

Donde $\tilde{X_t}=X_t - \mu$, supongamos sin p√©rdida de generalidad que a partir de ahora $\mu=0$.

[Proceso ARMA(1,1):]{style="color: red;"} Un modelo frecuentemente usado es el ARMA(1,1)

\begin{equation}X_t=\phi X_{t-1}+\varepsilon_t-\theta\varepsilon_{t-1}\label{eq:2.49}\end{equation}

Es facil ver que, por sustituciones sucesivas, podemos escribir que:

$$X_t=\psi(B)\varepsilon_t$$
Donde $\psi_j = \phi^{j-1}(\phi - \theta)$, $j>=1$. Una condici√≥n de estacionariedad es la misma que para un proceso AR(1), es decir, $|\phi|<1$. Del mimso modo una condici√≥n de invertibilidad $|\theta|<1|$ es v√°lida aqu√≠ e implica que podemos escribir el proceso en la forma (\ref{eq:2.44}), con pesos $\pi_j = \phi^{j-1}(\phi - \theta)$. 

Para un proceso ARMA(p,q) gen√©rico la condici√≥n de estacionariedad es la misma que para procesos AR(p), es decir, que las raices de $\phi(B)=0$ deben estar fuera del c√≠rculo unitario, y una condici√≥n de invertibilidad es la misma que para un proceso MA(q), es decir, que las raices de $\theta(B)=0$ est√©n por fuera del c√≠rculo unitario. 

Multiplicando (\ref{eq:2.47}) con $\mu=0$ por $X_{t-\tau}$ y tomando esperanzas, obtenemos:

\begin{equation}\gamma_{\tau}=\phi_1 \gamma_{\tau-1} + \phi_2 \gamma_{\tau-2}+...+\phi_p \gamma_{\tau-p}+\gamma_{X\varepsilon}(\tau)-\theta_1\gamma_{X\varepsilon}(\tau-1)-...-\theta_q\gamma_{X\varepsilon}(\tau-q)\label{eq:2.50}\end{equation}

Donde $\gamma_{X\varepsilon}(\tau)$ es la covarianza cruzada entre $X_t$ y $\varepsilon_t$, definida por:

$$\gamma_{X\varepsilon}(\tau)=E(\varepsilon_tX_{t-\tau})$$
Como $X_{t-\tau}$ si depende de choques $\varepsilon_t$ ocurridos hasta el instante $t-\tau$, tenemos que esta covarianza cruzada si es diferente de 0 para $\tau<=0$, luego:

\begin{equation}\gamma_{\tau}=\phi_1 \gamma_{\tau-1} + \phi_2 \gamma_{\tau-2}+...+\phi_p, \tau>q \label{eq:2.51}\end{equation}

Una conclusi√≥n es que las autocovarianzas (y por lo tanto las autocorrelaciones) de lags 1,2,...,q ser√°n afectadas por los par√°metros de medias m√≥viles, pero para $\tau>q$ las mismas se comportan como los modelos autorregresivos. 

Para el caso del modelo dado en (\ref{eq:2.49}), obtenemos f√°cilmente que:

$\rho_1 = \frac{\gamma_1}{\gamma_0}=\frac{(1-\phi \theta)(\phi - \theta)}{1+\theta^2-2\phi\theta}$

Y para $\tau>1$:

$\rho_{\tau}=\phi\rho_{\tau-1}$

La siguiente figura representa 100 observaciones generadas para un proceso ARMA(1,1), con $\phi=0.8, \theta=0.3$ y $\varepsilon_t \sim N(0,1)$. En la figura tenemos tambi√©n un gr√°fico de la f.a.c.

<figure>
<img src="Proceso ARMA(1,1).png"/>
<figcaption>Img 7: Proceso ARMA(1,1) simulado, $\phi=0.8$, $\theta_1=0.3$ y f.a.c f.a.c.</figcaption>
</figure>

## 4.8 Breve descripci√≥n del algoritmo de modelado

Para el establecimiento de un modelo ARIMA sobre una serie temporal hay tres estrategias a considerar:

[1. Identificaci√≥n:]{style="color: red;"} En esta etapa nos dedicamos a estudiar cu√°l es el tipo y orden adecuado para modelar la serie bajo estudio, y para ello nos apoyamos en los gr√°ficos de autocorrelaci√≥n y autocorrelaci√≥n parcial. Otras formas alternativas de identificaci√≥n es escoger los √≥rdenes del modelo que minimicen una cantidad, t√≠picamente aquella que represente el mejor balance entre error residual y cantidad de par√°metros en el modelo.

[2. Estimaci√≥n:]{style="color: red;"} Existen al menos tres m√©todos: m√©todos de momentos, m√©todos de m√≠nimos cuadrados y m√©todos de m√°xima verosimilitud, sin embargo estimadores obtenidos cuando usamos el m√©todo de momentos no tienen tan buenas propiedades como cuando se usan los dem√°s. Por eso estos estimadores son usados como valores iniciales para procedimientos m√°s complejos como el de MV.

[3. Diagn√≥stico:]{style="color: red;"} Despu√©s de estimar un modelo tenemos que verificar que √©l representa bien los datos observados. Cualquier deficiencia en el modelo puede sugerir un modelo alternativo que si represente bien los datos. Un m√©todo que podemos seguir si sabemos que un modelo m√°s elaborado es necesario (donde por elaborado entendemos uno con m√°s par√°metros) es usar "superajuste", el cual consiste en estimar un modelo con par√°metros extras validando si los mismos son significativos e incluso si reducen de forma substancial la varianza residual. Esto es muy √∫til cuando sabemos la direcci√≥n en la cual puede estar ocurriendo la inadecuaci√≥n del modelo, algo que puede indigarse analizando los residuales. Ac√° aplicamos t√©cnicas como el **Test de autocorrelaci√≥n residual**, y el **Test de Box-Pierce-Ljung**

En este informe estudiaremos de manera general dichos procedimientos, sin embargo para m√°s detalles el lector puede consultar Morettin y Toloi (2006), y a Box, Jenkins y Reinsel(1994)

El ciclo (1) a (3) que se describe arriba debe ser iterado hasta que en la etapa (3) verifiquemos que el modelo si es adecuado para los fines deseados. En muchas situaciones, m√°s de un modelo puede ser considerado adecuado, y la decisi√≥n de cu√°l adoptar va a depender de alg√∫n criterio. Por ejemplo escojemos el modelo que minimice el error cuadr√°tico de pron√≥stico. 

[4. Pron√≥sticos:]{style="color: red;"} Una vez se ha obtenido un modelo adecuado podemos usarlo para predicci√≥n. Ac√° podemos considerar la misma estructura funcional del modelo para derivar ecuaciones de pron√≥stico que consideren la estructura temporal ya detectada para el modelo estimado. 

# 5. T√âCNICAS GR√ÅFICAS PARA IDENTIFICACI√ìN DE MODELOS

El objetivo de la etapa de identificaci√≥n es determinar los valores p,d,q del modelo ARIMA(p,d,q). Un procedimiento de identificaci√≥n consiste de 3 partes.

a) Verificar si existe necesidad de una transformaci√≥n sobre la serie original con el objetivo de estabilizar su varianza.

b) Tomar diferencia de la serie obtenida en el item a), tantas veces como sea necesario para obtener una serie estacionaria, de modo que el proceso $\Delta X_t$ sea reducido a un proceso ARMA(p,q). El n√∫mero de diferencias $d$ necesarias para que el proceso se torne estacionario es alcanzado cuando la f.a.c muestral de $\Delta X_t$ decrece r√°pidamente a cero. En esta etapa un test para verificar la existencia de raices unitarias en un polinomio autorregresivo puede ser de gran utilidad. Se puede utilizar ac√° el test de Dickey y Fuller(1979).

c) Identificar el proceso ARMA(p,q) resultante por medio del an√°lisis de las autocorrelaciones y las autocorrelaciones parciales estimadas, cuyos comportamientos deben imitar los comportamientos de las respectivas cantidades te√≥ricas. Estos comportamientos para los modelos AR, MA, ARMA fueron estudiados en las secciones previas, pero un resumen de estas propiedades se ofrece a continuaci√≥n:

| Orden | (1,d,0) | (0,d,1) |
| :--- | :--- | :--- |
| Comportamiento de $p_k$| Disminuye exponencialmente |Solamente $\rho_1 \ne 0$  |
| Comportamiento de $\phi_{kk}$| Solamente $\phi_{11} \ne 0$ |Disminuci√≥n exponencial dominante|
| Estimaciones iniciales| $\phi = \rho_1$ | $\rho_1 = -\frac{\theta}{1 + \theta^2}$ |
| Dominio o regi√≥n admisible| $-1 < \phi < 1$ | $-1 < \theta < 1$ |

| Orden | (2,d,0) | (0,d,2) |
| :--- | :--- | :--- |
| Comportamiento de $p_k$| Mezcla de exponenciales u ondas  sinusoidales  amortiguadas | Solamente $\rho_1 \ne 0$ y $\rho_2 \ne 0$  |
| Comportamiento de $\phi_{kk}$| Solamente $\phi_{11} \ne 0$ y $\phi_{22} \ne 0$ |Dominada por una mezcla de exponenciales o   sinusoidales amortiguadas.|
| Estimaciones iniciales| $\begin{cases}\phi_1 = \frac{\rho_1 (1 - \rho_2)}{1 - \rho_1^2} \\\phi_2 = \frac{\rho_2 - \rho_1^2}{1 - \rho_1^2}\end{cases}$ | $\begin{cases}\rho_1 = \frac{-\theta_1 (1 - \theta_2)}{1 + \theta_1^2 + \theta_2^2}, \\\rho_2 =\frac{-\theta_2}{1 + \theta_1^2+\theta_2^2}\end{cases}$ |
| Dominio o regi√≥n admisible| $\begin{cases}-1 < \phi_2 < 1 \\\phi_2 - \phi_1 < 1 \\\phi_2 + \phi_1 < 1\end{cases}$ | $\begin{cases}-1 < \theta_2 < 1,\\\theta_2 - \theta_1 < 1, \\\theta_2 + \theta_1 < 1\end{cases}$|

| Orden | (1,d,1) |
| :--- | :--- |
| Comportamiento de $p_k$ | Decae exponencialmente despu√©s del retraso (lag) 1|
| Comportamiento de $\phi_{kk}$ | Dominada por disminuci√≥n exponencial despu√©s del retraso (lag) 1|
| Estimaciones iniciales |$\rho1 = \frac{(1 - \phi \theta)(\phi - \theta)}{1 + \theta^2 - 2 \phi \theta}, \rho_2 = \rho_1\phi$|
| Dominio o regi√≥n admisible |$-1 < \phi <1 , -1 < \theta < 1$|

Vamos a simular algunos modelos te√≥ricos para dar cuenta de la complicaci√≥n que tenemos para identificarlos s√≥lo usando el gr√°fico de la serie de tiempo:

```{r}
phi <- 0.7  # Coeficiente autorregresivo (AR)
theta <- -0.4  # Coeficiente de promedios m√≥viles (MA)
n <- 100  # N√∫mero de observaciones a simular
# Simula el proceso ARMA
ar_puro <- arima.sim(model = list(order = c(1, 0, 0), ar = phi), n=n)
ma_puro <- arima.sim(model = list(order = c(0, 0, 1), ma = theta), n=n)
arma <- arima.sim(model = list(order = c(1, 0, 1), ar = phi, ma = theta), n=n)
arima <- arima.sim(model = list(order = c(1, 1, 1), ar = phi, ma = theta), n=n)

par(mfrow = c(2, 2))

plot(ar_puro, type = 'l', main = "Simulaci√≥n de un Proceso AR(1)")
plot(ma_puro, type = 'l', main = "Simulaci√≥n de un Proceso MA(1)")
plot(arma, type = 'l', main = "Simulaci√≥n de un Proceso ARMA(1,1)")
plot(arima, type = 'l', main = "Simulaci√≥n de un Proceso ARIMA(1,1,1)")

par(mfrow = c(1, 1))
```

Se puede apreciar que cada modelo tiene comportamiento diferente uno respecto, pero salvo que los gr√°fiquemos todos juntos es d√≠ficil reconocer qu√© los diferencia respecto a los dem√°s. Por eso en este caso es mejor usar una estrategia basada en los gr√°ficos de ACF y PACF, porque la forma del gr√°fico da informaci√≥n sobre la estructura del componente autorregresivo y de media m√≥vil del proceso subyacente. 

# 6. Estimaci√≥n de par√°metros

R nos ofrece m√©todos autom√°ticos como la funci√≥n auto.arima para estimar los par√°metros de un modelo ARIMA, m√°s a√∫n nos ofrece un m√©todo para probar en autom√°tico con diferentes alternativas de modelos. Todo esto ocurre silenciosamente. Al final R nos ofrece el mejor modelo que encontr√≥, explorando internamente diferentes √≥rdenes para el componente AR(p) y MA(q). 

```{r}
library(forecast)
model <- auto.arima(df_variaciones$variaciones)
summary(model)
```

El modelo que me entrega R indica un orden 3 para un modelo de media m√≥vil puro. Construya usted a continuaci√≥n el modelo de media m√≥vil puro de orden 2 que dedujimos que ser√≠a apropiado con base en los an√°lisis gr√°ficos de ACF y PACF realizados en el cap√≠tulo 4.

```{r}

```

¬øQu√© puede decir sobre las medidas de desempe√±o de ambos modelos?


# 7. Diagn√≥stico del modelo


# 8. Uso en predicci√≥n


# 9. CASO APLICADO DATASET DE AGUACATE

Primero observamos la apariencia de la serie de tiempo diaria:


```{r}
plot(arima.sim(model = list(order = c(0, 0, 3), ma = c(0.0006, -0.1974,0.0716)), n=length(df_variaciones$variaciones)))
```

# 10. Conclusiones finales y √°reas de profundizaci√≥n
