---
title: "REGRESI√ìN LINEAL - PRIMERA CLASE"
author: "Maestr√≠a en Investigaci√≥n Operativa y Estad√≠stica"
output: 
  rmdformats::readthedown:
    css: styles.css
  html_document:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
date: "2023"
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: inline
---

<div>

<img src="https://media2.utp.edu.co/imagenes/Logo-UTP-Azul.png" alt="UPN" class="watermark"/>

</div>

```{r include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  echo = TRUE,
  message = FALSE
)

# Librer√≠as
library(tidyverse)
library(janitor)
library(openxlsx)
library(flextable)
library(viridis)
library(scales)
library(DT)
library(lubridate)
library(gridExtra)


```

# INTRODUCCI√ìN

## DOCENTES

### H√©ctor Hern√°n Montes Garc√≠a.

Ingeniero industrial de la Universidad Tecnol√≥gica de Pereira.

Estudios en Maestr√≠a en Ciencias con orientaci√≥n en Matem√°ticas
(pendiente trabajo de grado).

[Desempe√±o laboral]{style="color: red;"}

Cient√≠fico de datos con m√°s de 3 a√±os de experiencia en la construcci√≥n
de modelos de aprendizaje autom√°tico y soluciones de miner√≠a de datos
para obtener mejores conocimientos comerciales. Experiencia utilizando
SQL, bibliotecas de Python (matplotlib, seaborn, flask, scikit-learn,
pandas, numpy), modelos matem√°ticos y estad√≠sticos para ofrecer
soluciones robustas que agregan valor al negocio.

Ha trabajado para clientes del sector industrial y financiero en M√©xico
y Colombia, en √°reas tales como: - Mantenimiento predictido - Dise√±o de
campa√±as basadas en datos - Reconocimiento de im√°genes y aplicaciones de
datos no estructurados.

### Juli√°n Piedrah√≠ta Monroy

Ingeniero industrial.

Mag√≠ster en desarrollo agroindustrial

Universidad Tecnol√≥gica de Pereira

[Desempe√±o laboral]{style="color: red;"}

-   Consultor en soluciones an√°liticas para instituciones educativas.
-   Analista de datos en el observatorio social de la UTP.
-   Actualmente dise√±ador de tableros informativos (dashboards) para la
    Universidad Pedag√≥gica Nacional de Bogot√° y la misma Universidad
    Tecnol√≥gica de Pereira.
-   Docente catedr√°tico en el √°rea de inform√°tica y estad√≠stica general.
-   Uso principal del lenguaje R y sus librerias Tidyverse, RMarkdown,
    RShiny y otras.

## Contenido del curso

T1. Regresi√≥n Lineal, suposiciones y requisitos. T2. Estimaci√≥n de los
par√°metros e interpretaci√≥n de los valores. T3. Pruebas de hip√≥tesis
relacionadas con los par√°metros. T4. Evaluaci√≥n de modelos de acuerdo a
las suposiciones de normalidad e independencia. T5. Transformaciones de
las variables, y sus implicaciones en las pruebas de hip√≥tesis. T6.
Series de Tiempo. Estacionariedad y c√≥mo obtener una serie estacionaria
a partir de una que no lo es. Correlogramas. T7. Modelos de Box y Cox.
Estimaci√≥n de par√°metros. T8. Criterios de evaluaci√≥n de un modelo de
series de tiempo. Transformaciones.

## Bibliograf√≠a.

Regression Modeling and Data Analysis with Applications in R. Samprit
Chatterjee, Jeffrey S. Simonoff\
Montgomery, D. C., Peck, E. A., & Vining, G. G. (2002). Introducci√≥n al
an√°lisis de regresi√≥n lineal (3¬™ ed.). Limusa Wiley.\
Time Series Analysis and Its Applications: With R Examples by Shumway
and Stoffer.\
Time Series Analysis: Forecasting and Control by Author(s): George E. P.
Box, Gwilym M. Jenkins, Gregory C. Reinsel, Greta M. Ljung.\
<https://fhernanb.github.io/libro_regresion/rls.html#modelo-estad%C3%ADstico>\
<https://bookdown.org/victor_morales/SeriesdeTiempo/>

## Motivaci√≥n

Los m√©todos de regresi√≥n son t√©cnicas estad√≠sticas utilizadas para
modelar y comprender la relaci√≥n entre variables. La motivaci√≥n detr√°s
de estos m√©todos radica en la necesidad de analizar y cuantificar c√≥mo
una o m√°s variables independientes influyen en una variable dependiente.
Los m√©todos de regresi√≥n son ampliamente utilizados en diversos campos,
incluyendo la investigaci√≥n cient√≠fica, la econom√≠a, la medicina, la
ingenier√≠a y m√°s. A continuaci√≥n, se detallan algunas de las principales
motivaciones detr√°s de estos m√©todos:

![Imagen 1](Imagen%20de%20regresi√≥n%20lineal%201.jpeg)


[Modelado de Relaciones:]{style="color: red;"}

La motivaci√≥n fundamental de la regresi√≥n es modelar la relaci√≥n entre
variables. En muchos casos, existe la necesidad de entender c√≥mo una
variable dependiente cambia en funci√≥n de una o m√°s variables
independientes. Por ejemplo, en la econom√≠a, podr√≠a ser necesario
entender c√≥mo las tasas de inter√©s afectan el gasto del consumidor.

[Predicci√≥n y Estimaci√≥n:]{style="color: red;"} Los modelos de regresi√≥n
tambi√©n se utilizan para hacer predicciones y estimaciones. Dado un
conjunto de datos hist√≥ricos, los modelos de regresi√≥n pueden ayudar a
predecir valores futuros de la variable dependiente en funci√≥n de los
valores de las variables independientes. Esto es especialmente √∫til en
campos como la meteorolog√≠a, la finanzas y el marketing.

![Imagen 2](Imagen%20de%20regresi√≥n%20lineal%202.jpeg)
[Control y Optimizaci√≥n:]{style="color: red;"} En algunos casos, se
utilizan modelos de regresi√≥n para optimizar y controlar procesos. Por
ejemplo, en la manufactura, los modelos de regresi√≥n pueden usarse para
identificar las condiciones ideales que conducen a la m√°xima eficiencia
o calidad del producto.

[Identificaci√≥n de Factores Importantes:]{style="color: red;"} Los
m√©todos de regresi√≥n pueden ayudar a identificar qu√© variables
independientes tienen un impacto significativo en la variable
dependiente. Esto es valioso para comprender qu√© factores son m√°s
relevantes y merecen m√°s atenci√≥n en un an√°lisis.

[Validaci√≥n de Teor√≠as:]{style="color: red;"} En la investigaci√≥n
cient√≠fica y social, los modelos de regresi√≥n pueden usarse para validar
o refutar teor√≠as existentes. Al comparar los resultados del modelo con
las expectativas te√≥ricas, se puede evaluar la validez de las hip√≥tesis.

[Gesti√≥n de Riesgos:]{style="color: red;"} Los modelos de regresi√≥n
tambi√©n se utilizan para evaluar riesgos y tomar decisiones informadas.
En finanzas, por ejemplo, se pueden utilizar para evaluar el riesgo de
inversi√≥n y la exposici√≥n a diferentes factores del mercado.

** Ejemplo**

El modelo CAPM (Capital Asset Pricing Model) es un modelo de valoraci√≥n de activos financieros desarrollado por William Sharpe que permite estimar su rentabilidad esperada en funci√≥n del riesgo sistem√°tico. Su desarrollo est√° basado en diversas formulaciones de Harry Markowitz sobre la diversificaci√≥n y la teor√≠a moderna de Portfolio 1. El modelo CAPM es utilizado para calcular la rentabilidad que un inversionista debe exigir al realizar una inversi√≥n en un activo financiero en funci√≥n del riesgo que est√° asumiendo 2. El modelo CAPM establece una relaci√≥n lineal entre el rendimiento esperado de un activo y su riesgo sistem√°tico, medido por su beta 1. La f√≥rmula del modelo CAPM es la siguiente:

$E(r_i)=r_f+\beta_i*(E(r_m)-r_f)$

Donde:

$E(r_i)$ es la tasa de rentabilidad esperada de un activo concreto.
$rf$ es la rentabilidad del activo sin riesgo.
$\beta_i$ es la medida de la sensibilidad del activo respecto a su benchmark.
$E(r_m)$ es la tasa de rentabilidad esperada del mercado en que cotiza el activo.


![Imagen 3](Imagen%20de%20regresi√≥n%20lineal%203.jpeg)

[An√°lisis de Causa y Efecto:]{style="color: red;"} Los modelos de
regresi√≥n pueden ayudar a establecer relaciones causa-efecto entre
variables. Esto es √∫til para comprender c√≥mo los cambios en una variable
influyen en otras variables y viceversa.

![Imagen 4](Imagen%20de%20regresi√≥n%20lineal%204.png)


En resumen, la motivaci√≥n detr√°s de los m√©todos de regresi√≥n radica en
la necesidad de entender, modelar, predecir y cuantificar las relaciones
entre variables en una variedad de campos. Estos m√©todos permiten tomar
decisiones informadas, realizar an√°lisis profundos y obtener informaci√≥n
valiosa a partir de los datos disponibles.



# Ejercicio de introducci√≥n.

## Lectura de datos

A continuaci√≥n, cargaremos dos tablas de datos que contienen informaci√≥n sobre el precio del dolar, tambi√©n conocida como tasa representativa del mercado, y otra con la informaci√≥n del precio del aguacate Hass tomada de fuente gubernal y p√°gina del SIPSA.

Los enlaces para acceder a mayor informaci√≥n relacionada son:


https://www.agronet.gov.co/estadistica/Paginas/home.aspx?cod=11
https://www.bde.es/webbe/es/estadisticas/temas/tipos-cambio.html


```{r}

# Cargue y limpieza de datos.

dolar <- read.csv2("DATASETS/Tasa_de_Cambio_Representativa_del__Mercado_-Historico.csv") %>% 
            clean_names()


hass <- read.xlsx("DATASETS/Hass_Precios_Historicos.xlsx") %>% clean_names()


```

## Creaci√≥n de funciones.

Se programa una funci√≥n de flextable que mejorar√° la impresi√≥n de tablas de resumen.
Para usarla s√≥lo bastar√° invocar la funci√≥n ftable() al final de la sentencia (usando %>% tidyverse) o con el ftable() encapsular o encerrar la tabla que se quiere ajustar.

```{r}

# Funciones

## Funci√≥n para crear flextable
ftable <- function(x) {
  x %>% 
    flextable() %>% 
    theme_vanilla() %>%
    color(part = "footer", color = "#666666") %>%
    color( part = "header", color = "#FFFFFF") %>%
    bg( part = "header", bg = "#2c7fb8") %>%
    fontsize(size = 11) %>%
    font(fontname = 'Calibri') %>%
    # Ajustes de ancho y tipo de alineaci√≥n de las columnas
    set_table_properties(layout = "autofit") %>% 
    # width(j=1, width = 3) %>%
    align(i = NULL, j = c(2:ncol(x)), align = "right", part = "all")
}


```

Como ejemplo del uso de la funci√≥n:

*Sin flextable*

```{r}

dolar %>% 
  head(5) 
  

```

*Con flextable*

```{r}

dolar %>% 
  head(5) %>% 
  ftable()
  

```



# DEPURACI√ìN DE LOS DATOS

## Dolar

Como todo proceso de an√°lisis de datos, al menos en su mayor√≠a, es necesario realizar una depuraci√≥n y ajuste de los datos. Para este caso, fue necesario trabajar sobre la variable de la fecha y el valor.

```{r}

# Vamos a sacar un valor promedio de cada variable por mes
#Para el dolar vamos a tomar el campo vigenciadesde

dolar <- dolar %>% 
    mutate(fecha = as.Date(vigenciadesde, format = "%d/%m/%y")) %>% 
    mutate(mes = month(fecha), anio = year(fecha)) %>% 
    mutate(valor = as.double(str_replace(valor,",",""))) %>% 
    group_by(anio,mes) %>%
    summarise(promedio_dolar = mean(valor), .groups = "drop") 
    

```

A continuaci√≥n se muestra una fracci√≥n de los datos depurados.

```{r}

dolar %>%
  datatable()

```

## Aguacate


```{r}

hass %>% 
  head(5) %>% 
  ftable()


```


```{r}

# üñáÔ∏è Se genera el vector de meses para usarlo m√°s abajo con la funci√≥n match.
meses <- c("Enero", "Febrero", "Marzo", "Abril", "Mayo", "Junio", "Julio", "Agosto", "Septiembre", "Octubre", "Noviembre", "Diciembre")


hass <- hass %>% 
      # Otra forma de sacar el anio.
      #mutate(anio = substring(fecha,nchar(fecha)-4,nchar(fecha))) %>% 
      #mutate(espacio = grepl(", ",fecha))
      mutate(mes = sapply(strsplit(fecha, " "), "[", 2),
             anio = sapply(strsplit(fecha, " "), "[", 5)) %>% 
      mutate(anio = as.double(anio)) %>% 
      # üñá Se utiliza la funci√≥n match con el vector meses.
      mutate(mes = match(mes,meses)) %>% 
      group_by(anio,mes) %>%
      summarise(promedio_aguacate_kg = mean(precio_kg), .groups = "drop") 


```

```{r}

hass %>% datatable()

```

# Datos unidos

```{r}

hass_dolar <- dolar %>% 
  right_join(hass, by = c("mes","anio")) 


```

# Regresi√≥n Lineal con los datos.

## Gr√°fico de dispersi√≥n

```{r}


hass_dolar %>% 
  ggplot(aes(x= promedio_dolar, y= promedio_aguacate_kg )) +
  geom_point()+ theme_light()

```

## Generando un modelo s√∫per simplificado

Modelo ingenuo.

```{r}

hass_dolar %>% 
  ggplot(aes(x= promedio_dolar, y= promedio_aguacate_kg )) +
  geom_point()+ 
  geom_hline(yintercept = mean(hass_dolar$promedio_aguacate_kg))+
  theme_light()


```

Varianza de la variable a predecir

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.

# üí° Recordemos que la varianza de los residuales respecto del modelo ingenuo, es la misma varianza de la variable a predecir.



```

```{r}

hass_dolar %>% 
  ggplot(aes(x= promedio_dolar, y= promedio_aguacate_kg )) +
  geom_point()+ 
  geom_hline(yintercept = mean(hass_dolar$promedio_aguacate_kg))+
  geom_segment(aes(xend=promedio_dolar, yend=mean(promedio_aguacate_kg)),
               col='red', lty='dashed')+
  theme_light()


```

## Modelo

```{r}

# lm(formula, data, subset, weights, na.action,
#    method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
#    singular.ok = TRUE, contrasts = NULL, offset, ...)


mod1 <- hass_dolar %>% lm(promedio_aguacate_kg ~ promedio_dolar,.)

summary(mod1)

# Generando el modelo con R Base ser√≠a as√≠:
# mod1 <- lm(promedio_aguacate_kg ~ promedio_dolar, hass_dolar)

```

¬øCu√°l es la representaci√≥n funcional del modelo?

```{r}




```



```{r}

hass_dolar %>% 
  ggplot(aes(x= promedio_dolar, y= promedio_aguacate_kg )) +
  geom_point()+ theme_light()+
  geom_smooth(method='lm', formula=y~x, se=FALSE, col='dodgerblue1')


```

# Varianza de los residuales del modelo



# Obteniendo los coeficientes.

```{r}

mod1$coefficients
  

```

# Valores ajustados - Fitted values.

```{r}

mod1$fitted.values %>% 
  head(20)

```

# Residuales

```{r}

mod1$residuals %>% 
  head(20)

# Los valores ajustados y los residuales tambi√©n se pueden recuperar usando las funciones fitted( ) y residuals( ). Consulte la ayuda de estas funciones para conocer otros detalles.

```


Calcular la varianza de los residuales.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.



```

# Porcentaje de variabilidad no explicada por el modelo.

```{r}

# En esta secci√≥n el estudiante desarrollar√° los c√°lculos.




```

# Diagrama de dispersi√≥n con los puntos originales

## Creaci√≥n de la columna de predicci√≥n

En este chunk trabajamos con el modo de escritura Tidyverse, aunque se
muestra c√≥mo ser√≠a con R base.

```{r}

#hass_dolar$predicciones <- predict(mod1)

hass_dolar <- hass_dolar %>% 
                mutate(predicciones = predict(mod1))
                

```

```{r}


hass_dolar %>% 
ggplot(aes(x = promedio_dolar, y = promedio_aguacate_kg)) +
  geom_smooth(method = "lm", se = FALSE, color="lightblue") +
  geom_segment(aes(xend=promedio_dolar, yend=predicciones),
               col='red', lty='dashed') +
  geom_point() +
  geom_point(aes(y=predicciones), col='red') +
  theme_light()


#Con R Base
# ggplot(datos, aes(x=Edad, y=Resistencia)) +
#   geom_smooth(method="lm", se=FALSE, color="lightgrey") +
#   geom_segment(aes(xend=Edad, yend=predicciones), col='red', lty='dashed') +
#   geom_point() +
#   geom_point(aes(y=predicciones), col='red') +
#   theme_light()

```

# REVISI√ìN B√ÅSICA DE CONCEPTOS - Simulaci√≥n.

Vamos a simular un modelo de regresi√≥n cuya especificaci√≥n funcional es la siguiente:

$$ y = \beta_0 + \beta_1 * x + e $$

Con $e \text{~} N(0,\sigma^2)$


Como se puede notar, todo modelo teorico se compone de dos t√©rminos:

1) El valor E(y|x)= E_y_x (valor esperado de y dado x)
2) El componente aleatorio tambi√©n llamado error.

Aunque en el modelo anterior hemos elegido una estructura lineal para representar E(y|x) en realidad podemos elegir cualquier otra estructura alternativa, siempre que respetemos la linealidad en los betas.

```{r}

bo = 2
b1_1 = 3
b1_2 = 0.5
x <- seq(1,10)

# Estructura para el valor esperado de y dado x ()
# Esta estructura admitir√≠a otras formas funcionales

# El valor esperado es deterministico.
E_y_x_1 <- bo + b1_1 * x

E_y_x_2 <- bo + b1_1 * x + b1_2 * x^2


# el valor observado o valor real, es aleatorio.



y_obs_1 <- E_y_x_1 + rnorm(10, 0, 4)

y_obs_2 <- E_y_x_2 + rnorm(10, 0, 4)

datos_simulados <- data.frame(
  x = x,
  x_2 = x^2,
  E_y_x_1 = E_y_x_1,
  E_y_x_2 = E_y_x_2,
  y_obs_1 = y_obs_1,
  y_obs_2 = y_obs_2,
  y_pred_1 = predict(lm(y_obs_1 ~ x)),
  y_pred_2 = predict(lm(y_obs_2 ~ x+ x^2)))

  
```

# Dibujamos un gr√°fico de dispersi√≥n

```{r}


datos_simulados %>% 
ggplot(aes(x = x, y = y_obs_1)) +
  geom_smooth(method = "lm", formula = y~x, se = FALSE, color="lightblue") +
  geom_point(col = "green")+
  geom_point(aes(y=y_obs_2), col='red')+ 
  geom_smooth(method="lm", se= FALSE ,formula=y_obs_2~poly(x, 2),color = "blue")+
  ylab("Y")

  

```

# DISCUTIENDO SOBRE LA LINEALIDAD DE LOS BETAS

En esta secci√≥n vamos a aclarar el asunto de que los BETAS sean lineales.

La linealidad en este contexto hace referencia a que no tengan potencias y que los **coeficientes sean independendientes entre ellos**.

Supongamos un modelo con coeficientes repetidos.

$$E(y|x) = \beta_0 + \alpha * x + \alpha * x^2 $$


Debemos factorizarlos para evitar estimarlos por separado y producir inconsistencias,esto debido a que ambos alpha (coeficientes) son dependientes.Es decir, reescribimos:

$$E(y|x) = \beta_0 + \alpha (x + x^2) $$ 

Donde el componente x + x¬≤ ser√° el vector de valores de la variable predictora, que puede pensarse como una variable nueva:

$$ \tilde{x} = x + x^2$$
Y por lo tanto pensar al modelo como:

$$ E(y|x) = \beta_0 + \alpha * \tilde{x}$$
Esta transformaci√≥n garantiza la creaci√≥n de un nuevo modelo lineal respecto a $\beta_0$ y $\alpha$.

Es importante anotar que el dataframe que alimentar√° el modelo deber√° tener computada la variable auxiliar $\tilde{x}$, la cual debe ser pasada a la funci√≥n encargada de estimar par√°metros.

Esto corresponde a crear una nueva variable con los c√°lculos mencionados $\tilde{x} = x + x^2$.
Sin embargo, para graficar es conveniente usar la variable original.



```{r}

# Se define una semilla para generar los aleatorios.
set.seed(77)

alpha <- 0.7

E_y_x_3 = bo + alpha * (x + x^2)
y_obs_3 = E_y_x_3 + rnorm(10, 0, 4)

x_auxiliar = x + x^2

datos_simulados_2 <- data.frame(
  x = x,
  x_2 = x^2,
  x_auxiliar = x_auxiliar,
  E_y_x_3 = E_y_x_3,
  y_obs_3 =  y_obs_3,
  y_pred_3_1 = predict(lm(y_obs_3 ~ poly(x, 2))),
  y_pred_3_2 = predict(lm(y_obs_3 ~ x_auxiliar )))


modelo1a <-  lm(y_obs_3 ~ poly(x, 2))
modelo1b <-  lm(y_obs_3 ~ poly(x, 2, raw = TRUE))
modelo2 <-  lm(y_obs_3 ~ x_auxiliar )



```


Notese que los coeficientes arrojados por el modelo1a al invocar la funci√≥n poly sin par√°metro de raw=TRUE, no son atribuibles o no est√°n asociados a los t√©rminos $1,x,x^2$, es decir, **No es v√°lido escribir**:


$$E(y|x)= `r round(modelo1a$coefficients[1], 2)` * 1 + `r round(modelo1a$coefficients[2], 2)` * x + `r round(modelo1a$coefficients[3], 2)` * x^2$$
Esta ecuaci√≥n no ser√≠a v√°lida porque al remplazar para $x=5$ se obtiene $E(y|5)=$ `r round(modelo1a$coefficients[1] * 1 + modelo1a$coefficients[2] * 5 + modelo1a$coefficients[3] * 5^2, 2)`, mientras que al remplazar en el modelo con el par√°metro raw = TRUE, se obtendr√≠a: $E(y|5)=$ `r round(modelo1b$coefficients[1] * 1 + modelo1b$coefficients[2] * 5 + modelo1b$coefficients[3] * 5^2, 2)`.

Observe que para el valor $x=5$ los valores observados de $y$ est√°n cercanos a 24 por lo que una predicci√≥n arrojando un valor cercano a 836, est√° claramente desfasada. Observe el gr√°fico m√°s abajo para comprender lo explicado.

Esto se debe a que los coeficientes anclados al modelo_1a est√°n dise√±ados para ser usados sobre la siguiente base de polinomios ortogonales:

- $P_1(x)= a$
- $P_2(x)=m*x - c$
- $P_3(x)= k_1 + k_2 *x + k_3 * x ^2$

Donde $a,m,c,k_1,k_2,k_3$ son par√°metros estimados que lamentablemente no est√°n siendo arrojados por el modelo.

A continuaci√≥n se muestra el resumen de cada uno de los modelos, utilizando la funci√≥n summary().

```{r}


summary(modelo1a)
summary(modelo1b)
summary(modelo2)




```

```{r}

datos_simulados_2 %>% 
  ggplot(aes(x = x , y = y_obs_3))+
  geom_point(col = 'green')+
  geom_point(aes(y = y_pred_3_1), col = 'blue')+
  geom_point(aes(y = y_pred_3_2), col = 'red')


```


```{r eval=FALSE, include=FALSE}

# TAREA,
# Buscar la descomposici√≥n ortogonal de los vectores.

u1 = 1/sqrt(2)
u2 = sqrt(3/2) * (5-1)
u3 = sqrt(5/2) * ((3/2)*(5^2) - 3*5 + 1)

#E_y_5 = coef1 * u1 + coef2*u2 + coef3*u3
E_y_5 = modelo1a$coefficients[1] * u1 + modelo1a$coefficients[2]*u2 + modelo1a$coefficients[3]*u3

modelo1a$coefficients[1]
modelo1a$coefficients[2]
modelo1a$coefficients[3]

E_y_5

```


# ESTIMACI√ìN DE PAR√ÅMETROS 

## Por m√°xima verosimilitud.

## Por m√≠nimos cuadrados.


```{r}


n <- 1000

vector_x <- c(1: n ) 
sigma <- 4 
beta_0 <- 2
beta_1 <- 0.5

# Esperada de y dado x
E_y_x <- beta_0 + (beta_1 * vector_x )

print(E_y_x)

plot(vector_x,E_y_x)



```

```{r}


error <- rnorm(n = n, mean = 0, sd = sigma)

y_observado <- beta_0 + (beta_1 * vector_x ) + error

print(y_observado)

plot(vector_x,y_observado)


```

# Modelo lineal

```{r}

modelo_y_predicho <- lm(y_observado ~ vector_x)

y_predicho <- predict(modelo_y_predicho)

print(y_predicho)


```

# Uni√≥n en un dataframe

```{r}

datos_modelo <- as.data.frame(vector_x) %>% 
                    mutate(E_y_x = E_y_x,
                           y_observado = y_observado,
                           y_predicho = y_predicho)

```

# Gr√°ficando los valores

```{r}

datos_modelo %>% 
  ggplot(  )+
  geom_point( aes( x= vector_x, y = E_y_x), col = "blue")+
  geom_point( aes( x= vector_x, y = y_observado), col = "red")+
  geom_point( aes( x= vector_x, y = y_predicho), col = "brown")+
  geom_line(aes( x= vector_x, y = E_y_x), col = "blue")+
  geom_line(aes( x= vector_x, y = y_predicho), col = "black")

```

# Diferencia entre esperado y el predicho (Error)

```{r}


modelo_y_predicho$coefficients

b_0_estimado <- modelo_y_predicho$coefficients[1]
b_1_estimado <- modelo_y_predicho$coefficients[2]



datos_modelo <- datos_modelo %>% 
                  mutate(error_predicho_observado = y_predicho - y_observado) 
                  


```

# Construir funci√≥n

```{r}


estimar_betas <-  function(n,b0,b1,sigma,distribucion){
  
  
  if (distribucion == "normal") {
  
    errores <- rnorm(40,0,4)
      
  } else
    
  { errores <- runif(40,-6.928203,6.928203)  }
  
  
  vector_x <- c(1: n) 
  E_y_x <- b0 + (b1 * vector_x )
  
  #error <- rnorm(n = n, mean = 0, sd = sigma)
  #error <- runif(n = n, min=-6.928203, max = 6.928203)
  
  
  y_observado <- E_y_x + errores
  modelo <- lm(y_observado ~ vector_x)
  y_predicho <- predict(modelo)
  
  b0_estimado <- modelo$coefficients[1]
  b1_estimado <- modelo$coefficients[2]
  
  return(list("b0_estimado" = b0_estimado,
              "b1_estimado" = b1_estimado))

}



```

# Usando la funci√≥n

```{r}

resultados1 <- estimar_betas(40,beta_0,beta_1,sigma,"normal")
# resultados2 <- estimar_betas(40,beta_0,beta_1,sigma)
# resultados3 <- estimar_betas(40,beta_0,beta_1,sigma)

print(resultados1)
# print(resultados2)
# print(resultados3)


```

# Creando vector BETAS

```{r}

vector_betas <- function(n,beta_0,beta_1,sigma, n_sim, distribucion) {
  
  vector_b0 <- vector()
  vector_b1 <- vector()
  
  for (i in 1:n_sim) {
    
    resultados <-  estimar_betas(n,beta_0,beta_1,sigma,distribucion)
  
    vector_b0[i] <- resultados$b0_estimado
    vector_b1[i] <- resultados$b1_estimado
    
  }
  
  df_resultados <- data.frame(
                      b0 = vector_b0,
                      b1 = vector_b1)
  
 

  return(df_resultados)

    }


```

```{r}


df_resultados1 <- vector_betas(40,2,2,4,1000,"normal")
df_resultados2 <- vector_betas(40,2,2,4,1000,"runif")


```

# Histograma comparando distribuciones.

```{r}

 histo_b0_normal <- df_resultados1 %>% 
                ggplot()+
                geom_histogram(aes(x = b0))+
                ylim(0,110)
  
  histo_b0_uniforme <- df_resultados2 %>% 
                ggplot()+
                geom_histogram(aes(x = b0))+
                ylim(0,110)
  
  grid.arrange(histo_b0_normal,
               histo_b0_uniforme, 
               ncol = 2)


```

```{r}

 histo_b0_normal <- df_resultados1 %>% 
                ggplot()+
                geom_histogram(aes(x = b0))+
                ylim(0,110)
  
  histo_b1_normal <- df_resultados1 %>% 
                ggplot()+
                geom_histogram(aes(x = b1))+
                ylim(0,110)
  
  grid.arrange(histo_b0_normal,
               histo_b1_normal, 
               ncol = 2)


```

```{r include=FALSE}

# Este chunk se configur√≥ para no imprimirse

vector_betas(100,2,2,4,1000,"normal")



```
